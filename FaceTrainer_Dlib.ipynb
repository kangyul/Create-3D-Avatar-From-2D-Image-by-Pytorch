{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Facial Recognition using PyTorch and OpenCV\n",
    "\n",
    "https://ritik12.medium.com/facial-recognition-using-pytorch-and-opencv-467c4e41d1f\n",
    "\n",
    "\n",
    "Machine Learning - Face Recognition CNN Pytorch.ipynb\n",
    "https://github.com/rubencg195/Pytorch-Tutorials/blob/master/Machine%20Learning%20-%20Face%20Recognition%20CNN%20Pytorch.ipynb\n",
    "\n",
    "\n",
    "\n",
    "Face Recognition Using Pytorch\n",
    "https://github.com/timesler/facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face Landmarks Detection With PyTorch\n",
    "\n",
    "https://towardsdatascience.com/face-landmarks-detection-with-pytorch-4b4852f5e9c4\n",
    "\n",
    "\n",
    "\n",
    "다중입력 deep neural network\n",
    "https://rosenfelder.ai/multi-input-neural-network-pytorch/\n",
    "\n",
    "\n",
    "\n",
    "Understanding dimensions in PyTorch\n",
    "https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.init import *\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from FaceFeatureDataset import FaceFeatureDataset\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 68])\n",
      "output_2d tensor([[[-6.3727e-01, -6.4303e-01, -7.5010e-01,  3.6031e-01, -9.4603e-01,\n",
      "           6.5386e-01, -5.4889e-01, -2.2606e-01, -2.3648e-01, -1.0633e+00,\n",
      "          -1.3182e-01, -7.2447e-01, -3.8411e-01, -6.1179e-01, -3.0288e-01,\n",
      "          -1.4347e-01, -3.2634e-01,  6.7208e-02, -5.1199e-01, -1.3262e-01,\n",
      "          -3.0489e-01, -6.0575e-01, -3.9401e-01, -1.7706e-01, -5.2545e-01,\n",
      "          -9.4922e-02,  4.3826e-02, -1.5253e-01,  3.0105e-01,  2.1141e-01,\n",
      "          -2.3795e-02,  4.3036e-01, -1.3217e-01,  1.8488e-02,  9.4067e-03,\n",
      "           3.0157e-01,  1.1920e-01, -7.0417e-01,  1.6482e-01, -7.7173e-01,\n",
      "           5.6371e-01,  4.2827e-02, -7.0549e-01,  8.6433e-01,  5.3368e-01,\n",
      "          -1.0552e-01, -4.1446e-03, -2.1007e-01, -2.3585e-01,  8.8352e-01,\n",
      "          -2.6643e-01, -1.6444e-01, -8.3990e-01, -1.1832e-01,  1.3146e-01,\n",
      "           5.0611e-01,  1.0961e+00, -3.9928e-01,  3.8632e-01, -3.0685e-01,\n",
      "          -8.9605e-02, -3.3475e-04,  1.0583e+00, -4.8248e-01,  2.3578e-01,\n",
      "           6.8058e-01,  3.7112e-01,  2.2245e-02, -2.6665e-01, -7.9791e-01,\n",
      "           3.8980e-01,  5.2135e-01, -6.4526e-02, -9.7866e-02,  1.3369e-02,\n",
      "           1.9110e-01,  8.4203e-01, -9.2147e-01,  3.4082e-01, -3.8645e-01,\n",
      "           4.8549e-01, -6.5016e-01, -1.6152e-01, -1.0170e+00, -5.3639e-01,\n",
      "           6.1317e-01, -7.8211e-02,  6.2690e-01,  5.2757e-01, -2.6125e-02,\n",
      "           1.2968e+00,  3.5072e-01,  9.4377e-01, -1.8917e-01, -2.3114e-01,\n",
      "           3.4282e-01,  2.3553e-01,  9.8210e-02, -5.6320e-02,  3.2797e-01,\n",
      "          -2.3677e-01,  5.7516e-01, -9.1125e-01, -3.4985e-02, -4.4474e-01,\n",
      "           5.0485e-01,  1.0278e-01, -1.7403e-01, -6.9230e-01,  1.4945e-01,\n",
      "          -2.4555e-01, -6.2500e-01, -8.8598e-01,  1.4545e-01, -3.3142e-01,\n",
      "           1.4140e-01,  3.0183e-01,  3.2450e-01, -2.5663e-01,  1.4139e+00,\n",
      "          -6.0430e-01, -3.9682e-01,  3.9953e-01, -1.1664e-01, -1.0007e-01,\n",
      "          -1.3020e-02, -2.2794e-01, -3.2141e-01],\n",
      "         [-3.8347e-01, -1.1710e-01, -1.0612e-01,  2.4040e-01, -1.4026e+00,\n",
      "           8.8934e-01,  6.7094e-01,  7.0794e-03,  1.4127e-01, -2.7534e-01,\n",
      "           7.2465e-01, -2.0107e-01, -5.5954e-01, -2.7607e-01, -4.3743e-01,\n",
      "           1.0590e+00, -1.2163e-01,  3.2619e-01, -2.9632e-01, -1.0824e+00,\n",
      "           3.9367e-01, -5.1288e-01, -5.2859e-01, -5.6087e-01,  4.0449e-01,\n",
      "          -6.7858e-01, -3.8268e-01, -6.3525e-01,  2.8515e-01, -1.2763e+00,\n",
      "          -8.6838e-01,  5.3495e-01,  8.2457e-01,  2.3573e-01, -5.2360e-01,\n",
      "          -2.7540e-01, -2.3188e-01, -6.7319e-02,  4.3493e-01, -7.0070e-01,\n",
      "          -1.0322e-01,  5.7242e-01,  1.1040e+00, -1.5585e-01, -6.6207e-01,\n",
      "          -2.5709e-01, -1.1583e-01, -4.9210e-01, -1.1693e-01,  4.1241e-01,\n",
      "           1.0556e-01, -8.5607e-02,  1.7294e-01, -1.5655e-01, -6.3838e-01,\n",
      "          -6.0000e-01,  2.1989e-01, -6.6925e-01, -7.1824e-01,  1.0280e+00,\n",
      "          -5.2958e-01,  5.3675e-02, -2.3612e-01, -7.4775e-01,  5.7534e-01,\n",
      "           3.3266e-01,  9.2919e-01,  3.8450e-01, -4.2002e-01,  2.5499e-01,\n",
      "          -1.0582e+00,  9.5675e-02,  4.5680e-01, -5.0492e-01,  2.2719e-01,\n",
      "           1.2870e-01,  2.8619e-01,  3.7229e-01,  5.6605e-01, -1.2460e-01,\n",
      "           3.0497e-01,  8.4367e-01,  9.7515e-01, -1.0751e-01, -2.4463e-01,\n",
      "          -2.2948e-01,  8.8231e-01,  1.5144e+00,  1.0207e-01, -8.1710e-01,\n",
      "          -5.6750e-01,  2.8078e-01,  1.2271e+00, -4.5726e-01,  3.2535e-01,\n",
      "          -7.9341e-01,  3.7441e-01,  8.7795e-01,  8.7529e-01,  1.4929e-01,\n",
      "          -4.0783e-02, -5.0107e-01, -7.2045e-01,  5.8164e-01,  5.5045e-01,\n",
      "          -6.2739e-01,  5.6079e-01, -2.9963e-01,  8.1832e-01,  4.2575e-01,\n",
      "          -6.6030e-01, -1.9485e-02, -4.9490e-01,  1.9632e-01, -2.2224e-02,\n",
      "          -1.4915e-01,  4.0400e-01, -7.9384e-01, -4.1632e-01,  1.5375e-01,\n",
      "          -6.1282e-01,  2.5755e-01,  2.0258e-01,  1.1405e-01, -4.5444e-01,\n",
      "           2.9577e-01, -1.1048e-01, -3.6196e-01]],\n",
      "\n",
      "        [[-1.7745e-02,  8.5151e-02, -9.9621e-01,  5.2720e-01, -6.0422e-01,\n",
      "          -1.1276e-01,  3.5798e-01,  2.7368e-01,  2.2764e-01,  4.7863e-01,\n",
      "           4.4912e-01, -5.2046e-01, -1.6556e-01, -2.7287e-01, -1.0059e-01,\n",
      "           4.0161e-01, -7.5632e-01,  8.4901e-02, -3.6540e-01, -2.9498e-01,\n",
      "          -1.2401e+00,  1.8298e-01, -3.8684e-01,  4.0330e-01,  5.1628e-01,\n",
      "           1.4069e-01,  8.0297e-01,  3.2768e-01, -5.9931e-02,  9.2995e-01,\n",
      "          -7.4393e-01,  1.0845e-01,  8.9740e-01,  6.7090e-01, -1.0697e+00,\n",
      "          -6.3418e-01, -3.1300e-01,  8.1647e-02,  1.8352e-01,  8.2229e-02,\n",
      "          -4.9849e-01,  9.7485e-01,  6.0796e-01,  4.8355e-02, -1.2511e-01,\n",
      "           4.3151e-01, -9.4976e-01,  8.1310e-01,  6.6150e-01, -3.0319e-01,\n",
      "          -5.1466e-01,  2.0606e-03,  5.4435e-01,  3.9718e-01, -1.1279e-01,\n",
      "           3.7192e-01,  4.2461e-02, -4.5311e-01, -1.5329e-01,  4.6552e-03,\n",
      "           2.6896e-01,  2.3136e-01,  5.0199e-01, -3.2431e-02,  3.3611e-01,\n",
      "           1.7141e-02,  1.9665e-01,  1.1273e+00,  1.3856e+00,  3.7067e-01,\n",
      "          -5.8700e-01,  1.8838e-01,  7.6619e-01,  1.1829e+00, -2.7641e-01,\n",
      "           2.9880e-01,  4.0898e-01,  4.5501e-01,  4.3321e-01, -1.0636e-02,\n",
      "           6.4708e-01, -2.5137e-01, -1.5038e+00, -1.7568e+00,  1.0906e-01,\n",
      "           7.4987e-01,  4.2084e-01,  1.6093e+00,  1.4786e-01,  7.0245e-01,\n",
      "          -1.9883e-01,  8.5570e-01,  1.2312e+00,  5.1617e-01,  8.6774e-03,\n",
      "           8.3735e-01,  2.5176e-01,  1.2300e-01, -4.2039e-01,  4.4664e-01,\n",
      "           5.1420e-01,  5.1854e-01,  1.0260e+00,  1.1153e+00,  3.7137e-02,\n",
      "           7.3404e-01, -6.8246e-03,  6.2876e-02,  8.9838e-01, -9.4908e-02,\n",
      "          -8.0732e-01, -3.8425e-01, -1.1176e-01,  5.2396e-02, -8.6313e-01,\n",
      "           4.1192e-01, -2.3783e-01,  1.9515e-01,  1.0179e+00,  3.7620e-01,\n",
      "           3.1781e-01,  2.0259e-01, -3.5962e-01,  6.5841e-01, -3.2205e-01,\n",
      "           9.0159e-01,  2.3282e-01, -6.1985e-01],\n",
      "         [-4.9251e-01, -5.8803e-01,  5.2621e-01, -4.8687e-01, -1.0712e+00,\n",
      "           1.5401e+00, -3.4232e-01, -5.1901e-01, -4.4647e-01, -1.4634e-01,\n",
      "           2.0483e-01,  1.2232e+00, -4.0711e-01, -6.4177e-01, -1.0506e+00,\n",
      "          -2.0380e-01,  2.5740e-01,  5.9689e-01, -6.4598e-01, -2.4095e-01,\n",
      "           7.4974e-01, -3.4685e-01,  6.4612e-01, -1.2798e+00, -1.6496e-01,\n",
      "          -3.8448e-01,  8.0564e-02, -6.0944e-01,  3.5385e-01,  3.5827e-01,\n",
      "          -3.7794e-01, -3.6594e-01, -8.8321e-01,  8.6657e-01,  5.4734e-01,\n",
      "           4.2064e-01,  3.6626e-01, -4.0743e-01, -4.5142e-01, -3.2720e-01,\n",
      "          -1.5849e-01, -1.4532e+00,  6.6816e-01,  2.3411e-01,  1.2376e+00,\n",
      "           1.1703e+00,  3.7489e-01, -1.3068e+00,  7.8677e-01,  3.2452e-01,\n",
      "           3.9090e-01,  4.8609e-01,  1.3847e-01, -1.4688e+00, -5.0203e-01,\n",
      "          -1.5216e-01,  8.7896e-01,  1.9754e-01, -3.2990e-01, -4.1100e-02,\n",
      "          -4.5766e-01, -2.6879e-01,  6.4488e-01, -6.9821e-01,  6.4663e-01,\n",
      "          -1.9754e-01,  6.6245e-01,  1.5053e-02, -9.9628e-01,  2.6348e-01,\n",
      "          -5.8007e-01,  2.6146e-01, -2.9542e-01, -1.2061e+00, -1.9212e+00,\n",
      "           3.8940e-01,  4.6796e-01, -2.5912e-01,  2.4445e-01, -4.9738e-02,\n",
      "           3.1026e-01,  5.4552e-01,  1.1539e+00, -3.6710e-01, -6.1565e-01,\n",
      "          -5.3746e-01,  5.7559e-01,  7.0367e-01,  7.2153e-01,  2.8579e-01,\n",
      "          -6.6994e-03, -2.7275e-01, -3.2438e-01, -1.1275e-01, -1.7622e-01,\n",
      "          -6.9209e-01, -9.9089e-01,  2.3720e-01, -5.5000e-02,  3.8329e-01,\n",
      "          -8.4109e-02, -4.5186e-02, -7.0918e-01, -9.6516e-01,  1.8561e-01,\n",
      "          -7.0417e-01,  4.7196e-01, -3.1364e-01, -1.4439e+00,  7.4082e-01,\n",
      "           6.2644e-02, -4.5898e-01,  5.1908e-01,  1.1018e-01,  2.1472e-01,\n",
      "          -1.7779e-01,  6.7293e-01, -8.2188e-01, -6.1945e-01,  1.2999e+00,\n",
      "           3.7668e-01, -2.6025e-01,  1.3993e-01,  6.1004e-01, -6.1297e-01,\n",
      "          -2.2972e-01, -4.5026e-01,  4.1100e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer_2d = nn.Linear(in_features=68, out_features=128)\n",
    "flatten = nn.Flatten()\n",
    "input_2d = torch.randn(2, 2, 68) # n, channel, features\n",
    "#print(input_2d)\n",
    "print(input_2d.shape)\n",
    "output_2d = linear_layer_2d(input_2d)\n",
    "print('output_2d' , output_2d)\n",
    "#print(output_2d.size())\n",
    "\n",
    "#output_1d = flatten(input_2d)\n",
    "#print(output_1d.size())\n",
    "#linear_layer_2d(output_1d)\n",
    "#print(\"1D \",  output_1d)\n",
    "\n",
    "#test_sequence = nn.Sequential(nn.Linear(64, 32), nn.ReLU(), nn.Linear(32,10))\n",
    "#out = test_sequence(input_2d)\n",
    "#print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15214901 0.26409625]\n",
      " [0.15847385 0.35434336]\n",
      " [0.17404998 0.4446534 ]\n",
      " [0.19193892 0.53497918]\n",
      " [0.21678205 0.62303933]\n",
      " [0.262472   0.70661546]\n",
      " [0.32675891 0.77644054]\n",
      " [0.40500139 0.83479593]\n",
      " [0.48582416 0.85384914]\n",
      " [0.56685146 0.84283572]\n",
      " [0.6481305  0.79481721]\n",
      " [0.71564276 0.73051458]\n",
      " [0.7716696  0.65456918]\n",
      " [0.80464692 0.56690236]\n",
      " [0.82840444 0.47454697]\n",
      " [0.8521777  0.37987876]\n",
      " [0.86440259 0.28281907]\n",
      " [0.20106432 0.21354466]\n",
      " [0.24754095 0.18147987]\n",
      " [0.30772143 0.17495049]\n",
      " [0.36313466 0.189205  ]\n",
      " [0.41614068 0.21732068]\n",
      " [0.58269505 0.21382786]\n",
      " [0.64068857 0.18878019]\n",
      " [0.70321333 0.17764091]\n",
      " [0.76562795 0.18269135]\n",
      " [0.81392966 0.22233966]\n",
      " [0.50349283 0.29655437]\n",
      " [0.50303656 0.3636261 ]\n",
      " [0.50025174 0.43299491]\n",
      " [0.4951541  0.50234799]\n",
      " [0.43711337 0.5343341 ]\n",
      " [0.46480426 0.54377418]\n",
      " [0.49713651 0.5509329 ]\n",
      " [0.52726608 0.54188616]\n",
      " [0.55509857 0.53051087]\n",
      " [0.27217955 0.29960666]\n",
      " [0.30701342 0.2790273 ]\n",
      " [0.35326978 0.27934197]\n",
      " [0.39471171 0.30737899]\n",
      " [0.35067376 0.32095696]\n",
      " [0.3044174  0.3206423 ]\n",
      " [0.60516242 0.31112355]\n",
      " [0.64468487 0.28132439]\n",
      " [0.6955826  0.2793577 ]\n",
      " [0.73708746 0.29814345]\n",
      " [0.69762795 0.31869135]\n",
      " [0.64904304 0.32067376]\n",
      " [0.38319482 0.64036187]\n",
      " [0.42267006 0.61750116]\n",
      " [0.46208237 0.60389172]\n",
      " [0.49439889 0.61336326]\n",
      " [0.52452846 0.60431652]\n",
      " [0.56373623 0.62077372]\n",
      " [0.61219528 0.63729385]\n",
      " [0.56796853 0.67862564]\n",
      " [0.52855622 0.69223508]\n",
      " [0.49384822 0.69431189]\n",
      " [0.45917168 0.69176307]\n",
      " [0.41997964 0.67299306]\n",
      " [0.40399445 0.64281629]\n",
      " [0.45956502 0.63394262]\n",
      " [0.49424155 0.63649144]\n",
      " [0.52432392 0.63438316]\n",
      " [0.58903563 0.64176215]\n",
      " [0.52660528 0.63902453]\n",
      " [0.49650717 0.64344563]\n",
      " [0.46184637 0.63858399]]\n",
      "imagefile\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testload = pd.read_csv(\"./outimg/Train/facefeature.csv\")\n",
    "landmarks = np.array(testload.iloc[0, 1:])\n",
    "landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "print(landmarks)\n",
    "\n",
    "dataiter = iter(testload)\n",
    "landmark = next(dataiter)\n",
    "print(landmark)\n",
    "# print(landmarks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, F, C]: torch.Size([10, 68, 2])\n",
      "Shape of Tensor y: torch.Size([10, 33]) torch.float32\n",
      "X type torch.float32\n",
      "y type torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader \n",
    "\n",
    "training_data = FaceFeatureDataset(feature_file=\"./outimg/Train/facefeature.csv\", label_file=\"./Dataset/Train/csv/train.csv\")\n",
    "test_data = FaceFeatureDataset(feature_file=\"./outimg/Test/facefeature.csv\", label_file=\"./Dataset/Test/csv/test.csv\")\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 데이터 로드 확인 \n",
    "for X, y in train_loader:\n",
    "    print(f\"Shape of X [N, F, C]: {X.shape}\") # N , Channel, H= width W = height\n",
    "    print(f\"Shape of Tensor y: {y.shape} {y.dtype}\")   \n",
    "    break\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "# print(f'Traing dat length {n_total_steps}')\n",
    "# print(y)\n",
    "print('X type', X.dtype)\n",
    "print('y type', y.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#dataiter = iter(train_loader)\n",
    "#landmark, labels = next(dataiter)\n",
    "#print(\"landmark Shape \", landmark.shape)\n",
    "#print(landmark)\n",
    "#flatten = nn.Flatten()\n",
    "#linear1 = nn.Linear(68, 32)\n",
    "#x = flatten(landmark)\n",
    "#print(x)\n",
    "#print(x.size())\n",
    "# print(landmark.shape)\n",
    "# print('linear1', x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=136, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=33, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 33\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(68 * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # print(pred)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('batch',  batch)\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.118177  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.121191  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.132526  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.121519  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.120681  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.137382  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.128027  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.121842  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.117471  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122807  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.129388  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.124150  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.131769  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.128903  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.133290  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122454  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.116993  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.120185  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.130063  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.123005  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.115863  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122330  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.121892  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122730  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.123967  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.128641  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122512  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.131656  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.123077  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.125304  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.120601  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122931  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.120340  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.115753  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.119558  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.125495  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122735  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.124857  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.120141  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.121526  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.129637  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.120854  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.111118  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.118461  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.119366  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.127342  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.130936  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.118331  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.125932  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.134817  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.115049  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122145  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.127108  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.126664  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.128393  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.125541  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.124261  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.121594  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.116772  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.128299  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.125025  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.123969  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.123489  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.129575  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.119411  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.120476  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.131306  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.118196  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.128481  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.126044  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.129073  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.116244  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.126980  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.127933  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.123558  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.124104  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.126001  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.123917  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.129793  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.126155  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.124003  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.133323  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.120779  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.129243  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.123413  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122600  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.125371  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.126145  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.130025  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122520  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.130111  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.125879  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.128923  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.129296  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.124990  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.130497  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.122139  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.124823  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.124196  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "batch 0\n",
      "loss: 0.128364  [    0/  100]\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 100\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, criterion, optimizer)    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    #print('test size', size )\n",
    "    # num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            #print(X.size())\n",
    "            #print(y)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)            \n",
    "            print('pred', pred)\n",
    "            #print('loss', loss)\n",
    "            #print('real', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred tensor([[0.4924, 0.4831, 0.0000, 0.0000, 0.0000, 0.0000, 0.4763, 0.5001, 0.5210,\n",
      "         0.5093, 0.0000, 0.0000, 0.4974, 0.0000, 0.4840, 0.4896, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4980, 0.5073, 0.4991, 0.4995, 0.5207, 0.4927, 0.5271,\n",
      "         0.4884, 0.4853, 0.5032, 0.5223, 0.5195, 0.0000],\n",
      "        [0.4895, 0.4818, 0.0000, 0.0000, 0.0000, 0.0000, 0.4790, 0.4957, 0.5180,\n",
      "         0.5109, 0.0000, 0.0000, 0.4944, 0.0000, 0.4818, 0.4897, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4952, 0.4987, 0.4964, 0.4978, 0.5145, 0.4861, 0.5230,\n",
      "         0.4887, 0.4795, 0.5064, 0.5229, 0.5155, 0.0000],\n",
      "        [0.4907, 0.4821, 0.0000, 0.0000, 0.0000, 0.0000, 0.4745, 0.4989, 0.5169,\n",
      "         0.5083, 0.0000, 0.0000, 0.4922, 0.0000, 0.4812, 0.4882, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4962, 0.5024, 0.4941, 0.4967, 0.5155, 0.4851, 0.5219,\n",
      "         0.4851, 0.4782, 0.5025, 0.5210, 0.5157, 0.0000],\n",
      "        [0.4874, 0.4787, 0.0000, 0.0000, 0.0000, 0.0000, 0.4747, 0.4959, 0.5169,\n",
      "         0.5074, 0.0000, 0.0000, 0.4891, 0.0000, 0.4792, 0.4857, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4941, 0.4994, 0.4907, 0.4939, 0.5111, 0.4822, 0.5187,\n",
      "         0.4834, 0.4752, 0.5025, 0.5194, 0.5113, 0.0000],\n",
      "        [0.4872, 0.4799, 0.0000, 0.0000, 0.0000, 0.0000, 0.4758, 0.4954, 0.5188,\n",
      "         0.5097, 0.0000, 0.0000, 0.4924, 0.0000, 0.4819, 0.4877, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4954, 0.5010, 0.4936, 0.4966, 0.5133, 0.4849, 0.5218,\n",
      "         0.4865, 0.4782, 0.5041, 0.5216, 0.5145, 0.0000],\n",
      "        [0.4875, 0.4799, 0.0000, 0.0000, 0.0000, 0.0000, 0.4756, 0.4952, 0.5175,\n",
      "         0.5092, 0.0000, 0.0000, 0.4912, 0.0000, 0.4807, 0.4874, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4953, 0.5009, 0.4938, 0.4963, 0.5132, 0.4843, 0.5209,\n",
      "         0.4855, 0.4774, 0.5037, 0.5219, 0.5146, 0.0000],\n",
      "        [0.4901, 0.4814, 0.0000, 0.0000, 0.0000, 0.0000, 0.4760, 0.4979, 0.5162,\n",
      "         0.5083, 0.0000, 0.0000, 0.4917, 0.0000, 0.4813, 0.4880, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4961, 0.5009, 0.4934, 0.4959, 0.5140, 0.4836, 0.5210,\n",
      "         0.4842, 0.4772, 0.5032, 0.5217, 0.5146, 0.0000],\n",
      "        [0.4845, 0.4750, 0.0000, 0.0000, 0.0000, 0.0000, 0.4688, 0.4966, 0.5112,\n",
      "         0.5037, 0.0000, 0.0000, 0.4808, 0.0000, 0.4737, 0.4797, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4923, 0.4953, 0.4828, 0.4899, 0.5050, 0.4723, 0.5092,\n",
      "         0.4762, 0.4673, 0.4957, 0.5136, 0.5051, 0.0000],\n",
      "        [0.4920, 0.4843, 0.0000, 0.0000, 0.0000, 0.0000, 0.4726, 0.5005, 0.5219,\n",
      "         0.5106, 0.0000, 0.0000, 0.4951, 0.0000, 0.4835, 0.4909, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4988, 0.5086, 0.4994, 0.4989, 0.5223, 0.4921, 0.5272,\n",
      "         0.4903, 0.4831, 0.5041, 0.5236, 0.5202, 0.0000],\n",
      "        [0.4800, 0.4731, 0.0000, 0.0000, 0.0000, 0.0000, 0.4685, 0.4930, 0.5131,\n",
      "         0.5041, 0.0000, 0.0000, 0.4836, 0.0000, 0.4742, 0.4808, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4906, 0.4969, 0.4837, 0.4914, 0.5069, 0.4754, 0.5128,\n",
      "         0.4789, 0.4702, 0.4949, 0.5142, 0.5072, 0.0000]], device='cuda:0')\n",
      "pred tensor([[0.4910, 0.4822, 0.0000, 0.0000, 0.0000, 0.0000, 0.4755, 0.4996, 0.5202,\n",
      "         0.5089, 0.0000, 0.0000, 0.4949, 0.0000, 0.4830, 0.4891, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4979, 0.5067, 0.4971, 0.4971, 0.5194, 0.4904, 0.5251,\n",
      "         0.4868, 0.4832, 0.5033, 0.5228, 0.5177, 0.0000],\n",
      "        [0.5025, 0.4912, 0.0000, 0.0000, 0.0000, 0.0000, 0.4794, 0.5054, 0.5271,\n",
      "         0.5137, 0.0000, 0.0000, 0.5022, 0.0000, 0.4882, 0.4959, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5032, 0.5140, 0.5100, 0.5027, 0.5300, 0.5038, 0.5354,\n",
      "         0.4959, 0.4917, 0.5114, 0.5287, 0.5264, 0.0000],\n",
      "        [0.4892, 0.4831, 0.0000, 0.0000, 0.0000, 0.0000, 0.4776, 0.4963, 0.5234,\n",
      "         0.5123, 0.0000, 0.0000, 0.4999, 0.0000, 0.4854, 0.4925, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4975, 0.5072, 0.5018, 0.5011, 0.5224, 0.4951, 0.5304,\n",
      "         0.4934, 0.4871, 0.5065, 0.5257, 0.5219, 0.0000],\n",
      "        [0.5010, 0.4916, 0.0000, 0.0000, 0.0000, 0.0000, 0.4789, 0.5041, 0.5298,\n",
      "         0.5157, 0.0000, 0.0000, 0.5048, 0.0000, 0.4911, 0.4979, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5036, 0.5150, 0.5110, 0.5045, 0.5315, 0.5050, 0.5385,\n",
      "         0.4996, 0.4928, 0.5130, 0.5300, 0.5283, 0.0000],\n",
      "        [0.4890, 0.4788, 0.0000, 0.0000, 0.0000, 0.0000, 0.4686, 0.5001, 0.5161,\n",
      "         0.5057, 0.0000, 0.0000, 0.4878, 0.0000, 0.4757, 0.4838, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4948, 0.5017, 0.4913, 0.4944, 0.5164, 0.4830, 0.5169,\n",
      "         0.4843, 0.4762, 0.4965, 0.5152, 0.5119, 0.0000],\n",
      "        [0.4896, 0.4797, 0.0000, 0.0000, 0.0000, 0.0000, 0.4712, 0.5005, 0.5147,\n",
      "         0.5068, 0.0000, 0.0000, 0.4887, 0.0000, 0.4791, 0.4851, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4972, 0.5020, 0.4893, 0.4943, 0.5137, 0.4804, 0.5157,\n",
      "         0.4816, 0.4758, 0.4987, 0.5179, 0.5121, 0.0000],\n",
      "        [0.4839, 0.4759, 0.0000, 0.0000, 0.0000, 0.0000, 0.4672, 0.4951, 0.5151,\n",
      "         0.5057, 0.0000, 0.0000, 0.4839, 0.0000, 0.4746, 0.4822, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4922, 0.4981, 0.4874, 0.4930, 0.5104, 0.4781, 0.5151,\n",
      "         0.4826, 0.4706, 0.4961, 0.5145, 0.5090, 0.0000],\n",
      "        [0.4912, 0.4841, 0.0000, 0.0000, 0.0000, 0.0000, 0.4783, 0.4975, 0.5205,\n",
      "         0.5134, 0.0000, 0.0000, 0.4955, 0.0000, 0.4848, 0.4913, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4985, 0.5016, 0.4994, 0.4996, 0.5168, 0.4882, 0.5257,\n",
      "         0.4911, 0.4811, 0.5079, 0.5254, 0.5182, 0.0000],\n",
      "        [0.4928, 0.4829, 0.0000, 0.0000, 0.0000, 0.0000, 0.4740, 0.5016, 0.5212,\n",
      "         0.5096, 0.0000, 0.0000, 0.4939, 0.0000, 0.4823, 0.4884, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4989, 0.5068, 0.4979, 0.4971, 0.5204, 0.4906, 0.5237,\n",
      "         0.4882, 0.4830, 0.5035, 0.5218, 0.5171, 0.0000],\n",
      "        [0.4938, 0.4853, 0.0000, 0.0000, 0.0000, 0.0000, 0.4780, 0.5009, 0.5217,\n",
      "         0.5129, 0.0000, 0.0000, 0.4974, 0.0000, 0.4857, 0.4919, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5008, 0.5058, 0.5014, 0.4994, 0.5219, 0.4915, 0.5264,\n",
      "         0.4917, 0.4850, 0.5076, 0.5263, 0.5200, 0.0000]], device='cuda:0')\n",
      "pred tensor([[0.4832, 0.4732, 0.0000, 0.0000, 0.0000, 0.0000, 0.4675, 0.4949, 0.5091,\n",
      "         0.5027, 0.0000, 0.0000, 0.4774, 0.0000, 0.4707, 0.4778, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4903, 0.4909, 0.4800, 0.4883, 0.5023, 0.4678, 0.5060,\n",
      "         0.4754, 0.4627, 0.4940, 0.5111, 0.5018, 0.0000],\n",
      "        [0.4926, 0.4825, 0.0000, 0.0000, 0.0000, 0.0000, 0.4721, 0.5007, 0.5193,\n",
      "         0.5084, 0.0000, 0.0000, 0.4911, 0.0000, 0.4806, 0.4876, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4973, 0.5051, 0.4949, 0.4963, 0.5176, 0.4872, 0.5216,\n",
      "         0.4863, 0.4787, 0.5026, 0.5200, 0.5155, 0.0000],\n",
      "        [0.4946, 0.4838, 0.0000, 0.0000, 0.0000, 0.0000, 0.4743, 0.5019, 0.5182,\n",
      "         0.5088, 0.0000, 0.0000, 0.4931, 0.0000, 0.4812, 0.4885, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4979, 0.5036, 0.4965, 0.4973, 0.5189, 0.4872, 0.5219,\n",
      "         0.4872, 0.4799, 0.5031, 0.5206, 0.5164, 0.0000],\n",
      "        [0.4979, 0.4866, 0.0000, 0.0000, 0.0000, 0.0000, 0.4744, 0.5043, 0.5211,\n",
      "         0.5092, 0.0000, 0.0000, 0.4954, 0.0000, 0.4848, 0.4906, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5001, 0.5089, 0.4988, 0.4984, 0.5216, 0.4914, 0.5262,\n",
      "         0.4878, 0.4824, 0.5054, 0.5226, 0.5194, 0.0000],\n",
      "        [0.4844, 0.4758, 0.0000, 0.0000, 0.0000, 0.0000, 0.4683, 0.4953, 0.5129,\n",
      "         0.5045, 0.0000, 0.0000, 0.4821, 0.0000, 0.4742, 0.4814, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4912, 0.4952, 0.4843, 0.4911, 0.5066, 0.4740, 0.5124,\n",
      "         0.4796, 0.4672, 0.4969, 0.5138, 0.5063, 0.0000],\n",
      "        [0.4935, 0.4828, 0.0000, 0.0000, 0.0000, 0.0000, 0.4738, 0.5004, 0.5183,\n",
      "         0.5078, 0.0000, 0.0000, 0.4903, 0.0000, 0.4803, 0.4871, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4962, 0.5017, 0.4951, 0.4957, 0.5158, 0.4854, 0.5218,\n",
      "         0.4857, 0.4765, 0.5033, 0.5197, 0.5141, 0.0000],\n",
      "        [0.4970, 0.4865, 0.0000, 0.0000, 0.0000, 0.0000, 0.4707, 0.5050, 0.5208,\n",
      "         0.5090, 0.0000, 0.0000, 0.4934, 0.0000, 0.4814, 0.4901, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4988, 0.5084, 0.4996, 0.4981, 0.5236, 0.4918, 0.5259,\n",
      "         0.4896, 0.4815, 0.5029, 0.5208, 0.5192, 0.0000],\n",
      "        [0.4924, 0.4857, 0.0000, 0.0000, 0.0000, 0.0000, 0.4790, 0.4989, 0.5215,\n",
      "         0.5124, 0.0000, 0.0000, 0.4996, 0.0000, 0.4872, 0.4930, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4996, 0.5076, 0.5026, 0.5016, 0.5215, 0.4937, 0.5303,\n",
      "         0.4906, 0.4863, 0.5073, 0.5276, 0.5232, 0.0000],\n",
      "        [0.4865, 0.4799, 0.0000, 0.0000, 0.0000, 0.0000, 0.4749, 0.4939, 0.5171,\n",
      "         0.5107, 0.0000, 0.0000, 0.4896, 0.0000, 0.4789, 0.4873, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4940, 0.4967, 0.4944, 0.4970, 0.5117, 0.4828, 0.5209,\n",
      "         0.4880, 0.4749, 0.5034, 0.5207, 0.5134, 0.0000],\n",
      "        [0.4858, 0.4792, 0.0000, 0.0000, 0.0000, 0.0000, 0.4712, 0.4973, 0.5170,\n",
      "         0.5075, 0.0000, 0.0000, 0.4924, 0.0000, 0.4810, 0.4862, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4955, 0.5032, 0.4930, 0.4972, 0.5168, 0.4839, 0.5213,\n",
      "         0.4853, 0.4791, 0.4982, 0.5195, 0.5161, 0.0000]], device='cuda:0')\n",
      "pred tensor([[0.4908, 0.4834, 0.0000, 0.0000, 0.0000, 0.0000, 0.4802, 0.4972, 0.5200,\n",
      "         0.5126, 0.0000, 0.0000, 0.4984, 0.0000, 0.4866, 0.4918, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4998, 0.5048, 0.4995, 0.5004, 0.5185, 0.4904, 0.5269,\n",
      "         0.4894, 0.4848, 0.5066, 0.5267, 0.5203, 0.0000],\n",
      "        [0.4932, 0.4845, 0.0000, 0.0000, 0.0000, 0.0000, 0.4774, 0.4992, 0.5197,\n",
      "         0.5109, 0.0000, 0.0000, 0.4950, 0.0000, 0.4843, 0.4905, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4982, 0.5035, 0.4985, 0.4986, 0.5176, 0.4884, 0.5252,\n",
      "         0.4885, 0.4807, 0.5068, 0.5243, 0.5182, 0.0000],\n",
      "        [0.4916, 0.4827, 0.0000, 0.0000, 0.0000, 0.0000, 0.4780, 0.4978, 0.5201,\n",
      "         0.5089, 0.0000, 0.0000, 0.4949, 0.0000, 0.4840, 0.4895, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4961, 0.5039, 0.4961, 0.4973, 0.5153, 0.4889, 0.5258,\n",
      "         0.4860, 0.4805, 0.5062, 0.5229, 0.5166, 0.0000],\n",
      "        [0.5008, 0.4897, 0.0000, 0.0000, 0.0000, 0.0000, 0.4789, 0.5038, 0.5272,\n",
      "         0.5127, 0.0000, 0.0000, 0.5009, 0.0000, 0.4887, 0.4947, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5015, 0.5119, 0.5063, 0.5014, 0.5261, 0.5003, 0.5340,\n",
      "         0.4943, 0.4883, 0.5117, 0.5270, 0.5237, 0.0000],\n",
      "        [0.4900, 0.4823, 0.0000, 0.0000, 0.0000, 0.0000, 0.4771, 0.4983, 0.5190,\n",
      "         0.5106, 0.0000, 0.0000, 0.4952, 0.0000, 0.4833, 0.4904, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4979, 0.5042, 0.4962, 0.4975, 0.5173, 0.4884, 0.5239,\n",
      "         0.4876, 0.4821, 0.5051, 0.5239, 0.5173, 0.0000],\n",
      "        [0.4937, 0.4862, 0.0000, 0.0000, 0.0000, 0.0000, 0.4784, 0.5000, 0.5236,\n",
      "         0.5135, 0.0000, 0.0000, 0.5008, 0.0000, 0.4869, 0.4938, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5003, 0.5088, 0.5041, 0.5021, 0.5243, 0.4966, 0.5311,\n",
      "         0.4939, 0.4885, 0.5079, 0.5269, 0.5235, 0.0000],\n",
      "        [0.4919, 0.4839, 0.0000, 0.0000, 0.0000, 0.0000, 0.4805, 0.4978, 0.5179,\n",
      "         0.5104, 0.0000, 0.0000, 0.4955, 0.0000, 0.4854, 0.4915, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4968, 0.5011, 0.4947, 0.4972, 0.5129, 0.4856, 0.5245,\n",
      "         0.4856, 0.4789, 0.5086, 0.5248, 0.5162, 0.0000],\n",
      "        [0.4935, 0.4821, 0.0000, 0.0000, 0.0000, 0.0000, 0.4720, 0.5021, 0.5179,\n",
      "         0.5069, 0.0000, 0.0000, 0.4905, 0.0000, 0.4800, 0.4862, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4977, 0.5050, 0.4944, 0.4955, 0.5180, 0.4862, 0.5203,\n",
      "         0.4843, 0.4787, 0.5006, 0.5190, 0.5148, 0.0000],\n",
      "        [0.4989, 0.4915, 0.0000, 0.0000, 0.0000, 0.0000, 0.4835, 0.5013, 0.5298,\n",
      "         0.5172, 0.0000, 0.0000, 0.5086, 0.0000, 0.4922, 0.5004, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5030, 0.5145, 0.5133, 0.5059, 0.5321, 0.5078, 0.5413,\n",
      "         0.5013, 0.4965, 0.5153, 0.5332, 0.5304, 0.0000],\n",
      "        [0.4983, 0.4897, 0.0000, 0.0000, 0.0000, 0.0000, 0.4794, 0.5025, 0.5266,\n",
      "         0.5148, 0.0000, 0.0000, 0.5019, 0.0000, 0.4894, 0.4961, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5023, 0.5112, 0.5072, 0.5023, 0.5267, 0.4995, 0.5340,\n",
      "         0.4960, 0.4893, 0.5124, 0.5294, 0.5251, 0.0000]], device='cuda:0')\n",
      "pred tensor([[0.4867, 0.4803, 0.0000, 0.0000, 0.0000, 0.0000, 0.4775, 0.4945, 0.5190,\n",
      "         0.5107, 0.0000, 0.0000, 0.4943, 0.0000, 0.4821, 0.4896, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4954, 0.5014, 0.4953, 0.4973, 0.5148, 0.4872, 0.5238,\n",
      "         0.4881, 0.4804, 0.5052, 0.5233, 0.5159, 0.0000],\n",
      "        [0.4912, 0.4836, 0.0000, 0.0000, 0.0000, 0.0000, 0.4779, 0.4997, 0.5212,\n",
      "         0.5119, 0.0000, 0.0000, 0.4999, 0.0000, 0.4859, 0.4914, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5000, 0.5073, 0.5008, 0.5010, 0.5225, 0.4932, 0.5277,\n",
      "         0.4908, 0.4880, 0.5044, 0.5251, 0.5214, 0.0000],\n",
      "        [0.4863, 0.4772, 0.0000, 0.0000, 0.0000, 0.0000, 0.4684, 0.4975, 0.5118,\n",
      "         0.5046, 0.0000, 0.0000, 0.4831, 0.0000, 0.4744, 0.4819, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4929, 0.4971, 0.4859, 0.4922, 0.5086, 0.4751, 0.5123,\n",
      "         0.4789, 0.4692, 0.4961, 0.5147, 0.5084, 0.0000],\n",
      "        [0.4923, 0.4827, 0.0000, 0.0000, 0.0000, 0.0000, 0.4739, 0.5000, 0.5204,\n",
      "         0.5106, 0.0000, 0.0000, 0.4924, 0.0000, 0.4806, 0.4887, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4979, 0.5038, 0.4973, 0.4971, 0.5188, 0.4889, 0.5225,\n",
      "         0.4896, 0.4806, 0.5041, 0.5212, 0.5158, 0.0000],\n",
      "        [0.4896, 0.4793, 0.0000, 0.0000, 0.0000, 0.0000, 0.4732, 0.4981, 0.5152,\n",
      "         0.5052, 0.0000, 0.0000, 0.4867, 0.0000, 0.4781, 0.4844, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4940, 0.4995, 0.4888, 0.4924, 0.5101, 0.4802, 0.5171,\n",
      "         0.4801, 0.4726, 0.5010, 0.5178, 0.5100, 0.0000],\n",
      "        [0.4825, 0.4751, 0.0000, 0.0000, 0.0000, 0.0000, 0.4729, 0.4931, 0.5131,\n",
      "         0.5057, 0.0000, 0.0000, 0.4855, 0.0000, 0.4771, 0.4826, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4923, 0.4955, 0.4859, 0.4920, 0.5069, 0.4753, 0.5141,\n",
      "         0.4796, 0.4707, 0.4985, 0.5175, 0.5082, 0.0000],\n",
      "        [0.4972, 0.4880, 0.0000, 0.0000, 0.0000, 0.0000, 0.4786, 0.5021, 0.5244,\n",
      "         0.5134, 0.0000, 0.0000, 0.4999, 0.0000, 0.4873, 0.4934, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5015, 0.5093, 0.5062, 0.5019, 0.5253, 0.4972, 0.5314,\n",
      "         0.4938, 0.4881, 0.5091, 0.5276, 0.5238, 0.0000],\n",
      "        [0.4806, 0.4726, 0.0000, 0.0000, 0.0000, 0.0000, 0.4654, 0.4939, 0.5091,\n",
      "         0.5026, 0.0000, 0.0000, 0.4781, 0.0000, 0.4704, 0.4784, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4895, 0.4924, 0.4800, 0.4891, 0.5037, 0.4688, 0.5074,\n",
      "         0.4762, 0.4636, 0.4923, 0.5110, 0.5033, 0.0000],\n",
      "        [0.4832, 0.4749, 0.0000, 0.0000, 0.0000, 0.0000, 0.4708, 0.4947, 0.5130,\n",
      "         0.5042, 0.0000, 0.0000, 0.4844, 0.0000, 0.4760, 0.4817, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4922, 0.4973, 0.4845, 0.4912, 0.5071, 0.4756, 0.5134,\n",
      "         0.4780, 0.4704, 0.4968, 0.5157, 0.5076, 0.0000],\n",
      "        [0.4828, 0.4750, 0.0000, 0.0000, 0.0000, 0.0000, 0.4695, 0.4951, 0.5113,\n",
      "         0.5057, 0.0000, 0.0000, 0.4836, 0.0000, 0.4753, 0.4817, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4934, 0.4959, 0.4839, 0.4921, 0.5068, 0.4733, 0.5110,\n",
      "         0.4787, 0.4696, 0.4956, 0.5154, 0.5075, 0.0000]], device='cuda:0')\n",
      "pred tensor([[0.4949, 0.4881, 0.0000, 0.0000, 0.0000, 0.0000, 0.4798, 0.4995, 0.5262,\n",
      "         0.5150, 0.0000, 0.0000, 0.5034, 0.0000, 0.4888, 0.4963, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5006, 0.5103, 0.5076, 0.5037, 0.5268, 0.5003, 0.5352,\n",
      "         0.4973, 0.4905, 0.5110, 0.5293, 0.5261, 0.0000],\n",
      "        [0.4933, 0.4825, 0.0000, 0.0000, 0.0000, 0.0000, 0.4741, 0.5014, 0.5159,\n",
      "         0.5076, 0.0000, 0.0000, 0.4899, 0.0000, 0.4805, 0.4874, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4973, 0.5014, 0.4922, 0.4947, 0.5146, 0.4828, 0.5194,\n",
      "         0.4834, 0.4759, 0.5022, 0.5198, 0.5131, 0.0000],\n",
      "        [0.4884, 0.4795, 0.0000, 0.0000, 0.0000, 0.0000, 0.4716, 0.4972, 0.5186,\n",
      "         0.5076, 0.0000, 0.0000, 0.4882, 0.0000, 0.4792, 0.4855, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4948, 0.5014, 0.4919, 0.4944, 0.5131, 0.4834, 0.5199,\n",
      "         0.4848, 0.4749, 0.5017, 0.5187, 0.5122, 0.0000],\n",
      "        [0.4821, 0.4728, 0.0000, 0.0000, 0.0000, 0.0000, 0.4661, 0.4962, 0.5099,\n",
      "         0.5014, 0.0000, 0.0000, 0.4797, 0.0000, 0.4723, 0.4779, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4909, 0.4958, 0.4793, 0.4888, 0.5043, 0.4702, 0.5072,\n",
      "         0.4738, 0.4662, 0.4922, 0.5111, 0.5038, 0.0000],\n",
      "        [0.4842, 0.4762, 0.0000, 0.0000, 0.0000, 0.0000, 0.4722, 0.4938, 0.5161,\n",
      "         0.5073, 0.0000, 0.0000, 0.4877, 0.0000, 0.4760, 0.4841, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4928, 0.4984, 0.4901, 0.4942, 0.5116, 0.4816, 0.5167,\n",
      "         0.4844, 0.4750, 0.4993, 0.5173, 0.5107, 0.0000],\n",
      "        [0.4832, 0.4735, 0.0000, 0.0000, 0.0000, 0.0000, 0.4693, 0.4942, 0.5098,\n",
      "         0.5019, 0.0000, 0.0000, 0.4802, 0.0000, 0.4726, 0.4781, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4895, 0.4925, 0.4816, 0.4901, 0.5026, 0.4706, 0.5094,\n",
      "         0.4748, 0.4652, 0.4941, 0.5116, 0.5041, 0.0000],\n",
      "        [0.4915, 0.4840, 0.0000, 0.0000, 0.0000, 0.0000, 0.4784, 0.4982, 0.5201,\n",
      "         0.5118, 0.0000, 0.0000, 0.4973, 0.0000, 0.4850, 0.4918, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4983, 0.5040, 0.4988, 0.4993, 0.5188, 0.4897, 0.5264,\n",
      "         0.4899, 0.4831, 0.5069, 0.5252, 0.5192, 0.0000],\n",
      "        [0.4863, 0.4786, 0.0000, 0.0000, 0.0000, 0.0000, 0.4759, 0.4934, 0.5172,\n",
      "         0.5082, 0.0000, 0.0000, 0.4890, 0.0000, 0.4800, 0.4863, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4930, 0.4972, 0.4907, 0.4942, 0.5093, 0.4811, 0.5200,\n",
      "         0.4844, 0.4733, 0.5043, 0.5202, 0.5111, 0.0000],\n",
      "        [0.5010, 0.4915, 0.0000, 0.0000, 0.0000, 0.0000, 0.4803, 0.5044, 0.5301,\n",
      "         0.5155, 0.0000, 0.0000, 0.5058, 0.0000, 0.4908, 0.4979, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5034, 0.5154, 0.5117, 0.5043, 0.5319, 0.5066, 0.5384,\n",
      "         0.4994, 0.4949, 0.5135, 0.5302, 0.5283, 0.0000],\n",
      "        [0.4887, 0.4781, 0.0000, 0.0000, 0.0000, 0.0000, 0.4693, 0.4986, 0.5168,\n",
      "         0.5049, 0.0000, 0.0000, 0.4860, 0.0000, 0.4763, 0.4830, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4937, 0.5009, 0.4892, 0.4927, 0.5129, 0.4817, 0.5170,\n",
      "         0.4823, 0.4734, 0.4981, 0.5151, 0.5098, 0.0000]], device='cuda:0')\n",
      "pred tensor([[0.4892, 0.4816, 0.0000, 0.0000, 0.0000, 0.0000, 0.4763, 0.4985, 0.5166,\n",
      "         0.5101, 0.0000, 0.0000, 0.4931, 0.0000, 0.4840, 0.4884, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4987, 0.5018, 0.4935, 0.4969, 0.5144, 0.4827, 0.5205,\n",
      "         0.4847, 0.4789, 0.5034, 0.5234, 0.5157, 0.0000],\n",
      "        [0.4904, 0.4816, 0.0000, 0.0000, 0.0000, 0.0000, 0.4743, 0.4989, 0.5154,\n",
      "         0.5081, 0.0000, 0.0000, 0.4907, 0.0000, 0.4815, 0.4869, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4966, 0.5005, 0.4923, 0.4962, 0.5132, 0.4811, 0.5191,\n",
      "         0.4834, 0.4757, 0.5022, 0.5207, 0.5144, 0.0000],\n",
      "        [0.4859, 0.4761, 0.0000, 0.0000, 0.0000, 0.0000, 0.4695, 0.4977, 0.5124,\n",
      "         0.5042, 0.0000, 0.0000, 0.4850, 0.0000, 0.4744, 0.4815, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4925, 0.4967, 0.4857, 0.4922, 0.5096, 0.4762, 0.5122,\n",
      "         0.4797, 0.4715, 0.4954, 0.5134, 0.5077, 0.0000],\n",
      "        [0.4942, 0.4851, 0.0000, 0.0000, 0.0000, 0.0000, 0.4778, 0.4992, 0.5221,\n",
      "         0.5119, 0.0000, 0.0000, 0.4965, 0.0000, 0.4842, 0.4912, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4983, 0.5046, 0.5017, 0.4999, 0.5203, 0.4924, 0.5277,\n",
      "         0.4917, 0.4833, 0.5076, 0.5244, 0.5196, 0.0000],\n",
      "        [0.4821, 0.4746, 0.0000, 0.0000, 0.0000, 0.0000, 0.4695, 0.4952, 0.5094,\n",
      "         0.5045, 0.0000, 0.0000, 0.4830, 0.0000, 0.4747, 0.4813, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4922, 0.4943, 0.4819, 0.4912, 0.5051, 0.4708, 0.5096,\n",
      "         0.4768, 0.4681, 0.4948, 0.5146, 0.5064, 0.0000],\n",
      "        [0.4792, 0.4726, 0.0000, 0.0000, 0.0000, 0.0000, 0.4688, 0.4924, 0.5129,\n",
      "         0.5046, 0.0000, 0.0000, 0.4835, 0.0000, 0.4743, 0.4810, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4910, 0.4965, 0.4833, 0.4910, 0.5068, 0.4748, 0.5123,\n",
      "         0.4791, 0.4701, 0.4950, 0.5149, 0.5068, 0.0000],\n",
      "        [0.4920, 0.4836, 0.0000, 0.0000, 0.0000, 0.0000, 0.4762, 0.4982, 0.5230,\n",
      "         0.5112, 0.0000, 0.0000, 0.4958, 0.0000, 0.4838, 0.4903, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4974, 0.5054, 0.5003, 0.4990, 0.5200, 0.4923, 0.5274,\n",
      "         0.4914, 0.4829, 0.5068, 0.5236, 0.5188, 0.0000],\n",
      "        [0.4811, 0.4724, 0.0000, 0.0000, 0.0000, 0.0000, 0.4680, 0.4946, 0.5106,\n",
      "         0.5023, 0.0000, 0.0000, 0.4816, 0.0000, 0.4718, 0.4790, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4901, 0.4953, 0.4814, 0.4896, 0.5058, 0.4729, 0.5088,\n",
      "         0.4762, 0.4687, 0.4930, 0.5119, 0.5047, 0.0000],\n",
      "        [0.4850, 0.4784, 0.0000, 0.0000, 0.0000, 0.0000, 0.4740, 0.4954, 0.5164,\n",
      "         0.5076, 0.0000, 0.0000, 0.4922, 0.0000, 0.4803, 0.4868, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4944, 0.5017, 0.4916, 0.4960, 0.5141, 0.4837, 0.5205,\n",
      "         0.4843, 0.4786, 0.5003, 0.5202, 0.5145, 0.0000],\n",
      "        [0.4886, 0.4787, 0.0000, 0.0000, 0.0000, 0.0000, 0.4722, 0.4970, 0.5169,\n",
      "         0.5060, 0.0000, 0.0000, 0.4873, 0.0000, 0.4781, 0.4842, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4939, 0.5000, 0.4902, 0.4932, 0.5121, 0.4813, 0.5177,\n",
      "         0.4827, 0.4735, 0.5010, 0.5177, 0.5108, 0.0000]], device='cuda:0')\n",
      "pred tensor([[0.4862, 0.4799, 0.0000, 0.0000, 0.0000, 0.0000, 0.4752, 0.4951, 0.5187,\n",
      "         0.5099, 0.0000, 0.0000, 0.4928, 0.0000, 0.4814, 0.4883, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4953, 0.5017, 0.4946, 0.4969, 0.5150, 0.4858, 0.5226,\n",
      "         0.4874, 0.4792, 0.5035, 0.5222, 0.5155, 0.0000],\n",
      "        [0.4943, 0.4852, 0.0000, 0.0000, 0.0000, 0.0000, 0.4774, 0.5011, 0.5227,\n",
      "         0.5112, 0.0000, 0.0000, 0.4975, 0.0000, 0.4852, 0.4919, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4997, 0.5087, 0.5010, 0.4988, 0.5222, 0.4944, 0.5282,\n",
      "         0.4902, 0.4858, 0.5072, 0.5255, 0.5204, 0.0000],\n",
      "        [0.4871, 0.4771, 0.0000, 0.0000, 0.0000, 0.0000, 0.4712, 0.4973, 0.5153,\n",
      "         0.5061, 0.0000, 0.0000, 0.4857, 0.0000, 0.4761, 0.4827, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4940, 0.4984, 0.4884, 0.4927, 0.5105, 0.4795, 0.5145,\n",
      "         0.4817, 0.4732, 0.4986, 0.5159, 0.5088, 0.0000],\n",
      "        [0.4961, 0.4866, 0.0000, 0.0000, 0.0000, 0.0000, 0.4794, 0.5011, 0.5267,\n",
      "         0.5134, 0.0000, 0.0000, 0.5009, 0.0000, 0.4865, 0.4937, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5008, 0.5111, 0.5063, 0.5013, 0.5258, 0.5012, 0.5326,\n",
      "         0.4948, 0.4907, 0.5096, 0.5270, 0.5231, 0.0000],\n",
      "        [0.4905, 0.4809, 0.0000, 0.0000, 0.0000, 0.0000, 0.4731, 0.4976, 0.5187,\n",
      "         0.5077, 0.0000, 0.0000, 0.4885, 0.0000, 0.4805, 0.4860, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4953, 0.5017, 0.4926, 0.4949, 0.5123, 0.4836, 0.5206,\n",
      "         0.4840, 0.4744, 0.5036, 0.5197, 0.5129, 0.0000],\n",
      "        [0.4906, 0.4808, 0.0000, 0.0000, 0.0000, 0.0000, 0.4717, 0.4997, 0.5171,\n",
      "         0.5062, 0.0000, 0.0000, 0.4896, 0.0000, 0.4793, 0.4860, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4955, 0.5040, 0.4922, 0.4949, 0.5149, 0.4848, 0.5203,\n",
      "         0.4827, 0.4768, 0.5004, 0.5186, 0.5140, 0.0000],\n",
      "        [0.4772, 0.4718, 0.0000, 0.0000, 0.0000, 0.0000, 0.4704, 0.4905, 0.5100,\n",
      "         0.5060, 0.0000, 0.0000, 0.4826, 0.0000, 0.4733, 0.4803, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4904, 0.4914, 0.4821, 0.4915, 0.5031, 0.4701, 0.5084,\n",
      "         0.4787, 0.4680, 0.4953, 0.5149, 0.5054, 0.0000],\n",
      "        [0.4845, 0.4762, 0.0000, 0.0000, 0.0000, 0.0000, 0.4679, 0.4981, 0.5148,\n",
      "         0.5057, 0.0000, 0.0000, 0.4872, 0.0000, 0.4762, 0.4822, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4948, 0.5012, 0.4884, 0.4942, 0.5135, 0.4798, 0.5143,\n",
      "         0.4819, 0.4758, 0.4945, 0.5151, 0.5111, 0.0000],\n",
      "        [0.4984, 0.4882, 0.0000, 0.0000, 0.0000, 0.0000, 0.4834, 0.5028, 0.5221,\n",
      "         0.5136, 0.0000, 0.0000, 0.5023, 0.0000, 0.4887, 0.4948, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5017, 0.5057, 0.5037, 0.5011, 0.5231, 0.4946, 0.5297,\n",
      "         0.4928, 0.4884, 0.5109, 0.5277, 0.5217, 0.0000],\n",
      "        [0.4878, 0.4795, 0.0000, 0.0000, 0.0000, 0.0000, 0.4700, 0.4970, 0.5179,\n",
      "         0.5080, 0.0000, 0.0000, 0.4890, 0.0000, 0.4774, 0.4858, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4944, 0.5014, 0.4936, 0.4961, 0.5161, 0.4846, 0.5201,\n",
      "         0.4873, 0.4763, 0.4996, 0.5178, 0.5138, 0.0000]], device='cuda:0')\n",
      "pred tensor([[0.4843, 0.4746, 0.0000, 0.0000, 0.0000, 0.0000, 0.4662, 0.4958, 0.5099,\n",
      "         0.5019, 0.0000, 0.0000, 0.4782, 0.0000, 0.4718, 0.4789, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4904, 0.4943, 0.4808, 0.4890, 0.5038, 0.4703, 0.5092,\n",
      "         0.4750, 0.4633, 0.4937, 0.5116, 0.5040, 0.0000],\n",
      "        [0.4942, 0.4844, 0.0000, 0.0000, 0.0000, 0.0000, 0.4777, 0.4999, 0.5188,\n",
      "         0.5096, 0.0000, 0.0000, 0.4939, 0.0000, 0.4841, 0.4897, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4977, 0.5026, 0.4966, 0.4974, 0.5157, 0.4868, 0.5242,\n",
      "         0.4863, 0.4790, 0.5066, 0.5232, 0.5166, 0.0000],\n",
      "        [0.4861, 0.4779, 0.0000, 0.0000, 0.0000, 0.0000, 0.4691, 0.4978, 0.5170,\n",
      "         0.5060, 0.0000, 0.0000, 0.4887, 0.0000, 0.4774, 0.4842, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4940, 0.5023, 0.4906, 0.4946, 0.5151, 0.4828, 0.5181,\n",
      "         0.4841, 0.4764, 0.4975, 0.5165, 0.5125, 0.0000],\n",
      "        [0.4969, 0.4887, 0.0000, 0.0000, 0.0000, 0.0000, 0.4788, 0.5035, 0.5226,\n",
      "         0.5133, 0.0000, 0.0000, 0.5023, 0.0000, 0.4880, 0.4956, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5020, 0.5103, 0.5047, 0.5022, 0.5266, 0.4969, 0.5316,\n",
      "         0.4934, 0.4897, 0.5083, 0.5279, 0.5249, 0.0000],\n",
      "        [0.4857, 0.4761, 0.0000, 0.0000, 0.0000, 0.0000, 0.4673, 0.4966, 0.5133,\n",
      "         0.5041, 0.0000, 0.0000, 0.4821, 0.0000, 0.4730, 0.4808, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4914, 0.4963, 0.4859, 0.4916, 0.5088, 0.4761, 0.5129,\n",
      "         0.4803, 0.4686, 0.4954, 0.5127, 0.5070, 0.0000],\n",
      "        [0.4887, 0.4809, 0.0000, 0.0000, 0.0000, 0.0000, 0.4753, 0.4969, 0.5205,\n",
      "         0.5093, 0.0000, 0.0000, 0.4936, 0.0000, 0.4825, 0.4889, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4960, 0.5044, 0.4949, 0.4963, 0.5162, 0.4882, 0.5238,\n",
      "         0.4872, 0.4804, 0.5049, 0.5223, 0.5158, 0.0000],\n",
      "        [0.4792, 0.4710, 0.0000, 0.0000, 0.0000, 0.0000, 0.4676, 0.4919, 0.5104,\n",
      "         0.5038, 0.0000, 0.0000, 0.4780, 0.0000, 0.4696, 0.4778, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4891, 0.4909, 0.4804, 0.4887, 0.5021, 0.4699, 0.5066,\n",
      "         0.4773, 0.4646, 0.4937, 0.5112, 0.5017, 0.0000],\n",
      "        [0.4776, 0.4707, 0.0000, 0.0000, 0.0000, 0.0000, 0.4654, 0.4918, 0.5070,\n",
      "         0.5025, 0.0000, 0.0000, 0.4758, 0.0000, 0.4690, 0.4765, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4880, 0.4883, 0.4773, 0.4884, 0.4991, 0.4642, 0.5042,\n",
      "         0.4745, 0.4604, 0.4913, 0.5098, 0.5007, 0.0000],\n",
      "        [0.4875, 0.4798, 0.0000, 0.0000, 0.0000, 0.0000, 0.4752, 0.4952, 0.5200,\n",
      "         0.5092, 0.0000, 0.0000, 0.4933, 0.0000, 0.4802, 0.4882, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4946, 0.5025, 0.4960, 0.4970, 0.5170, 0.4887, 0.5241,\n",
      "         0.4888, 0.4804, 0.5032, 0.5211, 0.5156, 0.0000],\n",
      "        [0.4874, 0.4803, 0.0000, 0.0000, 0.0000, 0.0000, 0.4728, 0.4978, 0.5185,\n",
      "         0.5098, 0.0000, 0.0000, 0.4924, 0.0000, 0.4801, 0.4881, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4966, 0.5027, 0.4950, 0.4966, 0.5181, 0.4861, 0.5213,\n",
      "         0.4885, 0.4801, 0.5015, 0.5211, 0.5155, 0.0000]], device='cuda:0')\n",
      "pred tensor([[0.4888, 0.4796, 0.0000, 0.0000, 0.0000, 0.0000, 0.4677, 0.4994, 0.5177,\n",
      "         0.5061, 0.0000, 0.0000, 0.4872, 0.0000, 0.4764, 0.4843, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4941, 0.5025, 0.4919, 0.4945, 0.5157, 0.4837, 0.5185,\n",
      "         0.4849, 0.4751, 0.4982, 0.5159, 0.5125, 0.0000],\n",
      "        [0.4904, 0.4823, 0.0000, 0.0000, 0.0000, 0.0000, 0.4715, 0.5005, 0.5177,\n",
      "         0.5083, 0.0000, 0.0000, 0.4918, 0.0000, 0.4816, 0.4880, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4977, 0.5055, 0.4944, 0.4968, 0.5178, 0.4856, 0.5219,\n",
      "         0.4852, 0.4789, 0.5010, 0.5212, 0.5169, 0.0000],\n",
      "        [0.4855, 0.4775, 0.0000, 0.0000, 0.0000, 0.0000, 0.4728, 0.4957, 0.5160,\n",
      "         0.5077, 0.0000, 0.0000, 0.4888, 0.0000, 0.4778, 0.4852, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4943, 0.4995, 0.4902, 0.4944, 0.5122, 0.4819, 0.5178,\n",
      "         0.4840, 0.4759, 0.4996, 0.5183, 0.5114, 0.0000],\n",
      "        [0.4795, 0.4714, 0.0000, 0.0000, 0.0000, 0.0000, 0.4704, 0.4919, 0.5096,\n",
      "         0.5033, 0.0000, 0.0000, 0.4799, 0.0000, 0.4721, 0.4784, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4892, 0.4909, 0.4792, 0.4888, 0.5002, 0.4689, 0.5070,\n",
      "         0.4750, 0.4651, 0.4951, 0.5123, 0.5020, 0.0000],\n",
      "        [0.4848, 0.4762, 0.0000, 0.0000, 0.0000, 0.0000, 0.4688, 0.4963, 0.5154,\n",
      "         0.5056, 0.0000, 0.0000, 0.4857, 0.0000, 0.4751, 0.4825, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4930, 0.4998, 0.4883, 0.4933, 0.5116, 0.4803, 0.5153,\n",
      "         0.4822, 0.4736, 0.4964, 0.5151, 0.5099, 0.0000],\n",
      "        [0.5056, 0.4931, 0.0000, 0.0000, 0.0000, 0.0000, 0.4800, 0.5080, 0.5270,\n",
      "         0.5130, 0.0000, 0.0000, 0.5033, 0.0000, 0.4902, 0.4966, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5035, 0.5140, 0.5089, 0.5027, 0.5292, 0.5026, 0.5354,\n",
      "         0.4948, 0.4908, 0.5130, 0.5281, 0.5261, 0.0000],\n",
      "        [0.4895, 0.4819, 0.0000, 0.0000, 0.0000, 0.0000, 0.4743, 0.4984, 0.5188,\n",
      "         0.5099, 0.0000, 0.0000, 0.4931, 0.0000, 0.4817, 0.4882, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4970, 0.5031, 0.4971, 0.4981, 0.5174, 0.4871, 0.5232,\n",
      "         0.4878, 0.4805, 0.5027, 0.5221, 0.5171, 0.0000],\n",
      "        [0.4944, 0.4842, 0.0000, 0.0000, 0.0000, 0.0000, 0.4736, 0.5015, 0.5189,\n",
      "         0.5087, 0.0000, 0.0000, 0.4919, 0.0000, 0.4818, 0.4882, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4978, 0.5043, 0.4972, 0.4973, 0.5182, 0.4875, 0.5235,\n",
      "         0.4866, 0.4788, 0.5034, 0.5213, 0.5169, 0.0000],\n",
      "        [0.4844, 0.4770, 0.0000, 0.0000, 0.0000, 0.0000, 0.4700, 0.4955, 0.5153,\n",
      "         0.5066, 0.0000, 0.0000, 0.4859, 0.0000, 0.4762, 0.4838, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4932, 0.4991, 0.4889, 0.4932, 0.5111, 0.4796, 0.5161,\n",
      "         0.4826, 0.4729, 0.4985, 0.5174, 0.5105, 0.0000],\n",
      "        [0.4953, 0.4859, 0.0000, 0.0000, 0.0000, 0.0000, 0.4804, 0.4995, 0.5242,\n",
      "         0.5128, 0.0000, 0.0000, 0.4997, 0.0000, 0.4859, 0.4934, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4994, 0.5072, 0.5039, 0.5003, 0.5231, 0.4970, 0.5309,\n",
      "         0.4936, 0.4872, 0.5097, 0.5265, 0.5212, 0.0000]], device='cuda:0')\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test(test_loader, model, criterion)\n",
    "\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2, 68)\n",
    "b = torch.randn(2, 2, 68)\n",
    "c = criterion(a,b)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
