{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Facial Recognition using PyTorch and OpenCV\n",
    "\n",
    "https://ritik12.medium.com/facial-recognition-using-pytorch-and-opencv-467c4e41d1f\n",
    "\n",
    "\n",
    "Machine Learning - Face Recognition CNN Pytorch.ipynb\n",
    "https://github.com/rubencg195/Pytorch-Tutorials/blob/master/Machine%20Learning%20-%20Face%20Recognition%20CNN%20Pytorch.ipynb\n",
    "\n",
    "\n",
    "\n",
    "Face Recognition Using Pytorch\n",
    "https://github.com/timesler/facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face Landmarks Detection With PyTorch\n",
    "\n",
    "https://towardsdatascience.com/face-landmarks-detection-with-pytorch-4b4852f5e9c4\n",
    "\n",
    "\n",
    "\n",
    "다중입력 deep neural network\n",
    "https://rosenfelder.ai/multi-input-neural-network-pytorch/\n",
    "\n",
    "\n",
    "\n",
    "Understanding dimensions in PyTorch\n",
    "https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습하기 \n",
    "https://github.com/deeplearningzerotoall/PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.init import *\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from FaceFeatureDataset import FaceFeatureDataset\n",
    "import dlib_index\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, F, C]: torch.Size([30, 2, 68])\n",
      "Shape of Tensor y: torch.Size([30, 33]) torch.float32\n",
      "tensor(-0.3848)\n",
      "tensor(-0.3833)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataLoader \n",
    "training_data = FaceFeatureDataset(feature_file=\"./outimg/Train/facefeature.csv\", label_file=\"./Dataset/Train/csv/train.csv\")\n",
    "test_data = FaceFeatureDataset(feature_file=\"./outimg/Test/facefeature.csv\", label_file=\"./Dataset/Test/csv/test.csv\")\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 데이터 로드 확인 \n",
    "for X, y in test_loader:\n",
    "    print(f\"Shape of X [N, F, C]: {X.shape}\") # N , Channel, H= width W = height\n",
    "    print(f\"Shape of Tensor y: {y.shape} {y.dtype}\")       \n",
    "    break\n",
    "\n",
    "print(X[0][0][0]) # X series\n",
    "print(X[1][0][0]) # Y series\n",
    "\n",
    "# plt.scatter(X[0][0], X[1][0])\n",
    "plt.scatter(0.5, 0.3)\n",
    "plt.show()\n",
    "\n",
    "# -0.3848343540531417,-0.20385301634759112,\n",
    "# -0.3737179919949848,-0.10460915272218729,\n",
    "# -0.35550224236871286,-0.007677098905338076,\n",
    "# -0.33490909967690585,0.08691035347446618,-0.3048719679799391,0.1815633891112507,-0.24878188744755736,0.2645917924482808,-0.1736726623908953,0.3288633842889521,-0.09141486232338314,0.38374017456719867,-0.006550127790904914,0.40321840189034086,0.07618315088971417,0.3896262718811785,0.16388436128658945,0.3406519747311567,0.24222356174953008,0.27980710806770515,0.30177315908762137,0.20466509138255295,0.33782755461252845,0.11283213579592033,0.36207696388098576,0.02091720113806228,0.3840309591551334,-0.08045811833920058,0.3941799681728311,-0.18191541688768886,-0.31111877320731063,-0.2788966581472728,-0.26125910208805514,-0.31868929932005585,-0.19508559579495588,-0.32767420552635385,-0.1337816463326421,-0.3154429280995321,-0.07733085788686889,-0.284356464290881,0.076166755075469,-0.28801273086753154,0.14247142788252876,-0.31588561508414914,0.2086613299898732,-0.32723151854173715,0.2770482712060567,-0.31495105367218024,0.32397309157544485,-0.2721251868640595,0.004779379852437704,-0.20823069875102473,0.001959299802285819,-0.1421391715291509,0.0015002170034238427,-0.07603124849303189,0.001024738390316915,-0.00756232820562297,-0.06070550224236859,0.0415923229010946,-0.030061725418334295,0.04888846024015048,0.0005984472199451751,0.05382360032791622,0.03134059892944985,0.0469537541592322,0.057377151950619765,0.03769011911076814,-0.23610792303611894,-0.20045908279886182,-0.19342961855620389,-0.22613492790664036,-0.14384867627911457,-0.22579061580749382,-0.10159666296957126,-0.19008053238173306,-0.14655398562955102,-0.17622606934464957,-0.19849592515793024,-0.17658677725804117,0.09908810339007568,-0.1886868881709023,0.14420938419250606,-0.22615132372088542,0.1961513237208854,-0.22579061580749393,0.23846892028740907,-0.19952452138689303,0.2005290061243188,-0.1761768819019146,0.14858706659593968,-0.1765375898153061,-0.1182384144283164,0.16633167767758117,-0.07321550851135639,0.1430332256353377,-0.032963784539711494,0.1267849737184743,-0.0023364035299222596,0.13644210830881986,0.030783141245117496,0.1272276607030911,0.0708053238173314,0.14403337030428687,0.1202550995804601,0.163265660413753,0.07511742296378476,0.20309109321502627,0.03250470174084985,0.21932294931764473,-0.0029430486569897996,0.22379900660654872,-0.03596421854655918,0.2188474707045378,-0.07598640111877308,0.20204176110334204,-0.09698943916670666,0.16647924000578684,-0.03553792737618733,0.15746154217099884,-0.0025167574866179487,0.16241307807300953,0.03056999565993168,0.1579206249698606,0.09664512706756057,0.16310170227130238,0.030520808217196493,0.16500361672373054,-0.00020494767806322756,0.16951246564112443,-0.03322611756763261,0.16456092973911374\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "# print(f'Traing dat length {n_total_steps}')\n",
    "# print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=136, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=128, out_features=33, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 33\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(68 * 2, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, num_classes),            \n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)        \n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('batch',  batch)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.295945  [    0/  100]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.286648  [    0/  100]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.295373  [    0/  100]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.292029  [    0/  100]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.292723  [    0/  100]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.292984  [    0/  100]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.289150  [    0/  100]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.276151  [    0/  100]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.281154  [    0/  100]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.271787  [    0/  100]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.275508  [    0/  100]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.282641  [    0/  100]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.278848  [    0/  100]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.267339  [    0/  100]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.267507  [    0/  100]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.274383  [    0/  100]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.268006  [    0/  100]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.272935  [    0/  100]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.270455  [    0/  100]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.259114  [    0/  100]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.258698  [    0/  100]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.256502  [    0/  100]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.255226  [    0/  100]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.250938  [    0/  100]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.251337  [    0/  100]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.250461  [    0/  100]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.245030  [    0/  100]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.239914  [    0/  100]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.246395  [    0/  100]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.241672  [    0/  100]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.242202  [    0/  100]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.237536  [    0/  100]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.236440  [    0/  100]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.239050  [    0/  100]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.236472  [    0/  100]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.229168  [    0/  100]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.227347  [    0/  100]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.226048  [    0/  100]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.233244  [    0/  100]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.223376  [    0/  100]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.227453  [    0/  100]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.222604  [    0/  100]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.213585  [    0/  100]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.213655  [    0/  100]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.219439  [    0/  100]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.211186  [    0/  100]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.206969  [    0/  100]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.215025  [    0/  100]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.214101  [    0/  100]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.209815  [    0/  100]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.211114  [    0/  100]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.206772  [    0/  100]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.206661  [    0/  100]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.196937  [    0/  100]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.197373  [    0/  100]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.198191  [    0/  100]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.193675  [    0/  100]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.197885  [    0/  100]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.199040  [    0/  100]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.195769  [    0/  100]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.189670  [    0/  100]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.194075  [    0/  100]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.184475  [    0/  100]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.185781  [    0/  100]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.179365  [    0/  100]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.184373  [    0/  100]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.179671  [    0/  100]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.171699  [    0/  100]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.178554  [    0/  100]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.177080  [    0/  100]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.172179  [    0/  100]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.170770  [    0/  100]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.172607  [    0/  100]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.169768  [    0/  100]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.168104  [    0/  100]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.171220  [    0/  100]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.163786  [    0/  100]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.157624  [    0/  100]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.162604  [    0/  100]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.161006  [    0/  100]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.157287  [    0/  100]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.157435  [    0/  100]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.153855  [    0/  100]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.157097  [    0/  100]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.149095  [    0/  100]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.154155  [    0/  100]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.151717  [    0/  100]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.148663  [    0/  100]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.143017  [    0/  100]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.147309  [    0/  100]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.141673  [    0/  100]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.140717  [    0/  100]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.137537  [    0/  100]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.140277  [    0/  100]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.136407  [    0/  100]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.134264  [    0/  100]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.132130  [    0/  100]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.135842  [    0/  100]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.129197  [    0/  100]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.126583  [    0/  100]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.131642  [    0/  100]\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.124933  [    0/  100]\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.127942  [    0/  100]\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.120173  [    0/  100]\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.118352  [    0/  100]\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.123126  [    0/  100]\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.120800  [    0/  100]\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.120672  [    0/  100]\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.116246  [    0/  100]\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.108903  [    0/  100]\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.114532  [    0/  100]\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.105414  [    0/  100]\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.108751  [    0/  100]\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.109318  [    0/  100]\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.108298  [    0/  100]\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.102142  [    0/  100]\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.105737  [    0/  100]\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.104629  [    0/  100]\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.096611  [    0/  100]\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.099145  [    0/  100]\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.099504  [    0/  100]\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.094804  [    0/  100]\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.093865  [    0/  100]\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.093309  [    0/  100]\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.088773  [    0/  100]\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.090602  [    0/  100]\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.090400  [    0/  100]\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.087252  [    0/  100]\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.088197  [    0/  100]\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.084075  [    0/  100]\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.084201  [    0/  100]\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.081354  [    0/  100]\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.083422  [    0/  100]\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.078318  [    0/  100]\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.076248  [    0/  100]\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.074735  [    0/  100]\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.076447  [    0/  100]\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.073060  [    0/  100]\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.073732  [    0/  100]\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.071665  [    0/  100]\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.069691  [    0/  100]\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.072686  [    0/  100]\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.066491  [    0/  100]\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.066823  [    0/  100]\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.064242  [    0/  100]\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.062758  [    0/  100]\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.066067  [    0/  100]\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.059679  [    0/  100]\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.059612  [    0/  100]\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.060611  [    0/  100]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.062784  [    0/  100]\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.055673  [    0/  100]\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.052913  [    0/  100]\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.056262  [    0/  100]\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.054072  [    0/  100]\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.054826  [    0/  100]\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.054195  [    0/  100]\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.049176  [    0/  100]\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.054909  [    0/  100]\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.050762  [    0/  100]\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.049566  [    0/  100]\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.051180  [    0/  100]\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.050328  [    0/  100]\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.044837  [    0/  100]\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.047416  [    0/  100]\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.045599  [    0/  100]\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.047327  [    0/  100]\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.046705  [    0/  100]\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.045655  [    0/  100]\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.046702  [    0/  100]\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.043893  [    0/  100]\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.046278  [    0/  100]\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.041602  [    0/  100]\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.042170  [    0/  100]\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.041126  [    0/  100]\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.037998  [    0/  100]\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.042175  [    0/  100]\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.039674  [    0/  100]\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.038649  [    0/  100]\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.038968  [    0/  100]\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.040883  [    0/  100]\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.039867  [    0/  100]\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.036921  [    0/  100]\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.037105  [    0/  100]\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.035804  [    0/  100]\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.037784  [    0/  100]\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.039647  [    0/  100]\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.037338  [    0/  100]\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.037169  [    0/  100]\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.035606  [    0/  100]\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.035728  [    0/  100]\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.035000  [    0/  100]\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.035288  [    0/  100]\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.034310  [    0/  100]\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.033444  [    0/  100]\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.034156  [    0/  100]\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.033695  [    0/  100]\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.033251  [    0/  100]\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.033408  [    0/  100]\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.033677  [    0/  100]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.034221  [    0/  100]\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.033103  [    0/  100]\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.032853  [    0/  100]\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.033300  [    0/  100]\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.033624  [    0/  100]\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.033256  [    0/  100]\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.033741  [    0/  100]\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.032109  [    0/  100]\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.032914  [    0/  100]\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.031691  [    0/  100]\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.032592  [    0/  100]\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.031121  [    0/  100]\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.033675  [    0/  100]\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.032379  [    0/  100]\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.032575  [    0/  100]\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.030553  [    0/  100]\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.030714  [    0/  100]\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.032328  [    0/  100]\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.031183  [    0/  100]\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.031666  [    0/  100]\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.031107  [    0/  100]\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.031744  [    0/  100]\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.029703  [    0/  100]\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.031472  [    0/  100]\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.031132  [    0/  100]\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.031947  [    0/  100]\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.032008  [    0/  100]\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.032443  [    0/  100]\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.030342  [    0/  100]\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.030904  [    0/  100]\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.031407  [    0/  100]\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.030296  [    0/  100]\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.031308  [    0/  100]\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.029976  [    0/  100]\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.031194  [    0/  100]\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.030340  [    0/  100]\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.032649  [    0/  100]\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.030694  [    0/  100]\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.031010  [    0/  100]\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.031711  [    0/  100]\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.030435  [    0/  100]\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.030269  [    0/  100]\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.030258  [    0/  100]\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.030411  [    0/  100]\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.031672  [    0/  100]\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.031093  [    0/  100]\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.030954  [    0/  100]\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.030220  [    0/  100]\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.031053  [    0/  100]\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.030792  [    0/  100]\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.029862  [    0/  100]\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.030160  [    0/  100]\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.030381  [    0/  100]\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.031270  [    0/  100]\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.030519  [    0/  100]\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.031322  [    0/  100]\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.029987  [    0/  100]\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.030526  [    0/  100]\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.029704  [    0/  100]\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.029277  [    0/  100]\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.030531  [    0/  100]\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.029928  [    0/  100]\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.030164  [    0/  100]\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.029835  [    0/  100]\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.031338  [    0/  100]\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.030183  [    0/  100]\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.031308  [    0/  100]\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.031916  [    0/  100]\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.031669  [    0/  100]\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.030394  [    0/  100]\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.031403  [    0/  100]\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.032013  [    0/  100]\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.029683  [    0/  100]\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.030727  [    0/  100]\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.031824  [    0/  100]\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.030776  [    0/  100]\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.030793  [    0/  100]\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.030254  [    0/  100]\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.030075  [    0/  100]\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.030641  [    0/  100]\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.031502  [    0/  100]\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.029763  [    0/  100]\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.030135  [    0/  100]\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.030285  [    0/  100]\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.030394  [    0/  100]\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.030254  [    0/  100]\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.031159  [    0/  100]\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.030972  [    0/  100]\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.030374  [    0/  100]\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.030857  [    0/  100]\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.030010  [    0/  100]\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.030462  [    0/  100]\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.030581  [    0/  100]\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.030510  [    0/  100]\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.030815  [    0/  100]\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.031371  [    0/  100]\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.030561  [    0/  100]\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.030459  [    0/  100]\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.031789  [    0/  100]\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.031266  [    0/  100]\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.030430  [    0/  100]\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.031159  [    0/  100]\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.030603  [    0/  100]\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.030821  [    0/  100]\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.030748  [    0/  100]\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.030570  [    0/  100]\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.030730  [    0/  100]\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.031099  [    0/  100]\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.031077  [    0/  100]\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.029793  [    0/  100]\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.028856  [    0/  100]\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.031846  [    0/  100]\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.030346  [    0/  100]\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.030518  [    0/  100]\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.031479  [    0/  100]\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.029143  [    0/  100]\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.030874  [    0/  100]\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.029206  [    0/  100]\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.029505  [    0/  100]\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.029712  [    0/  100]\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.031698  [    0/  100]\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.030681  [    0/  100]\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.029544  [    0/  100]\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.029563  [    0/  100]\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.031229  [    0/  100]\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.031301  [    0/  100]\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.031486  [    0/  100]\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.030499  [    0/  100]\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.030283  [    0/  100]\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.028973  [    0/  100]\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.029620  [    0/  100]\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.029804  [    0/  100]\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.030398  [    0/  100]\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.031251  [    0/  100]\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.030962  [    0/  100]\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.029051  [    0/  100]\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.029695  [    0/  100]\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.029483  [    0/  100]\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.031116  [    0/  100]\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.031548  [    0/  100]\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.028928  [    0/  100]\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.030162  [    0/  100]\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.030546  [    0/  100]\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.029789  [    0/  100]\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.031907  [    0/  100]\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.030316  [    0/  100]\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.030277  [    0/  100]\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.029957  [    0/  100]\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.030025  [    0/  100]\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.029946  [    0/  100]\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.030193  [    0/  100]\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.030928  [    0/  100]\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.031137  [    0/  100]\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.030375  [    0/  100]\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.029918  [    0/  100]\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.031802  [    0/  100]\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.031415  [    0/  100]\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.030811  [    0/  100]\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.030548  [    0/  100]\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.030313  [    0/  100]\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.030587  [    0/  100]\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.030302  [    0/  100]\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.030877  [    0/  100]\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.031164  [    0/  100]\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.032288  [    0/  100]\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.030797  [    0/  100]\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.030476  [    0/  100]\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.030459  [    0/  100]\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.029090  [    0/  100]\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.031105  [    0/  100]\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.029917  [    0/  100]\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.030786  [    0/  100]\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.030437  [    0/  100]\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.029808  [    0/  100]\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.031208  [    0/  100]\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.031378  [    0/  100]\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.030407  [    0/  100]\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.031911  [    0/  100]\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.030940  [    0/  100]\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.030982  [    0/  100]\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.030635  [    0/  100]\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.030189  [    0/  100]\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.030660  [    0/  100]\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.030513  [    0/  100]\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.030402  [    0/  100]\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.032034  [    0/  100]\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.029866  [    0/  100]\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.031448  [    0/  100]\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.031016  [    0/  100]\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.030022  [    0/  100]\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.029953  [    0/  100]\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.029200  [    0/  100]\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.030728  [    0/  100]\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.030033  [    0/  100]\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.030432  [    0/  100]\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.031743  [    0/  100]\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.029279  [    0/  100]\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.031260  [    0/  100]\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.031038  [    0/  100]\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.030488  [    0/  100]\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.028854  [    0/  100]\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.029821  [    0/  100]\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.030747  [    0/  100]\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.030278  [    0/  100]\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.030075  [    0/  100]\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.029093  [    0/  100]\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.029452  [    0/  100]\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.029823  [    0/  100]\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.030220  [    0/  100]\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.030452  [    0/  100]\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.031711  [    0/  100]\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.032062  [    0/  100]\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.030683  [    0/  100]\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.029870  [    0/  100]\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.031023  [    0/  100]\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.030736  [    0/  100]\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.029975  [    0/  100]\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.030901  [    0/  100]\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.029261  [    0/  100]\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.031465  [    0/  100]\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.031416  [    0/  100]\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.031735  [    0/  100]\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.030473  [    0/  100]\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.030016  [    0/  100]\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.031967  [    0/  100]\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.031061  [    0/  100]\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.029229  [    0/  100]\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.030382  [    0/  100]\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.033278  [    0/  100]\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.029563  [    0/  100]\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.031262  [    0/  100]\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.030953  [    0/  100]\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.029545  [    0/  100]\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.030384  [    0/  100]\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.028830  [    0/  100]\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.030606  [    0/  100]\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.029603  [    0/  100]\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.029246  [    0/  100]\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.031191  [    0/  100]\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.031032  [    0/  100]\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.030165  [    0/  100]\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.030038  [    0/  100]\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.029792  [    0/  100]\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.030422  [    0/  100]\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.030360  [    0/  100]\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.031003  [    0/  100]\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.030759  [    0/  100]\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.030089  [    0/  100]\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.030561  [    0/  100]\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.030051  [    0/  100]\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.029337  [    0/  100]\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.030880  [    0/  100]\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.029657  [    0/  100]\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.029711  [    0/  100]\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.029635  [    0/  100]\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.031533  [    0/  100]\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.030588  [    0/  100]\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.030193  [    0/  100]\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.030267  [    0/  100]\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.029223  [    0/  100]\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.030411  [    0/  100]\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.031644  [    0/  100]\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.030333  [    0/  100]\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.031380  [    0/  100]\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.029490  [    0/  100]\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.032520  [    0/  100]\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.030015  [    0/  100]\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.031288  [    0/  100]\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.030233  [    0/  100]\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.030066  [    0/  100]\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.029894  [    0/  100]\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.030762  [    0/  100]\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.030758  [    0/  100]\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.030419  [    0/  100]\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.030006  [    0/  100]\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.031866  [    0/  100]\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.029991  [    0/  100]\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.031155  [    0/  100]\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.030087  [    0/  100]\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.028497  [    0/  100]\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.030457  [    0/  100]\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.030399  [    0/  100]\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.030653  [    0/  100]\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.031045  [    0/  100]\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.031400  [    0/  100]\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.030367  [    0/  100]\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.030461  [    0/  100]\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.030756  [    0/  100]\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.030238  [    0/  100]\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.030751  [    0/  100]\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.030667  [    0/  100]\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.029969  [    0/  100]\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.032697  [    0/  100]\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.031220  [    0/  100]\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.030721  [    0/  100]\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.031863  [    0/  100]\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.030980  [    0/  100]\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.029273  [    0/  100]\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.030737  [    0/  100]\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.031570  [    0/  100]\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "loss: 0.029736  [    0/  100]\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "loss: 0.030164  [    0/  100]\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "loss: 0.029771  [    0/  100]\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "loss: 0.030484  [    0/  100]\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "loss: 0.030192  [    0/  100]\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "loss: 0.029377  [    0/  100]\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "loss: 0.029393  [    0/  100]\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "loss: 0.029384  [    0/  100]\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "loss: 0.030033  [    0/  100]\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "loss: 0.030105  [    0/  100]\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "loss: 0.029636  [    0/  100]\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "loss: 0.030579  [    0/  100]\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "loss: 0.030937  [    0/  100]\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "loss: 0.029634  [    0/  100]\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "loss: 0.030922  [    0/  100]\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "loss: 0.030685  [    0/  100]\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "loss: 0.029643  [    0/  100]\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "loss: 0.031351  [    0/  100]\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "loss: 0.030355  [    0/  100]\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "loss: 0.030759  [    0/  100]\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "loss: 0.030738  [    0/  100]\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "loss: 0.031543  [    0/  100]\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "loss: 0.029653  [    0/  100]\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "loss: 0.030069  [    0/  100]\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "loss: 0.029383  [    0/  100]\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "loss: 0.030735  [    0/  100]\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "loss: 0.029937  [    0/  100]\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "loss: 0.032339  [    0/  100]\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "loss: 0.031467  [    0/  100]\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "loss: 0.031068  [    0/  100]\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "loss: 0.029446  [    0/  100]\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "loss: 0.029449  [    0/  100]\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "loss: 0.030279  [    0/  100]\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "loss: 0.031082  [    0/  100]\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "loss: 0.030856  [    0/  100]\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "loss: 0.030395  [    0/  100]\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "loss: 0.030665  [    0/  100]\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "loss: 0.029509  [    0/  100]\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "loss: 0.030549  [    0/  100]\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "loss: 0.029797  [    0/  100]\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "loss: 0.031326  [    0/  100]\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "loss: 0.031676  [    0/  100]\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "loss: 0.030898  [    0/  100]\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "loss: 0.029136  [    0/  100]\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "loss: 0.029602  [    0/  100]\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "loss: 0.030652  [    0/  100]\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "loss: 0.031301  [    0/  100]\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "loss: 0.030631  [    0/  100]\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "loss: 0.030147  [    0/  100]\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "loss: 0.030844  [    0/  100]\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "loss: 0.030749  [    0/  100]\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "loss: 0.031324  [    0/  100]\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "loss: 0.029663  [    0/  100]\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "loss: 0.030663  [    0/  100]\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "loss: 0.029533  [    0/  100]\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "loss: 0.030006  [    0/  100]\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "loss: 0.031174  [    0/  100]\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "loss: 0.030886  [    0/  100]\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "loss: 0.029543  [    0/  100]\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "loss: 0.031421  [    0/  100]\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "loss: 0.031499  [    0/  100]\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "loss: 0.030856  [    0/  100]\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "loss: 0.030095  [    0/  100]\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "loss: 0.029632  [    0/  100]\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "loss: 0.030372  [    0/  100]\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "loss: 0.029882  [    0/  100]\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "loss: 0.029229  [    0/  100]\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "loss: 0.029892  [    0/  100]\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "loss: 0.031130  [    0/  100]\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "loss: 0.031451  [    0/  100]\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "loss: 0.029966  [    0/  100]\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "loss: 0.031101  [    0/  100]\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "loss: 0.030600  [    0/  100]\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "loss: 0.031956  [    0/  100]\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "loss: 0.030127  [    0/  100]\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "loss: 0.030118  [    0/  100]\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "loss: 0.030420  [    0/  100]\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "loss: 0.030860  [    0/  100]\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "loss: 0.030303  [    0/  100]\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "loss: 0.030359  [    0/  100]\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "loss: 0.030393  [    0/  100]\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "loss: 0.031600  [    0/  100]\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "loss: 0.029173  [    0/  100]\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "loss: 0.028984  [    0/  100]\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "loss: 0.030607  [    0/  100]\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "loss: 0.030453  [    0/  100]\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "loss: 0.031618  [    0/  100]\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "loss: 0.030588  [    0/  100]\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "loss: 0.032159  [    0/  100]\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "loss: 0.030555  [    0/  100]\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "loss: 0.029896  [    0/  100]\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "loss: 0.031162  [    0/  100]\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "loss: 0.030318  [    0/  100]\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "loss: 0.030203  [    0/  100]\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "loss: 0.030602  [    0/  100]\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "loss: 0.031497  [    0/  100]\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "loss: 0.030206  [    0/  100]\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "loss: 0.032129  [    0/  100]\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "loss: 0.030344  [    0/  100]\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "loss: 0.030285  [    0/  100]\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "loss: 0.031177  [    0/  100]\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "loss: 0.031417  [    0/  100]\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "loss: 0.031593  [    0/  100]\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "loss: 0.030848  [    0/  100]\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "loss: 0.030663  [    0/  100]\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "loss: 0.030813  [    0/  100]\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "loss: 0.029930  [    0/  100]\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "loss: 0.029882  [    0/  100]\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "loss: 0.029459  [    0/  100]\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "loss: 0.029939  [    0/  100]\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "loss: 0.030980  [    0/  100]\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "loss: 0.029623  [    0/  100]\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "loss: 0.030997  [    0/  100]\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "loss: 0.030643  [    0/  100]\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "loss: 0.031360  [    0/  100]\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "loss: 0.030618  [    0/  100]\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "loss: 0.030407  [    0/  100]\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "loss: 0.029622  [    0/  100]\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "loss: 0.030602  [    0/  100]\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "loss: 0.029602  [    0/  100]\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "loss: 0.030600  [    0/  100]\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "loss: 0.029658  [    0/  100]\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "loss: 0.030966  [    0/  100]\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "loss: 0.029973  [    0/  100]\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "loss: 0.030823  [    0/  100]\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "loss: 0.030959  [    0/  100]\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "loss: 0.030168  [    0/  100]\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "loss: 0.031809  [    0/  100]\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "loss: 0.030331  [    0/  100]\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "loss: 0.029985  [    0/  100]\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "loss: 0.030134  [    0/  100]\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "loss: 0.030951  [    0/  100]\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "loss: 0.030778  [    0/  100]\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "loss: 0.031044  [    0/  100]\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "loss: 0.029605  [    0/  100]\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "loss: 0.030229  [    0/  100]\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "loss: 0.029516  [    0/  100]\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "loss: 0.030627  [    0/  100]\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "loss: 0.030319  [    0/  100]\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "loss: 0.029519  [    0/  100]\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "loss: 0.029062  [    0/  100]\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "loss: 0.029967  [    0/  100]\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "loss: 0.029955  [    0/  100]\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "loss: 0.030391  [    0/  100]\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "loss: 0.029858  [    0/  100]\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "loss: 0.029880  [    0/  100]\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "loss: 0.030960  [    0/  100]\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "loss: 0.028977  [    0/  100]\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "loss: 0.029927  [    0/  100]\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "loss: 0.031290  [    0/  100]\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "loss: 0.031238  [    0/  100]\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "loss: 0.030774  [    0/  100]\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "loss: 0.032219  [    0/  100]\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "loss: 0.030577  [    0/  100]\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "loss: 0.030519  [    0/  100]\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "loss: 0.029990  [    0/  100]\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "loss: 0.029995  [    0/  100]\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "loss: 0.029870  [    0/  100]\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "loss: 0.032238  [    0/  100]\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "loss: 0.031922  [    0/  100]\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "loss: 0.030867  [    0/  100]\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "loss: 0.029339  [    0/  100]\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "loss: 0.029934  [    0/  100]\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "loss: 0.030936  [    0/  100]\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "loss: 0.031089  [    0/  100]\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "loss: 0.031084  [    0/  100]\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "loss: 0.029121  [    0/  100]\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "loss: 0.030424  [    0/  100]\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "loss: 0.031264  [    0/  100]\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "loss: 0.031065  [    0/  100]\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "loss: 0.030377  [    0/  100]\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "loss: 0.031271  [    0/  100]\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "loss: 0.029418  [    0/  100]\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "loss: 0.030583  [    0/  100]\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "loss: 0.031496  [    0/  100]\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "loss: 0.029472  [    0/  100]\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "loss: 0.029951  [    0/  100]\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "loss: 0.029546  [    0/  100]\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "loss: 0.029825  [    0/  100]\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "loss: 0.031184  [    0/  100]\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "loss: 0.030338  [    0/  100]\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "loss: 0.031795  [    0/  100]\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "loss: 0.030820  [    0/  100]\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "loss: 0.029933  [    0/  100]\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "loss: 0.030928  [    0/  100]\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "loss: 0.030675  [    0/  100]\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "loss: 0.031237  [    0/  100]\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "loss: 0.031184  [    0/  100]\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "loss: 0.030610  [    0/  100]\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "loss: 0.029980  [    0/  100]\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "loss: 0.029859  [    0/  100]\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "loss: 0.029631  [    0/  100]\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "loss: 0.029939  [    0/  100]\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "loss: 0.031450  [    0/  100]\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "loss: 0.029972  [    0/  100]\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "loss: 0.031626  [    0/  100]\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "loss: 0.029677  [    0/  100]\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "loss: 0.032253  [    0/  100]\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "loss: 0.030040  [    0/  100]\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "loss: 0.031173  [    0/  100]\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "loss: 0.028869  [    0/  100]\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "loss: 0.031392  [    0/  100]\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "loss: 0.029723  [    0/  100]\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "loss: 0.030359  [    0/  100]\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "loss: 0.031654  [    0/  100]\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "loss: 0.030996  [    0/  100]\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "loss: 0.029405  [    0/  100]\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "loss: 0.030117  [    0/  100]\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "loss: 0.030112  [    0/  100]\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "loss: 0.030211  [    0/  100]\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "loss: 0.029734  [    0/  100]\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "loss: 0.030118  [    0/  100]\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "loss: 0.030413  [    0/  100]\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "loss: 0.030554  [    0/  100]\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "loss: 0.030048  [    0/  100]\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "loss: 0.030805  [    0/  100]\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "loss: 0.029123  [    0/  100]\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "loss: 0.029220  [    0/  100]\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "loss: 0.028594  [    0/  100]\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "loss: 0.029605  [    0/  100]\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "loss: 0.030339  [    0/  100]\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "loss: 0.030390  [    0/  100]\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "loss: 0.030117  [    0/  100]\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "loss: 0.029673  [    0/  100]\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "loss: 0.031788  [    0/  100]\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "loss: 0.031189  [    0/  100]\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "loss: 0.031646  [    0/  100]\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "loss: 0.031439  [    0/  100]\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "loss: 0.030633  [    0/  100]\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "loss: 0.031045  [    0/  100]\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "loss: 0.030112  [    0/  100]\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "loss: 0.030313  [    0/  100]\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "loss: 0.030811  [    0/  100]\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "loss: 0.030885  [    0/  100]\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "loss: 0.030255  [    0/  100]\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "loss: 0.031804  [    0/  100]\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "loss: 0.029889  [    0/  100]\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "loss: 0.031508  [    0/  100]\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "loss: 0.030367  [    0/  100]\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "loss: 0.032054  [    0/  100]\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "loss: 0.030372  [    0/  100]\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "loss: 0.030531  [    0/  100]\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "loss: 0.030080  [    0/  100]\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "loss: 0.030242  [    0/  100]\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "loss: 0.031588  [    0/  100]\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "loss: 0.029641  [    0/  100]\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "loss: 0.030667  [    0/  100]\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "loss: 0.031113  [    0/  100]\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "loss: 0.031031  [    0/  100]\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "loss: 0.030262  [    0/  100]\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "loss: 0.030391  [    0/  100]\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "loss: 0.029486  [    0/  100]\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "loss: 0.031528  [    0/  100]\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "loss: 0.030732  [    0/  100]\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "loss: 0.029756  [    0/  100]\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "loss: 0.030704  [    0/  100]\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "loss: 0.030588  [    0/  100]\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "loss: 0.030123  [    0/  100]\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "loss: 0.030595  [    0/  100]\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "loss: 0.031584  [    0/  100]\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "loss: 0.029071  [    0/  100]\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "loss: 0.030903  [    0/  100]\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "loss: 0.028836  [    0/  100]\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "loss: 0.031965  [    0/  100]\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "loss: 0.030331  [    0/  100]\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "loss: 0.030515  [    0/  100]\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "loss: 0.029815  [    0/  100]\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "loss: 0.030973  [    0/  100]\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "loss: 0.030250  [    0/  100]\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "loss: 0.030588  [    0/  100]\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "loss: 0.030542  [    0/  100]\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "loss: 0.030343  [    0/  100]\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "loss: 0.029871  [    0/  100]\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "loss: 0.030712  [    0/  100]\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "loss: 0.030468  [    0/  100]\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "loss: 0.029826  [    0/  100]\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "loss: 0.031167  [    0/  100]\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "loss: 0.031147  [    0/  100]\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "loss: 0.029644  [    0/  100]\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "loss: 0.030212  [    0/  100]\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "loss: 0.029188  [    0/  100]\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "loss: 0.030394  [    0/  100]\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "loss: 0.030302  [    0/  100]\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "loss: 0.030377  [    0/  100]\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "loss: 0.029817  [    0/  100]\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "loss: 0.030838  [    0/  100]\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "loss: 0.030202  [    0/  100]\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "loss: 0.031040  [    0/  100]\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "loss: 0.031111  [    0/  100]\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "loss: 0.029085  [    0/  100]\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "loss: 0.031013  [    0/  100]\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "loss: 0.029322  [    0/  100]\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "loss: 0.029595  [    0/  100]\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "loss: 0.030494  [    0/  100]\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "loss: 0.030917  [    0/  100]\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "loss: 0.029575  [    0/  100]\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "loss: 0.030867  [    0/  100]\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "loss: 0.029429  [    0/  100]\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "loss: 0.029412  [    0/  100]\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "loss: 0.030394  [    0/  100]\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "loss: 0.030906  [    0/  100]\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "loss: 0.030521  [    0/  100]\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "loss: 0.029200  [    0/  100]\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "loss: 0.029716  [    0/  100]\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "loss: 0.029412  [    0/  100]\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "loss: 0.031469  [    0/  100]\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "loss: 0.029284  [    0/  100]\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "loss: 0.032047  [    0/  100]\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "loss: 0.030062  [    0/  100]\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "loss: 0.030123  [    0/  100]\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "loss: 0.029872  [    0/  100]\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "loss: 0.031007  [    0/  100]\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "loss: 0.028946  [    0/  100]\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "loss: 0.030112  [    0/  100]\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "loss: 0.031220  [    0/  100]\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "loss: 0.030361  [    0/  100]\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "loss: 0.030141  [    0/  100]\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "loss: 0.029570  [    0/  100]\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "loss: 0.030793  [    0/  100]\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "loss: 0.030519  [    0/  100]\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "loss: 0.030649  [    0/  100]\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "loss: 0.031197  [    0/  100]\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "loss: 0.030178  [    0/  100]\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "loss: 0.030797  [    0/  100]\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "loss: 0.030140  [    0/  100]\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "loss: 0.029866  [    0/  100]\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "loss: 0.030248  [    0/  100]\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "loss: 0.030178  [    0/  100]\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "loss: 0.030960  [    0/  100]\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "loss: 0.031478  [    0/  100]\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "loss: 0.029459  [    0/  100]\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "loss: 0.030723  [    0/  100]\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "loss: 0.029191  [    0/  100]\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "loss: 0.029882  [    0/  100]\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "loss: 0.032499  [    0/  100]\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "loss: 0.030604  [    0/  100]\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "loss: 0.030514  [    0/  100]\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "loss: 0.029380  [    0/  100]\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "loss: 0.029394  [    0/  100]\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "loss: 0.029774  [    0/  100]\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "loss: 0.029531  [    0/  100]\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "loss: 0.030772  [    0/  100]\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "loss: 0.031300  [    0/  100]\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "loss: 0.030962  [    0/  100]\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "loss: 0.028890  [    0/  100]\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "loss: 0.030026  [    0/  100]\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "loss: 0.030137  [    0/  100]\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "loss: 0.029753  [    0/  100]\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "loss: 0.030344  [    0/  100]\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "loss: 0.030598  [    0/  100]\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "loss: 0.030336  [    0/  100]\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "loss: 0.030750  [    0/  100]\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "loss: 0.031036  [    0/  100]\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "loss: 0.030818  [    0/  100]\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "loss: 0.030615  [    0/  100]\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "loss: 0.030399  [    0/  100]\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "loss: 0.029263  [    0/  100]\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "loss: 0.030390  [    0/  100]\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "loss: 0.029755  [    0/  100]\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "loss: 0.031190  [    0/  100]\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "loss: 0.028246  [    0/  100]\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "loss: 0.029856  [    0/  100]\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "loss: 0.029195  [    0/  100]\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "loss: 0.030140  [    0/  100]\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "loss: 0.030387  [    0/  100]\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "loss: 0.030589  [    0/  100]\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "loss: 0.028665  [    0/  100]\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "loss: 0.030985  [    0/  100]\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "loss: 0.029154  [    0/  100]\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "loss: 0.032131  [    0/  100]\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "loss: 0.029865  [    0/  100]\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "loss: 0.030099  [    0/  100]\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "loss: 0.030137  [    0/  100]\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "loss: 0.031608  [    0/  100]\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "loss: 0.028714  [    0/  100]\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "loss: 0.029009  [    0/  100]\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "loss: 0.029731  [    0/  100]\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "loss: 0.030192  [    0/  100]\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "loss: 0.030632  [    0/  100]\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "loss: 0.030502  [    0/  100]\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "loss: 0.030625  [    0/  100]\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "loss: 0.030463  [    0/  100]\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "loss: 0.029691  [    0/  100]\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "loss: 0.030421  [    0/  100]\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "loss: 0.031647  [    0/  100]\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "loss: 0.030014  [    0/  100]\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "loss: 0.029770  [    0/  100]\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "loss: 0.030411  [    0/  100]\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "loss: 0.031253  [    0/  100]\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "loss: 0.030915  [    0/  100]\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "loss: 0.029011  [    0/  100]\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "loss: 0.031039  [    0/  100]\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "loss: 0.029437  [    0/  100]\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "loss: 0.031321  [    0/  100]\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "loss: 0.030972  [    0/  100]\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "loss: 0.030200  [    0/  100]\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "loss: 0.030347  [    0/  100]\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "loss: 0.031499  [    0/  100]\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "loss: 0.030214  [    0/  100]\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "loss: 0.031294  [    0/  100]\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "loss: 0.030647  [    0/  100]\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "loss: 0.030260  [    0/  100]\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "loss: 0.030148  [    0/  100]\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "loss: 0.032287  [    0/  100]\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "loss: 0.031147  [    0/  100]\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "loss: 0.030886  [    0/  100]\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "loss: 0.030830  [    0/  100]\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "loss: 0.030871  [    0/  100]\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "loss: 0.029080  [    0/  100]\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "loss: 0.030916  [    0/  100]\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "loss: 0.030677  [    0/  100]\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "loss: 0.029343  [    0/  100]\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "loss: 0.032285  [    0/  100]\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "loss: 0.030008  [    0/  100]\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "loss: 0.031366  [    0/  100]\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "loss: 0.031623  [    0/  100]\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "loss: 0.030554  [    0/  100]\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "loss: 0.030313  [    0/  100]\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "loss: 0.031398  [    0/  100]\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "loss: 0.030756  [    0/  100]\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "loss: 0.031104  [    0/  100]\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "loss: 0.029388  [    0/  100]\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "loss: 0.030783  [    0/  100]\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "loss: 0.031966  [    0/  100]\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "loss: 0.030196  [    0/  100]\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "loss: 0.030486  [    0/  100]\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "loss: 0.030326  [    0/  100]\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "loss: 0.031588  [    0/  100]\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "loss: 0.030620  [    0/  100]\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "loss: 0.030099  [    0/  100]\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "loss: 0.030663  [    0/  100]\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "loss: 0.030418  [    0/  100]\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "loss: 0.030530  [    0/  100]\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "loss: 0.029836  [    0/  100]\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "loss: 0.031074  [    0/  100]\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "loss: 0.031030  [    0/  100]\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "loss: 0.030437  [    0/  100]\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "loss: 0.029265  [    0/  100]\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "loss: 0.030230  [    0/  100]\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "loss: 0.030603  [    0/  100]\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "loss: 0.030392  [    0/  100]\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "loss: 0.030289  [    0/  100]\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "loss: 0.030148  [    0/  100]\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "loss: 0.029105  [    0/  100]\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "loss: 0.030812  [    0/  100]\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "loss: 0.030317  [    0/  100]\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "loss: 0.030695  [    0/  100]\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "loss: 0.030785  [    0/  100]\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "loss: 0.030774  [    0/  100]\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "loss: 0.031850  [    0/  100]\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "loss: 0.030108  [    0/  100]\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "loss: 0.030082  [    0/  100]\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "loss: 0.029487  [    0/  100]\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "loss: 0.030756  [    0/  100]\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "loss: 0.029329  [    0/  100]\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "loss: 0.031109  [    0/  100]\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "loss: 0.030328  [    0/  100]\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "loss: 0.030434  [    0/  100]\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "loss: 0.031844  [    0/  100]\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "loss: 0.030162  [    0/  100]\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "loss: 0.030886  [    0/  100]\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "loss: 0.030645  [    0/  100]\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "loss: 0.031542  [    0/  100]\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "loss: 0.029941  [    0/  100]\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "loss: 0.030095  [    0/  100]\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "loss: 0.030855  [    0/  100]\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "loss: 0.029339  [    0/  100]\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "loss: 0.030508  [    0/  100]\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "loss: 0.030941  [    0/  100]\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "loss: 0.028734  [    0/  100]\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "loss: 0.030738  [    0/  100]\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "loss: 0.030673  [    0/  100]\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "loss: 0.031314  [    0/  100]\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "loss: 0.030195  [    0/  100]\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "loss: 0.030607  [    0/  100]\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "loss: 0.032047  [    0/  100]\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "loss: 0.031307  [    0/  100]\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "loss: 0.030147  [    0/  100]\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "loss: 0.029986  [    0/  100]\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "loss: 0.029804  [    0/  100]\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "loss: 0.030305  [    0/  100]\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "loss: 0.031035  [    0/  100]\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "loss: 0.030976  [    0/  100]\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "loss: 0.030198  [    0/  100]\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "loss: 0.030453  [    0/  100]\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "loss: 0.029592  [    0/  100]\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "loss: 0.030088  [    0/  100]\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "loss: 0.029524  [    0/  100]\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "loss: 0.030739  [    0/  100]\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "loss: 0.031429  [    0/  100]\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "loss: 0.029810  [    0/  100]\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "loss: 0.030039  [    0/  100]\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "loss: 0.032421  [    0/  100]\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "loss: 0.030497  [    0/  100]\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "loss: 0.029964  [    0/  100]\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "loss: 0.029878  [    0/  100]\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "loss: 0.031229  [    0/  100]\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "loss: 0.030060  [    0/  100]\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "loss: 0.029556  [    0/  100]\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "loss: 0.029599  [    0/  100]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, criterion, optimizer)    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    print('test size', size )\n",
    "    # num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            print(\"X Shape\", X.shape)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)            \n",
    "            print('pred =', pred)\n",
    "            #print('loss', loss)\n",
    "            #print('real', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size 100\n",
      "X Shape torch.Size([30, 2, 68])\n",
      "pred = tensor([[0.4948, 0.4883, 0.4935, 0.5121, 0.5205, 0.5242, 0.4778, 0.5006, 0.5232,\n",
      "         0.5142, 0.5230, 0.5013, 0.4974, 0.5161, 0.4857, 0.4941, 0.5157, 0.5356,\n",
      "         0.4974, 0.5233, 0.5033, 0.5064, 0.5011, 0.5021, 0.5202, 0.4916, 0.5269,\n",
      "         0.4921, 0.4833, 0.5074, 0.5272, 0.5202, 0.4869],\n",
      "        [0.4947, 0.4882, 0.4956, 0.5138, 0.5206, 0.5239, 0.4792, 0.5007, 0.5229,\n",
      "         0.5145, 0.5244, 0.5011, 0.4977, 0.5172, 0.4863, 0.4938, 0.5147, 0.5371,\n",
      "         0.4966, 0.5235, 0.5034, 0.5070, 0.4993, 0.5011, 0.5214, 0.4922, 0.5292,\n",
      "         0.4914, 0.4851, 0.5079, 0.5280, 0.5208, 0.4884],\n",
      "        [0.4967, 0.4902, 0.4954, 0.5136, 0.5216, 0.5248, 0.4801, 0.5010, 0.5240,\n",
      "         0.5163, 0.5249, 0.5021, 0.4979, 0.5177, 0.4878, 0.4953, 0.5177, 0.5363,\n",
      "         0.4986, 0.5248, 0.5039, 0.5064, 0.5012, 0.5024, 0.5210, 0.4922, 0.5288,\n",
      "         0.4932, 0.4851, 0.5084, 0.5297, 0.5221, 0.4887],\n",
      "        [0.4968, 0.4896, 0.4957, 0.5143, 0.5216, 0.5260, 0.4799, 0.5022, 0.5245,\n",
      "         0.5161, 0.5248, 0.5029, 0.4997, 0.5175, 0.4867, 0.4954, 0.5170, 0.5388,\n",
      "         0.4999, 0.5256, 0.5055, 0.5087, 0.5031, 0.5033, 0.5216, 0.4943, 0.5293,\n",
      "         0.4938, 0.4844, 0.5087, 0.5290, 0.5224, 0.4889],\n",
      "        [0.4939, 0.4876, 0.4945, 0.5119, 0.5194, 0.5229, 0.4778, 0.4995, 0.5220,\n",
      "         0.5141, 0.5235, 0.5002, 0.4966, 0.5156, 0.4863, 0.4925, 0.5144, 0.5342,\n",
      "         0.4962, 0.5228, 0.5022, 0.5057, 0.4991, 0.5005, 0.5201, 0.4910, 0.5269,\n",
      "         0.4904, 0.4835, 0.5071, 0.5272, 0.5196, 0.4866],\n",
      "        [0.4775, 0.4744, 0.4840, 0.4988, 0.5078, 0.5095, 0.4635, 0.4882, 0.5093,\n",
      "         0.5004, 0.5075, 0.4894, 0.4826, 0.5019, 0.4738, 0.4799, 0.5004, 0.5221,\n",
      "         0.4809, 0.5095, 0.4883, 0.4957, 0.4875, 0.4885, 0.5112, 0.4770, 0.5120,\n",
      "         0.4766, 0.4715, 0.4946, 0.5114, 0.5046, 0.4722],\n",
      "        [0.4908, 0.4848, 0.4928, 0.5097, 0.5169, 0.5198, 0.4755, 0.4971, 0.5193,\n",
      "         0.5114, 0.5202, 0.4980, 0.4936, 0.5132, 0.4836, 0.4901, 0.5116, 0.5321,\n",
      "         0.4925, 0.5201, 0.4990, 0.5035, 0.4962, 0.4972, 0.5183, 0.4880, 0.5241,\n",
      "         0.4875, 0.4815, 0.5044, 0.5241, 0.5166, 0.4842],\n",
      "        [0.4734, 0.4716, 0.4809, 0.4956, 0.5053, 0.5065, 0.4592, 0.4864, 0.5064,\n",
      "         0.4980, 0.5013, 0.4866, 0.4793, 0.4987, 0.4711, 0.4778, 0.4975, 0.5195,\n",
      "         0.4782, 0.5062, 0.4851, 0.4933, 0.4863, 0.4870, 0.5089, 0.4730, 0.5080,\n",
      "         0.4750, 0.4678, 0.4916, 0.5074, 0.5015, 0.4687],\n",
      "        [0.4905, 0.4845, 0.4942, 0.5115, 0.5176, 0.5207, 0.4759, 0.4987, 0.5200,\n",
      "         0.5104, 0.5218, 0.4983, 0.4945, 0.5142, 0.4838, 0.4906, 0.5104, 0.5336,\n",
      "         0.4934, 0.5212, 0.4998, 0.5060, 0.4959, 0.4979, 0.5200, 0.4902, 0.5263,\n",
      "         0.4874, 0.4827, 0.5055, 0.5242, 0.5169, 0.4845],\n",
      "        [0.4799, 0.4763, 0.4849, 0.5001, 0.5092, 0.5112, 0.4650, 0.4893, 0.5110,\n",
      "         0.5020, 0.5088, 0.4908, 0.4841, 0.5031, 0.4757, 0.4822, 0.5027, 0.5229,\n",
      "         0.4839, 0.5109, 0.4902, 0.4962, 0.4900, 0.4908, 0.5117, 0.4776, 0.5129,\n",
      "         0.4794, 0.4720, 0.4961, 0.5144, 0.5060, 0.4738],\n",
      "        [0.4894, 0.4838, 0.4931, 0.5104, 0.5166, 0.5199, 0.4746, 0.4980, 0.5188,\n",
      "         0.5099, 0.5186, 0.4973, 0.4935, 0.5129, 0.4829, 0.4903, 0.5095, 0.5337,\n",
      "         0.4930, 0.5198, 0.4989, 0.5048, 0.4963, 0.4977, 0.5190, 0.4882, 0.5245,\n",
      "         0.4874, 0.4808, 0.5042, 0.5228, 0.5159, 0.4833],\n",
      "        [0.4868, 0.4819, 0.4902, 0.5066, 0.5149, 0.5174, 0.4719, 0.4944, 0.5169,\n",
      "         0.5080, 0.5165, 0.4960, 0.4908, 0.5095, 0.4804, 0.4876, 0.5084, 0.5303,\n",
      "         0.4899, 0.5176, 0.4967, 0.5021, 0.4947, 0.4958, 0.5164, 0.4850, 0.5209,\n",
      "         0.4849, 0.4781, 0.5018, 0.5207, 0.5132, 0.4805],\n",
      "        [0.4861, 0.4816, 0.4901, 0.5067, 0.5145, 0.5166, 0.4715, 0.4945, 0.5164,\n",
      "         0.5070, 0.5162, 0.4950, 0.4903, 0.5099, 0.4808, 0.4872, 0.5075, 0.5293,\n",
      "         0.4896, 0.5169, 0.4957, 0.5020, 0.4936, 0.4952, 0.5166, 0.4850, 0.5210,\n",
      "         0.4844, 0.4786, 0.5016, 0.5201, 0.5131, 0.4804],\n",
      "        [0.4926, 0.4870, 0.4951, 0.5131, 0.5199, 0.5225, 0.4777, 0.5003, 0.5220,\n",
      "         0.5108, 0.5234, 0.4999, 0.4964, 0.5162, 0.4855, 0.4935, 0.5124, 0.5358,\n",
      "         0.4966, 0.5228, 0.5020, 0.5076, 0.4982, 0.5005, 0.5214, 0.4916, 0.5282,\n",
      "         0.4903, 0.4838, 0.5068, 0.5263, 0.5191, 0.4860],\n",
      "        [0.4870, 0.4815, 0.4906, 0.5072, 0.5144, 0.5175, 0.4723, 0.4952, 0.5168,\n",
      "         0.5079, 0.5170, 0.4962, 0.4907, 0.5100, 0.4804, 0.4873, 0.5081, 0.5301,\n",
      "         0.4898, 0.5178, 0.4963, 0.5023, 0.4943, 0.4952, 0.5164, 0.4855, 0.5208,\n",
      "         0.4848, 0.4785, 0.5020, 0.5204, 0.5128, 0.4808],\n",
      "        [0.4696, 0.4690, 0.4794, 0.4940, 0.5029, 0.5040, 0.4565, 0.4841, 0.5037,\n",
      "         0.4934, 0.4993, 0.4845, 0.4772, 0.4966, 0.4688, 0.4753, 0.4936, 0.5178,\n",
      "         0.4755, 0.5037, 0.4828, 0.4924, 0.4834, 0.4843, 0.5081, 0.4716, 0.5062,\n",
      "         0.4710, 0.4664, 0.4895, 0.5040, 0.4986, 0.4656],\n",
      "        [0.4882, 0.4834, 0.4903, 0.5072, 0.5152, 0.5185, 0.4723, 0.4953, 0.5176,\n",
      "         0.5087, 0.5164, 0.4968, 0.4913, 0.5099, 0.4817, 0.4891, 0.5096, 0.5307,\n",
      "         0.4923, 0.5183, 0.4976, 0.5019, 0.4970, 0.4975, 0.5164, 0.4856, 0.5208,\n",
      "         0.4871, 0.4780, 0.5023, 0.5219, 0.5140, 0.4811],\n",
      "        [0.4800, 0.4771, 0.4862, 0.5018, 0.5102, 0.5117, 0.4659, 0.4905, 0.5116,\n",
      "         0.5021, 0.5095, 0.4913, 0.4849, 0.5046, 0.4760, 0.4833, 0.5033, 0.5251,\n",
      "         0.4848, 0.5122, 0.4908, 0.4985, 0.4901, 0.4910, 0.5133, 0.4794, 0.5152,\n",
      "         0.4798, 0.4736, 0.4969, 0.5141, 0.5076, 0.4741],\n",
      "        [0.4923, 0.4864, 0.4920, 0.5095, 0.5183, 0.5216, 0.4758, 0.4975, 0.5207,\n",
      "         0.5136, 0.5199, 0.4997, 0.4946, 0.5128, 0.4839, 0.4914, 0.5139, 0.5338,\n",
      "         0.4946, 0.5213, 0.5009, 0.5037, 0.4994, 0.5001, 0.5180, 0.4885, 0.5242,\n",
      "         0.4898, 0.4809, 0.5051, 0.5254, 0.5180, 0.4848],\n",
      "        [0.4996, 0.4924, 0.4972, 0.5163, 0.5233, 0.5286, 0.4822, 0.5039, 0.5266,\n",
      "         0.5182, 0.5275, 0.5046, 0.5014, 0.5196, 0.4893, 0.4975, 0.5195, 0.5407,\n",
      "         0.5025, 0.5273, 0.5078, 0.5098, 0.5051, 0.5060, 0.5227, 0.4964, 0.5316,\n",
      "         0.4962, 0.4870, 0.5109, 0.5316, 0.5248, 0.4914],\n",
      "        [0.5022, 0.4941, 0.4993, 0.5182, 0.5247, 0.5302, 0.4845, 0.5049, 0.5282,\n",
      "         0.5195, 0.5299, 0.5058, 0.5026, 0.5213, 0.4912, 0.5002, 0.5213, 0.5413,\n",
      "         0.5047, 0.5289, 0.5090, 0.5101, 0.5058, 0.5067, 0.5242, 0.4971, 0.5337,\n",
      "         0.4980, 0.4887, 0.5122, 0.5345, 0.5263, 0.4929],\n",
      "        [0.4777, 0.4747, 0.4837, 0.4994, 0.5078, 0.5108, 0.4627, 0.4888, 0.5095,\n",
      "         0.4998, 0.5058, 0.4900, 0.4831, 0.5014, 0.4735, 0.4813, 0.4998, 0.5237,\n",
      "         0.4832, 0.5091, 0.4895, 0.4959, 0.4899, 0.4906, 0.5111, 0.4763, 0.5119,\n",
      "         0.4783, 0.4706, 0.4947, 0.5120, 0.5040, 0.4721],\n",
      "        [0.4817, 0.4781, 0.4862, 0.5023, 0.5111, 0.5134, 0.4668, 0.4912, 0.5131,\n",
      "         0.5042, 0.5100, 0.4924, 0.4866, 0.5053, 0.4769, 0.4841, 0.5044, 0.5261,\n",
      "         0.4852, 0.5131, 0.4924, 0.4982, 0.4923, 0.4929, 0.5131, 0.4803, 0.5154,\n",
      "         0.4817, 0.4737, 0.4978, 0.5156, 0.5085, 0.4758],\n",
      "        [0.4799, 0.4767, 0.4861, 0.5014, 0.5091, 0.5113, 0.4657, 0.4895, 0.5111,\n",
      "         0.5018, 0.5087, 0.4910, 0.4843, 0.5039, 0.4762, 0.4828, 0.5022, 0.5244,\n",
      "         0.4844, 0.5113, 0.4904, 0.4970, 0.4902, 0.4904, 0.5125, 0.4788, 0.5143,\n",
      "         0.4796, 0.4730, 0.4961, 0.5142, 0.5068, 0.4742],\n",
      "        [0.4954, 0.4884, 0.4967, 0.5141, 0.5203, 0.5241, 0.4803, 0.5005, 0.5233,\n",
      "         0.5143, 0.5260, 0.5013, 0.4980, 0.5174, 0.4875, 0.4938, 0.5148, 0.5367,\n",
      "         0.4972, 0.5242, 0.5034, 0.5072, 0.4993, 0.5008, 0.5214, 0.4932, 0.5292,\n",
      "         0.4914, 0.4855, 0.5083, 0.5288, 0.5209, 0.4887],\n",
      "        [0.4806, 0.4777, 0.4855, 0.5018, 0.5107, 0.5130, 0.4659, 0.4905, 0.5124,\n",
      "         0.5021, 0.5094, 0.4920, 0.4859, 0.5045, 0.4763, 0.4839, 0.5032, 0.5260,\n",
      "         0.4859, 0.5122, 0.4922, 0.4981, 0.4919, 0.4930, 0.5129, 0.4795, 0.5153,\n",
      "         0.4809, 0.4732, 0.4972, 0.5149, 0.5079, 0.4749],\n",
      "        [0.4869, 0.4816, 0.4907, 0.5063, 0.5142, 0.5168, 0.4720, 0.4942, 0.5162,\n",
      "         0.5090, 0.5168, 0.4954, 0.4902, 0.5091, 0.4810, 0.4869, 0.5086, 0.5290,\n",
      "         0.4894, 0.5175, 0.4958, 0.5016, 0.4939, 0.4947, 0.5161, 0.4847, 0.5203,\n",
      "         0.4843, 0.4781, 0.5016, 0.5207, 0.5129, 0.4802],\n",
      "        [0.4737, 0.4718, 0.4834, 0.4980, 0.5056, 0.5077, 0.4608, 0.4869, 0.5071,\n",
      "         0.4963, 0.5043, 0.4876, 0.4806, 0.4995, 0.4714, 0.4786, 0.4960, 0.5220,\n",
      "         0.4795, 0.5072, 0.4866, 0.4955, 0.4859, 0.4872, 0.5107, 0.4757, 0.5107,\n",
      "         0.4745, 0.4695, 0.4926, 0.5083, 0.5016, 0.4692],\n",
      "        [0.4899, 0.4849, 0.4920, 0.5098, 0.5172, 0.5204, 0.4746, 0.4971, 0.5194,\n",
      "         0.5105, 0.5182, 0.4981, 0.4935, 0.5123, 0.4825, 0.4908, 0.5112, 0.5340,\n",
      "         0.4938, 0.5202, 0.4997, 0.5042, 0.4981, 0.4989, 0.5180, 0.4878, 0.5240,\n",
      "         0.4883, 0.4802, 0.5041, 0.5234, 0.5164, 0.4833],\n",
      "        [0.4903, 0.4850, 0.4923, 0.5096, 0.5169, 0.5204, 0.4749, 0.4970, 0.5194,\n",
      "         0.5123, 0.5178, 0.4982, 0.4937, 0.5126, 0.4831, 0.4902, 0.5119, 0.5341,\n",
      "         0.4925, 0.5201, 0.4993, 0.5034, 0.4979, 0.4985, 0.5177, 0.4881, 0.5240,\n",
      "         0.4884, 0.4809, 0.5040, 0.5233, 0.5171, 0.4843]], device='cuda:0')\n",
      "X Shape torch.Size([30, 2, 68])\n",
      "pred = tensor([[0.4856, 0.4807, 0.4899, 0.5064, 0.5138, 0.5169, 0.4708, 0.4945, 0.5160,\n",
      "         0.5065, 0.5152, 0.4952, 0.4899, 0.5086, 0.4798, 0.4872, 0.5070, 0.5296,\n",
      "         0.4896, 0.5166, 0.4958, 0.5018, 0.4942, 0.4953, 0.5161, 0.4844, 0.5197,\n",
      "         0.4843, 0.4772, 0.5011, 0.5195, 0.5116, 0.4792],\n",
      "        [0.4917, 0.4857, 0.4932, 0.5108, 0.5181, 0.5222, 0.4762, 0.4980, 0.5209,\n",
      "         0.5121, 0.5206, 0.4998, 0.4953, 0.5133, 0.4833, 0.4911, 0.5123, 0.5358,\n",
      "         0.4949, 0.5213, 0.5017, 0.5055, 0.4992, 0.4999, 0.5188, 0.4896, 0.5257,\n",
      "         0.4893, 0.4816, 0.5055, 0.5249, 0.5175, 0.4851],\n",
      "        [0.4900, 0.4844, 0.4923, 0.5093, 0.5161, 0.5210, 0.4739, 0.4980, 0.5188,\n",
      "         0.5105, 0.5169, 0.4982, 0.4936, 0.5115, 0.4827, 0.4906, 0.5104, 0.5344,\n",
      "         0.4949, 0.5193, 0.4997, 0.5035, 0.4988, 0.4991, 0.5178, 0.4879, 0.5233,\n",
      "         0.4891, 0.4796, 0.5035, 0.5228, 0.5158, 0.4831],\n",
      "        [0.5059, 0.4969, 0.5018, 0.5209, 0.5277, 0.5325, 0.4883, 0.5077, 0.5313,\n",
      "         0.5249, 0.5347, 0.5085, 0.5057, 0.5246, 0.4943, 0.5011, 0.5249, 0.5439,\n",
      "         0.5065, 0.5328, 0.5115, 0.5129, 0.5075, 0.5087, 0.5262, 0.5018, 0.5374,\n",
      "         0.5005, 0.4922, 0.5154, 0.5374, 0.5306, 0.4969],\n",
      "        [0.4895, 0.4841, 0.4938, 0.5104, 0.5162, 0.5198, 0.4753, 0.4973, 0.5186,\n",
      "         0.5094, 0.5200, 0.4975, 0.4934, 0.5128, 0.4834, 0.4898, 0.5098, 0.5332,\n",
      "         0.4929, 0.5196, 0.4989, 0.5042, 0.4955, 0.4971, 0.5189, 0.4886, 0.5248,\n",
      "         0.4867, 0.4819, 0.5045, 0.5232, 0.5160, 0.4837],\n",
      "        [0.4894, 0.4838, 0.4910, 0.5074, 0.5160, 0.5191, 0.4735, 0.4959, 0.5180,\n",
      "         0.5109, 0.5179, 0.4973, 0.4923, 0.5107, 0.4822, 0.4890, 0.5111, 0.5310,\n",
      "         0.4922, 0.5190, 0.4982, 0.5022, 0.4967, 0.4977, 0.5168, 0.4861, 0.5217,\n",
      "         0.4871, 0.4790, 0.5032, 0.5228, 0.5151, 0.4824],\n",
      "        [0.4918, 0.4859, 0.4934, 0.5109, 0.5183, 0.5211, 0.4767, 0.4980, 0.5206,\n",
      "         0.5118, 0.5223, 0.4988, 0.4950, 0.5144, 0.4848, 0.4912, 0.5125, 0.5331,\n",
      "         0.4939, 0.5217, 0.5004, 0.5049, 0.4972, 0.4988, 0.5192, 0.4898, 0.5256,\n",
      "         0.4883, 0.4825, 0.5059, 0.5256, 0.5180, 0.4856],\n",
      "        [0.4886, 0.4829, 0.4915, 0.5084, 0.5155, 0.5189, 0.4734, 0.4964, 0.5178,\n",
      "         0.5094, 0.5181, 0.4972, 0.4921, 0.5111, 0.4815, 0.4885, 0.5092, 0.5315,\n",
      "         0.4914, 0.5188, 0.4976, 0.5032, 0.4954, 0.4965, 0.5173, 0.4867, 0.5224,\n",
      "         0.4861, 0.4797, 0.5030, 0.5218, 0.5145, 0.4822],\n",
      "        [0.4940, 0.4880, 0.4958, 0.5139, 0.5207, 0.5238, 0.4789, 0.5006, 0.5233,\n",
      "         0.5139, 0.5239, 0.5009, 0.4977, 0.5169, 0.4864, 0.4938, 0.5142, 0.5376,\n",
      "         0.4970, 0.5243, 0.5031, 0.5080, 0.4997, 0.5013, 0.5215, 0.4930, 0.5293,\n",
      "         0.4913, 0.4848, 0.5078, 0.5273, 0.5210, 0.4878],\n",
      "        [0.5035, 0.4954, 0.5010, 0.5208, 0.5271, 0.5319, 0.4863, 0.5073, 0.5304,\n",
      "         0.5208, 0.5328, 0.5079, 0.5057, 0.5240, 0.4921, 0.5015, 0.5227, 0.5447,\n",
      "         0.5067, 0.5313, 0.5121, 0.5137, 0.5071, 0.5082, 0.5264, 0.5004, 0.5371,\n",
      "         0.4994, 0.4907, 0.5143, 0.5357, 0.5290, 0.4948],\n",
      "        [0.4984, 0.4913, 0.4970, 0.5156, 0.5229, 0.5268, 0.4818, 0.5030, 0.5258,\n",
      "         0.5168, 0.5280, 0.5037, 0.5002, 0.5194, 0.4893, 0.4968, 0.5181, 0.5382,\n",
      "         0.5007, 0.5266, 0.5061, 0.5087, 0.5026, 0.5039, 0.5227, 0.4950, 0.5310,\n",
      "         0.4947, 0.4865, 0.5101, 0.5312, 0.5234, 0.4901],\n",
      "        [0.4958, 0.4886, 0.4978, 0.5157, 0.5210, 0.5246, 0.4811, 0.5016, 0.5241,\n",
      "         0.5159, 0.5265, 0.5014, 0.4987, 0.5183, 0.4879, 0.4942, 0.5151, 0.5379,\n",
      "         0.4973, 0.5254, 0.5037, 0.5085, 0.4992, 0.5011, 0.5223, 0.4944, 0.5308,\n",
      "         0.4917, 0.4866, 0.5090, 0.5293, 0.5224, 0.4899],\n",
      "        [0.4914, 0.4856, 0.4920, 0.5094, 0.5178, 0.5214, 0.4751, 0.4977, 0.5201,\n",
      "         0.5127, 0.5187, 0.4995, 0.4943, 0.5121, 0.4827, 0.4910, 0.5130, 0.5348,\n",
      "         0.4945, 0.5211, 0.5007, 0.5039, 0.4996, 0.5000, 0.5178, 0.4886, 0.5237,\n",
      "         0.4897, 0.4802, 0.5047, 0.5240, 0.5172, 0.4839],\n",
      "        [0.5074, 0.4984, 0.5032, 0.5230, 0.5293, 0.5343, 0.4897, 0.5099, 0.5327,\n",
      "         0.5232, 0.5378, 0.5096, 0.5075, 0.5268, 0.4953, 0.5041, 0.5255, 0.5453,\n",
      "         0.5098, 0.5340, 0.5137, 0.5153, 0.5084, 0.5101, 0.5281, 0.5033, 0.5397,\n",
      "         0.5021, 0.4934, 0.5167, 0.5392, 0.5316, 0.4977],\n",
      "        [0.4981, 0.4907, 0.4975, 0.5154, 0.5220, 0.5264, 0.4816, 0.5026, 0.5251,\n",
      "         0.5169, 0.5269, 0.5033, 0.4999, 0.5187, 0.4888, 0.4963, 0.5175, 0.5384,\n",
      "         0.5005, 0.5261, 0.5058, 0.5081, 0.5025, 0.5035, 0.5225, 0.4949, 0.5306,\n",
      "         0.4947, 0.4859, 0.5097, 0.5311, 0.5230, 0.4902],\n",
      "        [0.4851, 0.4804, 0.4895, 0.5059, 0.5137, 0.5154, 0.4704, 0.4941, 0.5152,\n",
      "         0.5055, 0.5158, 0.4942, 0.4891, 0.5092, 0.4799, 0.4870, 0.5064, 0.5273,\n",
      "         0.4886, 0.5161, 0.4947, 0.5013, 0.4922, 0.4939, 0.5164, 0.4835, 0.5197,\n",
      "         0.4831, 0.4774, 0.5007, 0.5192, 0.5115, 0.4787],\n",
      "        [0.4941, 0.4875, 0.4938, 0.5114, 0.5197, 0.5228, 0.4779, 0.4994, 0.5221,\n",
      "         0.5149, 0.5223, 0.5001, 0.4963, 0.5155, 0.4858, 0.4928, 0.5150, 0.5349,\n",
      "         0.4957, 0.5225, 0.5021, 0.5053, 0.4995, 0.5008, 0.5197, 0.4902, 0.5266,\n",
      "         0.4909, 0.4829, 0.5067, 0.5273, 0.5195, 0.4871],\n",
      "        [0.4790, 0.4760, 0.4862, 0.5019, 0.5092, 0.5118, 0.4651, 0.4905, 0.5107,\n",
      "         0.5009, 0.5081, 0.4909, 0.4850, 0.5040, 0.4751, 0.4824, 0.5010, 0.5260,\n",
      "         0.4842, 0.5113, 0.4907, 0.4984, 0.4901, 0.4910, 0.5131, 0.4792, 0.5149,\n",
      "         0.4792, 0.4731, 0.4964, 0.5128, 0.5065, 0.4739],\n",
      "        [0.4864, 0.4817, 0.4897, 0.5064, 0.5139, 0.5169, 0.4712, 0.4944, 0.5163,\n",
      "         0.5074, 0.5144, 0.4952, 0.4899, 0.5093, 0.4804, 0.4878, 0.5078, 0.5298,\n",
      "         0.4899, 0.5161, 0.4962, 0.5009, 0.4947, 0.4956, 0.5161, 0.4841, 0.5203,\n",
      "         0.4851, 0.4776, 0.5010, 0.5202, 0.5125, 0.4801],\n",
      "        [0.4813, 0.4775, 0.4866, 0.5029, 0.5109, 0.5136, 0.4668, 0.4914, 0.5127,\n",
      "         0.5028, 0.5107, 0.4928, 0.4866, 0.5051, 0.4758, 0.4836, 0.5031, 0.5271,\n",
      "         0.4858, 0.5131, 0.4927, 0.4994, 0.4917, 0.4926, 0.5136, 0.4808, 0.5163,\n",
      "         0.4807, 0.4741, 0.4979, 0.5150, 0.5081, 0.4756],\n",
      "        [0.4881, 0.4830, 0.4911, 0.5081, 0.5157, 0.5185, 0.4727, 0.4962, 0.5175,\n",
      "         0.5087, 0.5176, 0.4968, 0.4916, 0.5111, 0.4816, 0.4890, 0.5093, 0.5312,\n",
      "         0.4919, 0.5185, 0.4976, 0.5034, 0.4955, 0.4966, 0.5175, 0.4862, 0.5222,\n",
      "         0.4859, 0.4793, 0.5027, 0.5213, 0.5143, 0.4812],\n",
      "        [0.4774, 0.4749, 0.4846, 0.4991, 0.5072, 0.5095, 0.4636, 0.4882, 0.5086,\n",
      "         0.5017, 0.5060, 0.4896, 0.4827, 0.5017, 0.4742, 0.4798, 0.5005, 0.5235,\n",
      "         0.4813, 0.5098, 0.4884, 0.4955, 0.4888, 0.4889, 0.5109, 0.4771, 0.5121,\n",
      "         0.4775, 0.4713, 0.4947, 0.5110, 0.5052, 0.4727],\n",
      "        [0.4760, 0.4736, 0.4835, 0.4986, 0.5069, 0.5086, 0.4625, 0.4877, 0.5084,\n",
      "         0.4988, 0.5058, 0.4887, 0.4818, 0.5013, 0.4732, 0.4798, 0.4992, 0.5222,\n",
      "         0.4807, 0.5088, 0.4877, 0.4955, 0.4872, 0.4882, 0.5108, 0.4760, 0.5115,\n",
      "         0.4762, 0.4706, 0.4938, 0.5105, 0.5039, 0.4713],\n",
      "        [0.4786, 0.4753, 0.4851, 0.4999, 0.5085, 0.5104, 0.4644, 0.4891, 0.5101,\n",
      "         0.5020, 0.5079, 0.4898, 0.4837, 0.5030, 0.4752, 0.4812, 0.5018, 0.5226,\n",
      "         0.4819, 0.5106, 0.4890, 0.4965, 0.4887, 0.4894, 0.5119, 0.4781, 0.5127,\n",
      "         0.4777, 0.4722, 0.4957, 0.5124, 0.5057, 0.4728],\n",
      "        [0.5042, 0.4961, 0.5020, 0.5213, 0.5269, 0.5317, 0.4875, 0.5073, 0.5300,\n",
      "         0.5209, 0.5344, 0.5076, 0.5052, 0.5248, 0.4937, 0.5014, 0.5226, 0.5438,\n",
      "         0.5068, 0.5316, 0.5115, 0.5128, 0.5061, 0.5079, 0.5263, 0.5007, 0.5375,\n",
      "         0.4992, 0.4917, 0.5147, 0.5370, 0.5292, 0.4959],\n",
      "        [0.4856, 0.4810, 0.4909, 0.5069, 0.5135, 0.5169, 0.4716, 0.4939, 0.5159,\n",
      "         0.5073, 0.5154, 0.4955, 0.4904, 0.5087, 0.4799, 0.4867, 0.5067, 0.5312,\n",
      "         0.4891, 0.5169, 0.4962, 0.5020, 0.4944, 0.4949, 0.5162, 0.4853, 0.5208,\n",
      "         0.4841, 0.4779, 0.5013, 0.5195, 0.5123, 0.4800],\n",
      "        [0.4836, 0.4793, 0.4881, 0.5042, 0.5122, 0.5147, 0.4688, 0.4928, 0.5141,\n",
      "         0.5054, 0.5122, 0.4933, 0.4881, 0.5072, 0.4788, 0.4856, 0.5055, 0.5275,\n",
      "         0.4874, 0.5147, 0.4938, 0.4996, 0.4929, 0.4939, 0.5146, 0.4822, 0.5176,\n",
      "         0.4828, 0.4754, 0.4994, 0.5177, 0.5102, 0.4779],\n",
      "        [0.4708, 0.4699, 0.4806, 0.4952, 0.5038, 0.5048, 0.4578, 0.4849, 0.5045,\n",
      "         0.4942, 0.5005, 0.4855, 0.4779, 0.4972, 0.4694, 0.4766, 0.4944, 0.5194,\n",
      "         0.4772, 0.5053, 0.4838, 0.4934, 0.4845, 0.4855, 0.5087, 0.4726, 0.5075,\n",
      "         0.4726, 0.4668, 0.4902, 0.5054, 0.4998, 0.4667],\n",
      "        [0.4820, 0.4787, 0.4871, 0.5039, 0.5121, 0.5138, 0.4676, 0.4924, 0.5137,\n",
      "         0.5024, 0.5119, 0.4927, 0.4869, 0.5067, 0.4776, 0.4854, 0.5041, 0.5268,\n",
      "         0.4870, 0.5135, 0.4931, 0.4998, 0.4917, 0.4933, 0.5146, 0.4812, 0.5174,\n",
      "         0.4819, 0.4752, 0.4985, 0.5164, 0.5090, 0.4763],\n",
      "        [0.4783, 0.4752, 0.4855, 0.5015, 0.5093, 0.5112, 0.4646, 0.4899, 0.5109,\n",
      "         0.4999, 0.5083, 0.4910, 0.4843, 0.5033, 0.4738, 0.4819, 0.5001, 0.5257,\n",
      "         0.4833, 0.5110, 0.4903, 0.4985, 0.4891, 0.4901, 0.5130, 0.4791, 0.5151,\n",
      "         0.4783, 0.4730, 0.4959, 0.5123, 0.5059, 0.4733]], device='cuda:0')\n",
      "X Shape torch.Size([30, 2, 68])\n",
      "pred = tensor([[0.4878, 0.4821, 0.4919, 0.5084, 0.5152, 0.5177, 0.4735, 0.4955, 0.5175,\n",
      "         0.5076, 0.5187, 0.4963, 0.4913, 0.5112, 0.4811, 0.4883, 0.5087, 0.5309,\n",
      "         0.4904, 0.5184, 0.4971, 0.5033, 0.4938, 0.4949, 0.5173, 0.4862, 0.5225,\n",
      "         0.4849, 0.4799, 0.5025, 0.5218, 0.5139, 0.4818],\n",
      "        [0.5074, 0.4982, 0.5030, 0.5223, 0.5291, 0.5342, 0.4890, 0.5095, 0.5324,\n",
      "         0.5246, 0.5354, 0.5092, 0.5070, 0.5260, 0.4953, 0.5040, 0.5256, 0.5456,\n",
      "         0.5093, 0.5335, 0.5132, 0.5140, 0.5090, 0.5103, 0.5275, 0.5028, 0.5391,\n",
      "         0.5028, 0.4927, 0.5160, 0.5390, 0.5317, 0.4977],\n",
      "        [0.4845, 0.4804, 0.4884, 0.5048, 0.5134, 0.5161, 0.4694, 0.4935, 0.5151,\n",
      "         0.5060, 0.5126, 0.4946, 0.4893, 0.5075, 0.4787, 0.4867, 0.5061, 0.5298,\n",
      "         0.4891, 0.5154, 0.4952, 0.5007, 0.4947, 0.4955, 0.5152, 0.4830, 0.5189,\n",
      "         0.4843, 0.4758, 0.5000, 0.5184, 0.5112, 0.4785],\n",
      "        [0.4863, 0.4810, 0.4905, 0.5059, 0.5134, 0.5162, 0.4718, 0.4935, 0.5155,\n",
      "         0.5087, 0.5161, 0.4948, 0.4899, 0.5089, 0.4807, 0.4863, 0.5080, 0.5288,\n",
      "         0.4879, 0.5169, 0.4951, 0.5006, 0.4932, 0.4936, 0.5155, 0.4844, 0.5197,\n",
      "         0.4836, 0.4781, 0.5012, 0.5197, 0.5126, 0.4803],\n",
      "        [0.4864, 0.4812, 0.4904, 0.5068, 0.5139, 0.5170, 0.4717, 0.4942, 0.5161,\n",
      "         0.5075, 0.5160, 0.4959, 0.4903, 0.5091, 0.4798, 0.4868, 0.5073, 0.5303,\n",
      "         0.4895, 0.5171, 0.4961, 0.5017, 0.4942, 0.4950, 0.5161, 0.4849, 0.5206,\n",
      "         0.4844, 0.4781, 0.5013, 0.5201, 0.5126, 0.4804],\n",
      "        [0.4787, 0.4755, 0.4844, 0.4996, 0.5085, 0.5104, 0.4643, 0.4890, 0.5100,\n",
      "         0.5036, 0.5058, 0.4903, 0.4834, 0.5024, 0.4746, 0.4810, 0.5021, 0.5243,\n",
      "         0.4819, 0.5105, 0.4892, 0.4957, 0.4902, 0.4902, 0.5112, 0.4773, 0.5127,\n",
      "         0.4793, 0.4714, 0.4953, 0.5121, 0.5061, 0.4737],\n",
      "        [0.4991, 0.4919, 0.4980, 0.5168, 0.5237, 0.5279, 0.4829, 0.5032, 0.5268,\n",
      "         0.5179, 0.5281, 0.5044, 0.5014, 0.5203, 0.4892, 0.4971, 0.5191, 0.5408,\n",
      "         0.5010, 0.5271, 0.5076, 0.5097, 0.5035, 0.5049, 0.5230, 0.4958, 0.5324,\n",
      "         0.4953, 0.4877, 0.5110, 0.5319, 0.5246, 0.4918],\n",
      "        [0.4745, 0.4720, 0.4824, 0.4965, 0.5052, 0.5071, 0.4608, 0.4864, 0.5066,\n",
      "         0.4992, 0.5035, 0.4871, 0.4802, 0.4991, 0.4721, 0.4773, 0.4973, 0.5199,\n",
      "         0.4784, 0.5069, 0.4855, 0.4935, 0.4860, 0.4867, 0.5094, 0.4745, 0.5092,\n",
      "         0.4746, 0.4690, 0.4924, 0.5085, 0.5020, 0.4700],\n",
      "        [0.4716, 0.4703, 0.4803, 0.4947, 0.5042, 0.5047, 0.4581, 0.4848, 0.5049,\n",
      "         0.4943, 0.5018, 0.4853, 0.4779, 0.4981, 0.4704, 0.4767, 0.4953, 0.5174,\n",
      "         0.4770, 0.5048, 0.4835, 0.4928, 0.4838, 0.4848, 0.5089, 0.4722, 0.5072,\n",
      "         0.4725, 0.4675, 0.4905, 0.5062, 0.4999, 0.4671],\n",
      "        [0.4906, 0.4852, 0.4932, 0.5103, 0.5177, 0.5206, 0.4752, 0.4977, 0.5198,\n",
      "         0.5101, 0.5201, 0.4986, 0.4941, 0.5132, 0.4833, 0.4910, 0.5108, 0.5337,\n",
      "         0.4946, 0.5202, 0.5003, 0.5049, 0.4972, 0.4985, 0.5193, 0.4887, 0.5255,\n",
      "         0.4883, 0.4816, 0.5046, 0.5243, 0.5166, 0.4838],\n",
      "        [0.4847, 0.4801, 0.4883, 0.5045, 0.5135, 0.5152, 0.4695, 0.4936, 0.5151,\n",
      "         0.5060, 0.5143, 0.4942, 0.4887, 0.5082, 0.4790, 0.4861, 0.5063, 0.5274,\n",
      "         0.4883, 0.5155, 0.4945, 0.5007, 0.4930, 0.4943, 0.5154, 0.4825, 0.5187,\n",
      "         0.4835, 0.4762, 0.4999, 0.5186, 0.5111, 0.4785],\n",
      "        [0.4858, 0.4811, 0.4900, 0.5069, 0.5147, 0.5170, 0.4711, 0.4947, 0.5164,\n",
      "         0.5060, 0.5158, 0.4956, 0.4903, 0.5090, 0.4795, 0.4879, 0.5072, 0.5307,\n",
      "         0.4903, 0.5174, 0.4963, 0.5029, 0.4946, 0.4957, 0.5165, 0.4848, 0.5206,\n",
      "         0.4848, 0.4776, 0.5014, 0.5197, 0.5125, 0.4795],\n",
      "        [0.4907, 0.4850, 0.4926, 0.5095, 0.5172, 0.5197, 0.4755, 0.4972, 0.5195,\n",
      "         0.5117, 0.5196, 0.4976, 0.4934, 0.5133, 0.4841, 0.4905, 0.5124, 0.5321,\n",
      "         0.4928, 0.5205, 0.4986, 0.5038, 0.4966, 0.4976, 0.5184, 0.4881, 0.5240,\n",
      "         0.4880, 0.4812, 0.5043, 0.5241, 0.5172, 0.4842],\n",
      "        [0.4894, 0.4839, 0.4906, 0.5078, 0.5162, 0.5193, 0.4735, 0.4960, 0.5183,\n",
      "         0.5103, 0.5175, 0.4975, 0.4923, 0.5109, 0.4819, 0.4894, 0.5105, 0.5318,\n",
      "         0.4923, 0.5189, 0.4986, 0.5023, 0.4971, 0.4980, 0.5168, 0.4859, 0.5219,\n",
      "         0.4875, 0.4787, 0.5029, 0.5230, 0.5151, 0.4826],\n",
      "        [0.4781, 0.4752, 0.4847, 0.5002, 0.5084, 0.5103, 0.4640, 0.4891, 0.5103,\n",
      "         0.5009, 0.5065, 0.4897, 0.4833, 0.5029, 0.4746, 0.4815, 0.5009, 0.5238,\n",
      "         0.4824, 0.5099, 0.4892, 0.4965, 0.4891, 0.4898, 0.5119, 0.4777, 0.5133,\n",
      "         0.4784, 0.4721, 0.4952, 0.5122, 0.5055, 0.4731],\n",
      "        [0.4974, 0.4908, 0.4961, 0.5148, 0.5225, 0.5265, 0.4805, 0.5027, 0.5253,\n",
      "         0.5154, 0.5258, 0.5030, 0.4994, 0.5186, 0.4882, 0.4968, 0.5176, 0.5384,\n",
      "         0.5006, 0.5255, 0.5055, 0.5084, 0.5027, 0.5043, 0.5221, 0.4943, 0.5303,\n",
      "         0.4947, 0.4858, 0.5091, 0.5301, 0.5228, 0.4893],\n",
      "        [0.4856, 0.4810, 0.4895, 0.5064, 0.5142, 0.5168, 0.4708, 0.4945, 0.5162,\n",
      "         0.5068, 0.5144, 0.4950, 0.4901, 0.5091, 0.4793, 0.4874, 0.5071, 0.5304,\n",
      "         0.4895, 0.5167, 0.4960, 0.5022, 0.4945, 0.4953, 0.5161, 0.4843, 0.5205,\n",
      "         0.4845, 0.4773, 0.5009, 0.5194, 0.5124, 0.4797],\n",
      "        [0.4850, 0.4807, 0.4885, 0.5057, 0.5136, 0.5164, 0.4700, 0.4940, 0.5153,\n",
      "         0.5065, 0.5132, 0.4950, 0.4894, 0.5081, 0.4788, 0.4868, 0.5066, 0.5302,\n",
      "         0.4894, 0.5161, 0.4954, 0.5011, 0.4947, 0.4955, 0.5152, 0.4835, 0.5192,\n",
      "         0.4843, 0.4763, 0.5004, 0.5186, 0.5117, 0.4791],\n",
      "        [0.4698, 0.4690, 0.4791, 0.4933, 0.5032, 0.5029, 0.4570, 0.4836, 0.5028,\n",
      "         0.4934, 0.5001, 0.4838, 0.4767, 0.4968, 0.4691, 0.4747, 0.4936, 0.5164,\n",
      "         0.4745, 0.5029, 0.4814, 0.4914, 0.4816, 0.4823, 0.5079, 0.4707, 0.5059,\n",
      "         0.4699, 0.4668, 0.4888, 0.5037, 0.4983, 0.4657],\n",
      "        [0.4817, 0.4779, 0.4853, 0.5011, 0.5104, 0.5129, 0.4664, 0.4903, 0.5123,\n",
      "         0.5053, 0.5089, 0.4921, 0.4858, 0.5042, 0.4769, 0.4833, 0.5045, 0.5252,\n",
      "         0.4851, 0.5123, 0.4918, 0.4967, 0.4923, 0.4928, 0.5123, 0.4790, 0.5144,\n",
      "         0.4814, 0.4728, 0.4973, 0.5157, 0.5082, 0.4758],\n",
      "        [0.4871, 0.4829, 0.4900, 0.5068, 0.5141, 0.5177, 0.4720, 0.4944, 0.5165,\n",
      "         0.5084, 0.5151, 0.4961, 0.4908, 0.5097, 0.4816, 0.4879, 0.5084, 0.5306,\n",
      "         0.4909, 0.5169, 0.4970, 0.5004, 0.4960, 0.4967, 0.5158, 0.4848, 0.5207,\n",
      "         0.4860, 0.4781, 0.5017, 0.5210, 0.5136, 0.4814],\n",
      "        [0.4778, 0.4750, 0.4858, 0.5012, 0.5078, 0.5109, 0.4646, 0.4895, 0.5098,\n",
      "         0.5008, 0.5065, 0.4900, 0.4842, 0.5030, 0.4748, 0.4812, 0.4999, 0.5257,\n",
      "         0.4827, 0.5103, 0.4895, 0.4971, 0.4894, 0.4902, 0.5122, 0.4788, 0.5139,\n",
      "         0.4783, 0.4725, 0.4956, 0.5121, 0.5060, 0.4739],\n",
      "        [0.4971, 0.4899, 0.4978, 0.5164, 0.5224, 0.5266, 0.4815, 0.5027, 0.5253,\n",
      "         0.5153, 0.5277, 0.5034, 0.5002, 0.5188, 0.4875, 0.4957, 0.5163, 0.5400,\n",
      "         0.5001, 0.5263, 0.5059, 0.5103, 0.5016, 0.5032, 0.5229, 0.4957, 0.5317,\n",
      "         0.4933, 0.4869, 0.5099, 0.5299, 0.5228, 0.4899],\n",
      "        [0.4815, 0.4780, 0.4853, 0.5014, 0.5105, 0.5131, 0.4661, 0.4906, 0.5124,\n",
      "         0.5038, 0.5092, 0.4921, 0.4857, 0.5042, 0.4765, 0.4838, 0.5040, 0.5253,\n",
      "         0.4859, 0.5121, 0.4920, 0.4972, 0.4922, 0.4931, 0.5123, 0.4787, 0.5145,\n",
      "         0.4814, 0.4728, 0.4971, 0.5156, 0.5078, 0.4754],\n",
      "        [0.5004, 0.4929, 0.4994, 0.5183, 0.5245, 0.5287, 0.4841, 0.5050, 0.5275,\n",
      "         0.5188, 0.5304, 0.5051, 0.5024, 0.5216, 0.4908, 0.4981, 0.5198, 0.5413,\n",
      "         0.5028, 0.5288, 0.5082, 0.5112, 0.5039, 0.5055, 0.5244, 0.4979, 0.5338,\n",
      "         0.4963, 0.4888, 0.5121, 0.5330, 0.5260, 0.4927],\n",
      "        [0.4778, 0.4752, 0.4856, 0.5010, 0.5085, 0.5108, 0.4641, 0.4893, 0.5100,\n",
      "         0.4995, 0.5074, 0.4906, 0.4838, 0.5026, 0.4740, 0.4820, 0.5000, 0.5256,\n",
      "         0.4837, 0.5103, 0.4903, 0.4975, 0.4895, 0.4901, 0.5125, 0.4784, 0.5142,\n",
      "         0.4785, 0.4721, 0.4954, 0.5122, 0.5055, 0.4727],\n",
      "        [0.4937, 0.4879, 0.4941, 0.5117, 0.5199, 0.5226, 0.4778, 0.4992, 0.5222,\n",
      "         0.5138, 0.5224, 0.5002, 0.4966, 0.5155, 0.4862, 0.4932, 0.5144, 0.5351,\n",
      "         0.4963, 0.5229, 0.5021, 0.5054, 0.4998, 0.5012, 0.5200, 0.4908, 0.5267,\n",
      "         0.4915, 0.4829, 0.5068, 0.5274, 0.5200, 0.4869],\n",
      "        [0.4741, 0.4724, 0.4827, 0.4975, 0.5059, 0.5071, 0.4609, 0.4867, 0.5068,\n",
      "         0.4967, 0.5042, 0.4873, 0.4806, 0.5001, 0.4720, 0.4786, 0.4971, 0.5212,\n",
      "         0.4797, 0.5069, 0.4864, 0.4948, 0.4859, 0.4870, 0.5105, 0.4749, 0.5105,\n",
      "         0.4748, 0.4697, 0.4926, 0.5089, 0.5024, 0.4700],\n",
      "        [0.4698, 0.4688, 0.4803, 0.4941, 0.5026, 0.5036, 0.4578, 0.4839, 0.5032,\n",
      "         0.4942, 0.4997, 0.4843, 0.4771, 0.4963, 0.4689, 0.4746, 0.4930, 0.5186,\n",
      "         0.4749, 0.5036, 0.4825, 0.4919, 0.4830, 0.4834, 0.5079, 0.4720, 0.5065,\n",
      "         0.4709, 0.4663, 0.4892, 0.5043, 0.4983, 0.4664],\n",
      "        [0.4874, 0.4830, 0.4900, 0.5073, 0.5156, 0.5181, 0.4720, 0.4952, 0.5176,\n",
      "         0.5075, 0.5168, 0.4967, 0.4911, 0.5103, 0.4812, 0.4889, 0.5090, 0.5307,\n",
      "         0.4916, 0.5176, 0.4975, 0.5022, 0.4956, 0.4969, 0.5170, 0.4853, 0.5217,\n",
      "         0.4862, 0.4789, 0.5021, 0.5212, 0.5139, 0.4808]], device='cuda:0')\n",
      "X Shape torch.Size([10, 2, 68])\n",
      "pred = tensor([[0.4801, 0.4767, 0.4854, 0.5013, 0.5090, 0.5124, 0.4652, 0.4901, 0.5111,\n",
      "         0.5023, 0.5071, 0.4912, 0.4848, 0.5034, 0.4757, 0.4830, 0.5020, 0.5256,\n",
      "         0.4850, 0.5108, 0.4911, 0.4965, 0.4914, 0.4919, 0.5121, 0.4782, 0.5140,\n",
      "         0.4803, 0.4724, 0.4961, 0.5141, 0.5066, 0.4745],\n",
      "        [0.4862, 0.4814, 0.4898, 0.5063, 0.5138, 0.5169, 0.4710, 0.4947, 0.5158,\n",
      "         0.5076, 0.5153, 0.4952, 0.4902, 0.5089, 0.4803, 0.4872, 0.5074, 0.5294,\n",
      "         0.4903, 0.5170, 0.4960, 0.5015, 0.4948, 0.4956, 0.5159, 0.4845, 0.5197,\n",
      "         0.4846, 0.4771, 0.5014, 0.5198, 0.5123, 0.4798],\n",
      "        [0.4823, 0.4783, 0.4874, 0.5032, 0.5115, 0.5139, 0.4678, 0.4922, 0.5134,\n",
      "         0.5039, 0.5120, 0.4925, 0.4870, 0.5063, 0.4777, 0.4846, 0.5048, 0.5262,\n",
      "         0.4865, 0.5135, 0.4929, 0.4993, 0.4914, 0.4928, 0.5143, 0.4811, 0.5168,\n",
      "         0.4813, 0.4749, 0.4986, 0.5164, 0.5089, 0.4763],\n",
      "        [0.4723, 0.4708, 0.4815, 0.4965, 0.5053, 0.5058, 0.4590, 0.4864, 0.5058,\n",
      "         0.4954, 0.5026, 0.4861, 0.4797, 0.4992, 0.4708, 0.4772, 0.4952, 0.5196,\n",
      "         0.4776, 0.5055, 0.4846, 0.4946, 0.4843, 0.4854, 0.5100, 0.4739, 0.5094,\n",
      "         0.4729, 0.4689, 0.4915, 0.5064, 0.5004, 0.4680],\n",
      "        [0.4853, 0.4811, 0.4878, 0.5040, 0.5130, 0.5160, 0.4696, 0.4930, 0.5153,\n",
      "         0.5087, 0.5116, 0.4941, 0.4889, 0.5076, 0.4799, 0.4860, 0.5081, 0.5282,\n",
      "         0.4880, 0.5151, 0.4946, 0.4988, 0.4948, 0.4955, 0.5142, 0.4826, 0.5176,\n",
      "         0.4845, 0.4760, 0.5003, 0.5185, 0.5119, 0.4792],\n",
      "        [0.5038, 0.4951, 0.5011, 0.5206, 0.5266, 0.5317, 0.4866, 0.5073, 0.5302,\n",
      "         0.5222, 0.5318, 0.5075, 0.5055, 0.5236, 0.4924, 0.5007, 0.5223, 0.5451,\n",
      "         0.5061, 0.5314, 0.5112, 0.5130, 0.5075, 0.5081, 0.5259, 0.5012, 0.5365,\n",
      "         0.4997, 0.4904, 0.5142, 0.5355, 0.5291, 0.4955],\n",
      "        [0.4931, 0.4872, 0.4952, 0.5127, 0.5190, 0.5226, 0.4779, 0.4999, 0.5214,\n",
      "         0.5134, 0.5221, 0.4997, 0.4964, 0.5160, 0.4860, 0.4928, 0.5138, 0.5361,\n",
      "         0.4961, 0.5224, 0.5021, 0.5062, 0.4989, 0.5003, 0.5205, 0.4915, 0.5277,\n",
      "         0.4903, 0.4838, 0.5068, 0.5263, 0.5197, 0.4868],\n",
      "        [0.4883, 0.4829, 0.4914, 0.5085, 0.5154, 0.5194, 0.4731, 0.4960, 0.5178,\n",
      "         0.5091, 0.5170, 0.4975, 0.4921, 0.5100, 0.4808, 0.4887, 0.5088, 0.5332,\n",
      "         0.4926, 0.5184, 0.4985, 0.5034, 0.4967, 0.4974, 0.5171, 0.4865, 0.5225,\n",
      "         0.4866, 0.4789, 0.5027, 0.5218, 0.5141, 0.4818],\n",
      "        [0.4833, 0.4797, 0.4889, 0.5053, 0.5124, 0.5148, 0.4691, 0.4933, 0.5140,\n",
      "         0.5042, 0.5130, 0.4936, 0.4883, 0.5079, 0.4789, 0.4861, 0.5053, 0.5283,\n",
      "         0.4881, 0.5150, 0.4941, 0.5006, 0.4925, 0.4938, 0.5155, 0.4830, 0.5186,\n",
      "         0.4826, 0.4765, 0.4999, 0.5174, 0.5106, 0.4776],\n",
      "        [0.4938, 0.4875, 0.4939, 0.5121, 0.5200, 0.5232, 0.4777, 0.4998, 0.5224,\n",
      "         0.5122, 0.5238, 0.5012, 0.4965, 0.5155, 0.4850, 0.4933, 0.5141, 0.5354,\n",
      "         0.4973, 0.5228, 0.5028, 0.5067, 0.4998, 0.5011, 0.5203, 0.4910, 0.5271,\n",
      "         0.4912, 0.4831, 0.5067, 0.5271, 0.5191, 0.4863]], device='cuda:0')\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test(test_loader, model, criterion)\n",
    "\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 68, 2) float32\n",
      "tensor([[0.2590, 0.2972, 0.2939, 0.3077, 0.3555, 0.3288, 0.2748, 0.3621, 0.3410,\n",
      "         0.3513, 0.2907, 0.3273, 0.3210, 0.3155, 0.3237, 0.3019, 0.3131, 0.3481,\n",
      "         0.2595, 0.3279, 0.3073, 0.3428, 0.3551, 0.3274, 0.3798, 0.2514, 0.3272,\n",
      "         0.3050, 0.2674, 0.3195, 0.3084, 0.3065, 0.3026]], device='cuda:0') torch.Size([1, 33])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_ = np.array([[\n",
    "    [-0.36369685665697504, -0.21546217565417758], [-0.3574112677601623, -\n",
    "                                                   0.12973942618903955], [-0.34360908442591476, -0.041386801250754224],\n",
    "    [-0.32728893521312186, 0.047003126885731494], [-0.3008596192879478,\n",
    "                                                   0.13302430193647485], [-0.24910143178451916, 0.20934664545504422],\n",
    "    [-0.1820489330188162, 0.27330297877009146], [-0.1047753579461298,\n",
    "                                                 0.32733666136376105], [-0.016907674584453414, 0.3462680344506007],\n",
    "    [0.0714076471556312, 0.3349838169948983], [0.15768994459377905,\n",
    "                                               0.290928739919908], [0.23164353502660584, 0.22906138570409784],\n",
    "    [0.28812057710241923, 0.15686104558670244], [0.32215974546052994,\n",
    "                                                 0.06921718141423006], [0.3461270503044599, -0.018575895551044996],\n",
    "    [0.3676136924680454, -0.1089242415930659], [0.3790284711174502, -\n",
    "                                                0.19942180042788948], [-0.29962861374732563, -0.29006857205551595],\n",
    "    [-0.25378298315870307, -0.3246486367875362], [-0.19323989247901685, -\n",
    "                                                  0.3313072576663558], [-0.13799385594382574, -0.3204147237917603],\n",
    "    [-0.08297163859783863, -0.2944143946458938], [0.0857693784628889, -\n",
    "                                                  0.2944330462449941], [0.146536288331779, -0.3161994623950847],\n",
    "    [0.20970925448461253, -0.330374677711339], [0.27513906412858635, -\n",
    "                                                0.3268868286795764], [0.32741949640682433, -0.2858160074606396],\n",
    "    [0.00400076800702176, -0.21505184047396997], [0.0031054912502057608, -\n",
    "                                                  0.1546206593888858], [-0.0002704481869548747, -0.09674474738054728],\n",
    "    [-0.0037209940205167324, -0.033832903615118703], [-0.07230292391244719,\n",
    "                                                      0.005447364090185869], [-0.03971858028416264, 0.01600416918097547],\n",
    "    [-0.004616270777332843, 0.02659827746996546], [0.0307844643151024,\n",
    "                                                   0.017048658730594135], [0.06366723352899217, 0.007461736793021978],\n",
    "    [-0.2304684842832848, -0.19837731087827104], [-0.19228866092489982, -\n",
    "                                                  0.22551538756925782], [-0.14192934335399632, -0.22476932360524426],\n",
    "    [-0.10212683087388219, -0.19143891601294627], [-0.14767403587689942, -\n",
    "                                                   0.17700257830928734], [-0.19803335344780293, -0.17774864227330067],\n",
    "    [0.10182840528827686, -0.1884173569586921], [0.14263810411980915, -\n",
    "                                                 0.22307202808711402], [0.19558999396565901, -0.22732459268199012],\n",
    "    [0.23802238191892033, -0.20151077952712704], [0.1999171649569368, -\n",
    "                                                  0.17940863459323053], [0.1470025783092873, -0.17767403587689945],\n",
    "    [-0.137005321191508, 0.12285918042679256], [-0.08882824071534368,\n",
    "                                                0.10090624828569872], [-0.04324373251412583, 0.08395194470349432],\n",
    "    [-0.008104119809095423, 0.09202808711393928], [0.027259312085139098,\n",
    "                                                   0.0849964342531132], [0.07483954139009275, 0.10333095616874222],\n",
    "    [0.1299363651324813, 0.12429535355751808], [0.07649953371002238,\n",
    "                                                0.16128147457348174], [0.030989631905205983, 0.17319984639859565],\n",
    "    [-0.006854462669372952, 0.17767623018267586], [-0.04706731032969447,\n",
    "                                                   0.17204344725437493], [-0.08961160787755773, 0.15378353173514736],\n",
    "    [-0.11182566240605629, 0.12323221240879911], [-0.04624663996927969,\n",
    "                                                  0.11664819792638104], [-0.008514454989302811, 0.11972571177793634],\n",
    "    [0.026774370508530265, 0.11772999067420054], [0.10723736902737391,\n",
    "                                                  0.1264775906522574], [0.0292550331888749, 0.1202852597509464],\n",
    "    [-0.006071095507158897, 0.12479894673322722], [-0.043765977288935165, 0.1192034670031269]]], dtype=np.float32)\n",
    "\n",
    "print(x_.shape, x_.dtype)\n",
    "model.eval()\n",
    "x = torch.Tensor(x_).to(device)\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    print(f'{pred} {pred.shape}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
