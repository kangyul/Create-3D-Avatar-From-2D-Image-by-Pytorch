{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Facial Recognition using PyTorch and OpenCV\n",
    "\n",
    "https://ritik12.medium.com/facial-recognition-using-pytorch-and-opencv-467c4e41d1f\n",
    "\n",
    "\n",
    "Machine Learning - Face Recognition CNN Pytorch.ipynb\n",
    "https://github.com/rubencg195/Pytorch-Tutorials/blob/master/Machine%20Learning%20-%20Face%20Recognition%20CNN%20Pytorch.ipynb\n",
    "\n",
    "\n",
    "\n",
    "Face Recognition Using Pytorch\n",
    "https://github.com/timesler/facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face Landmarks Detection With PyTorch\n",
    "\n",
    "https://towardsdatascience.com/face-landmarks-detection-with-pytorch-4b4852f5e9c4\n",
    "\n",
    "\n",
    "\n",
    "다중입력 deep neural network\n",
    "https://rosenfelder.ai/multi-input-neural-network-pytorch/\n",
    "\n",
    "\n",
    "\n",
    "Understanding dimensions in PyTorch\n",
    "https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.init import *\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from FaceFeatureDataset import FaceFeatureDataset\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 68])\n",
      "output_2d tensor([[[-5.4272e-01,  4.4423e-01,  6.0596e-01, -2.6507e-02, -1.0593e+00,\n",
      "           3.2728e-01, -7.2961e-01, -6.5087e-02,  1.1298e+00,  4.8103e-01,\n",
      "           6.8498e-01,  7.2168e-01, -7.0304e-01, -1.0294e+00,  1.3081e+00,\n",
      "          -4.7290e-01,  3.1143e-01,  3.4690e-01, -6.1350e-01, -6.2106e-01,\n",
      "           1.3366e-02,  1.2418e+00,  3.2735e-01,  1.1143e-01,  3.9702e-01,\n",
      "           8.4877e-01, -2.9535e-01, -3.7765e-01, -8.7391e-01,  4.2160e-01,\n",
      "          -2.5399e-01, -6.9817e-01, -1.2811e-01, -6.2158e-01,  3.7030e-01,\n",
      "           4.7983e-01, -6.6515e-02, -3.2970e-01, -5.8664e-01, -4.1730e-01,\n",
      "           2.6763e-02, -1.1138e-01, -4.4584e-02, -1.5761e-01,  5.1167e-02,\n",
      "          -9.0404e-01, -3.3337e-01,  2.9068e-01, -1.9146e+00, -7.7935e-01,\n",
      "           8.8068e-02,  4.2703e-01, -6.9054e-01, -3.2494e-01,  1.9030e-01,\n",
      "           1.1646e+00, -1.0978e-01, -1.7878e-01,  8.4153e-01,  6.1945e-01,\n",
      "           2.3311e-01, -8.4344e-01,  8.4707e-01,  4.3089e-01,  6.7947e-01,\n",
      "           3.9587e-01,  9.9990e-01,  5.8882e-01,  1.1263e+00, -6.2270e-01,\n",
      "          -1.1040e+00,  1.9466e-02, -1.2909e-01, -1.4949e+00, -5.3444e-01,\n",
      "          -6.6847e-02, -2.9037e-01,  2.0769e-01, -6.5357e-01, -7.3487e-01,\n",
      "           1.1984e+00,  5.6225e-01,  1.0821e+00,  7.9168e-01,  4.0344e-02,\n",
      "          -2.8010e-01, -4.0352e-01, -6.7602e-02, -4.4006e-03, -4.2479e-01,\n",
      "          -7.7769e-01,  1.5096e-01,  9.5490e-01, -1.7215e+00, -1.0988e-02,\n",
      "           1.1810e+00,  5.5657e-01,  1.3659e-01, -6.1250e-01, -2.3690e-02,\n",
      "           1.1620e-01,  3.8213e-02, -3.0859e-01, -7.2191e-01, -8.8349e-02,\n",
      "           2.4258e-02, -5.9543e-02,  3.1665e-01, -3.1744e-01,  2.5396e-01,\n",
      "          -2.4259e-01, -2.5177e-01, -1.0088e-01, -7.1268e-01, -8.8208e-02,\n",
      "          -5.0212e-01,  3.3639e-01,  8.9644e-02, -1.1143e+00, -3.4671e-01,\n",
      "          -1.3905e-01, -1.9250e-01, -1.1071e+00,  5.5250e-01, -6.5977e-02,\n",
      "           7.5599e-02,  8.0343e-01,  2.6980e-01],\n",
      "         [-2.5808e-01, -7.2842e-01,  3.6554e-01,  1.3767e-02,  7.7616e-02,\n",
      "           8.7220e-01,  1.1428e+00, -7.0924e-01, -1.2962e-01, -8.5023e-01,\n",
      "          -3.4328e-02,  4.5501e-01, -1.5883e-01,  2.6931e-01, -3.4455e-01,\n",
      "          -5.5222e-01,  6.1407e-02,  5.6190e-02, -9.0881e-01, -1.2038e+00,\n",
      "          -8.1580e-01,  2.4487e-02,  5.0764e-01, -5.8507e-01, -5.1345e-01,\n",
      "           5.2180e-01,  3.4609e-01, -3.8517e-01, -9.0338e-01,  4.3633e-01,\n",
      "          -5.1907e-01,  5.0622e-01,  1.0965e+00, -3.3345e-01,  1.0750e-01,\n",
      "          -6.1998e-01, -4.3019e-01, -8.8181e-01,  1.1618e-01, -8.5924e-02,\n",
      "          -6.5810e-01,  3.4317e-01,  2.3337e-01,  2.3496e-01,  7.1487e-01,\n",
      "          -2.5648e-01,  3.2310e-01, -1.9544e-01, -2.7978e-01,  7.2271e-01,\n",
      "           5.4846e-01, -8.5782e-01,  3.7951e-01, -4.5199e-01,  1.0183e+00,\n",
      "          -1.5290e-01, -9.1681e-01,  6.2006e-02, -1.1552e+00,  4.2910e-01,\n",
      "          -2.0474e-02,  2.5061e-01, -5.0067e-01, -5.1344e-03,  4.6033e-01,\n",
      "          -1.6017e-01,  1.2779e-01,  9.6340e-02, -3.7621e-01, -6.5732e-01,\n",
      "           8.1696e-01,  2.7964e-01, -1.1065e-01, -1.2658e-01,  3.6980e-01,\n",
      "           6.6806e-01, -5.8684e-01, -1.1746e-01, -7.2849e-01, -4.8985e-02,\n",
      "          -2.2685e-01,  4.5611e-01, -9.1451e-01,  1.3498e-01,  3.8379e-02,\n",
      "           7.4549e-01, -1.0083e+00, -3.2526e-01, -1.5378e-01, -1.5239e-01,\n",
      "          -3.8440e-01,  9.1470e-01,  5.3092e-01,  3.9331e-01, -2.0416e-01,\n",
      "          -8.8017e-01,  5.7553e-01, -3.8889e-01, -4.5334e-01,  7.0342e-01,\n",
      "          -7.2100e-01, -2.0292e-01, -4.1858e-01, -6.1906e-01,  1.5102e-01,\n",
      "           6.0377e-02,  6.6946e-01,  1.5539e-01,  5.0722e-01, -3.5330e-01,\n",
      "           4.2629e-01, -3.1955e-01, -3.0322e-01,  1.5940e-01, -6.9906e-01,\n",
      "          -8.0881e-02, -1.0451e-01,  3.0661e-01,  3.4300e-01,  2.6916e-01,\n",
      "          -1.9472e-02, -3.6542e-01, -6.9903e-02,  9.9055e-01, -8.7890e-01,\n",
      "           5.5949e-01, -3.5403e-01,  1.9863e-02]],\n",
      "\n",
      "        [[ 3.5794e-01,  1.8378e-01, -9.4751e-01,  1.1856e-01, -3.5459e-01,\n",
      "          -5.2132e-01,  6.7112e-01,  5.3155e-01, -2.8593e-01,  2.2431e-01,\n",
      "           2.3189e-01, -4.1405e-01, -5.0754e-01,  5.7365e-02, -7.9849e-01,\n",
      "           6.6696e-01, -7.7768e-01, -7.7612e-01, -1.5996e-01, -2.1987e-01,\n",
      "          -1.0533e-01, -2.0179e-01, -2.6558e-01, -9.6912e-02,  6.1609e-01,\n",
      "           3.7409e-01, -1.0479e+00, -7.0686e-01,  7.0772e-02, -1.0189e+00,\n",
      "          -4.5588e-01,  6.8254e-01, -4.1082e-01, -1.2018e+00, -9.4020e-01,\n",
      "           2.9671e-01,  2.7837e-02,  5.1047e-02,  5.3823e-01,  4.7361e-02,\n",
      "           6.0646e-01, -5.0494e-01,  2.7226e-01,  9.5061e-02,  3.0936e-01,\n",
      "          -2.6056e-01,  6.4272e-01,  1.6353e-01, -5.7883e-01, -3.3438e-02,\n",
      "          -6.9017e-02,  1.9515e-01, -6.9508e-01, -4.0187e-01, -7.1765e-01,\n",
      "          -5.9209e-01, -1.9818e-01, -7.3791e-02,  6.6877e-01, -2.1932e-01,\n",
      "           5.5034e-01,  1.3981e-01,  2.7066e-01,  6.5159e-02, -1.0619e+00,\n",
      "          -2.9870e-01,  7.0491e-01, -2.3557e-01,  1.5284e-01,  2.7121e-01,\n",
      "           1.1471e-02,  1.3004e-01, -1.3098e+00, -5.1197e-01,  7.4057e-01,\n",
      "          -4.5958e-04, -8.5058e-02,  3.2316e-01, -8.5503e-03,  8.5995e-02,\n",
      "           7.3563e-01,  2.1925e-01, -4.4551e-02,  1.4360e-01,  4.6514e-01,\n",
      "          -1.1158e+00,  4.1904e-01,  3.0446e-03, -8.2887e-01, -3.4821e-02,\n",
      "           6.2690e-01, -1.1499e+00,  1.8425e-01,  1.3850e-01,  5.7851e-01,\n",
      "           5.0225e-01, -1.0121e+00, -2.2565e-01,  3.1109e-01, -7.1649e-02,\n",
      "          -1.2702e-01, -4.2883e-01, -9.4026e-01,  8.7545e-01, -4.5423e-01,\n",
      "          -8.1673e-01, -6.7350e-02, -5.6049e-01, -8.0274e-01, -3.9612e-01,\n",
      "          -3.2778e-01,  4.8419e-01, -7.7392e-01, -5.7063e-01,  6.2007e-01,\n",
      "          -5.5680e-01,  4.3271e-01,  2.2510e-01, -1.5527e-01, -7.0217e-01,\n",
      "           9.4776e-01, -7.9466e-01, -3.2424e-01,  5.3894e-01,  2.5663e-01,\n",
      "           6.2193e-01,  8.0424e-01,  2.7868e-01],\n",
      "         [-3.8078e-02,  4.6935e-01,  5.2397e-01, -3.2413e-01,  8.0656e-01,\n",
      "           2.0632e-01,  1.2242e+00, -2.9205e-01, -5.9350e-01, -2.8197e-01,\n",
      "           5.0504e-01, -6.3040e-01,  3.7778e-02, -4.6655e-01, -1.1637e+00,\n",
      "           2.3127e-01,  1.2003e-01, -5.8265e-01,  1.3442e-01, -3.9422e-01,\n",
      "          -3.5045e-01,  9.8100e-01,  4.5734e-01,  3.1720e-01, -3.3557e-02,\n",
      "          -8.6680e-01,  6.6325e-01, -4.3706e-01, -9.7915e-01, -4.4355e-01,\n",
      "           4.3750e-01, -8.5132e-01,  4.0973e-01,  3.5828e-01, -1.4394e-02,\n",
      "          -2.4262e-01, -6.2650e-01,  2.1094e-01, -2.0453e-01,  2.4210e-01,\n",
      "           1.4601e-01,  2.2614e-01, -5.7198e-02, -7.5908e-01,  8.2263e-01,\n",
      "          -5.2022e-01,  6.7744e-01, -6.4053e-01,  2.0752e-01,  2.7037e-01,\n",
      "          -3.2082e-01, -3.6973e-01, -5.7013e-01, -3.5658e-03,  6.8079e-02,\n",
      "          -1.8446e-01, -6.2386e-02,  6.0610e-01, -6.2748e-01,  1.3793e+00,\n",
      "          -6.7278e-02,  4.7839e-02, -2.8100e-01, -4.7528e-01, -3.6376e-02,\n",
      "           4.7803e-01,  4.4955e-01, -5.0218e-01,  7.1859e-03,  6.2758e-01,\n",
      "           1.1883e+00, -5.4822e-01, -2.1414e-01,  3.1843e-01,  2.5470e-01,\n",
      "           3.9633e-01, -9.9196e-02, -5.2140e-01, -8.6445e-01,  3.0400e-02,\n",
      "          -8.6845e-01, -2.7169e-01, -4.2463e-01, -1.9186e-01, -4.0011e-01,\n",
      "           8.9893e-02, -7.1694e-01, -3.9939e-01, -8.0223e-02, -6.8938e-01,\n",
      "          -1.9305e-01, -5.1398e-01,  6.5536e-01,  9.9011e-01,  7.0916e-03,\n",
      "          -1.8883e-01,  7.4582e-01,  3.2231e-01, -2.3648e-01,  3.5393e-01,\n",
      "          -7.5660e-01, -1.3958e+00, -6.3078e-01,  5.6786e-01,  6.5125e-01,\n",
      "           2.8701e-01,  7.8151e-01,  2.1916e-01, -1.9918e-01, -7.9732e-01,\n",
      "           5.1707e-01, -1.0783e+00,  1.8366e-01,  2.6974e-01, -1.0200e-01,\n",
      "          -1.1309e-01, -5.1602e-01,  4.6296e-01,  1.4837e-01, -1.2640e-01,\n",
      "           1.3964e-01, -1.1799e-01,  3.9306e-01, -4.5728e-01, -1.0268e+00,\n",
      "          -2.2786e-01, -4.7192e-01,  5.2644e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer_2d = nn.Linear(in_features=68, out_features=128)\n",
    "flatten = nn.Flatten()\n",
    "input_2d = torch.randn(2, 2, 68) # n, channel, features\n",
    "#print(input_2d)\n",
    "print(input_2d.shape)\n",
    "output_2d = linear_layer_2d(input_2d)\n",
    "print('output_2d' , output_2d)\n",
    "#print(output_2d.size())\n",
    "\n",
    "#output_1d = flatten(input_2d)\n",
    "#print(output_1d.size())\n",
    "#linear_layer_2d(output_1d)\n",
    "#print(\"1D \",  output_1d)\n",
    "\n",
    "#test_sequence = nn.Sequential(nn.Linear(64, 32), nn.ReLU(), nn.Linear(32,10))\n",
    "#out = test_sequence(input_2d)\n",
    "#print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntestload = pd.read_csv(\"./outimg/Train/facefeature.csv\")\\nlandmarks = np.array(testload.iloc[0, 1:])\\nlandmarks = landmarks.astype(\\'float\\').reshape(-1, 2)\\nprint(landmarks)\\n\\ndataiter = iter(testload)\\nlandmark = next(dataiter)\\nprint(landmark)\\n# print(landmarks.shape)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "testload = pd.read_csv(\"./outimg/Train/facefeature.csv\")\n",
    "landmarks = np.array(testload.iloc[0, 1:])\n",
    "landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "print(landmarks)\n",
    "\n",
    "dataiter = iter(testload)\n",
    "landmark = next(dataiter)\n",
    "print(landmark)\n",
    "# print(landmarks.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, F, C]: torch.Size([10, 136])\n",
      "Shape of Tensor y: torch.Size([10, 33]) torch.float32\n",
      "X type torch.float32\n",
      "y type torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader \n",
    "\n",
    "training_data = FaceFeatureDataset(feature_file=\"./outimg/Train/facefeature.csv\", label_file=\"./Dataset/Train/csv/train.csv\")\n",
    "test_data = FaceFeatureDataset(feature_file=\"./outimg/Test/facefeature.csv\", label_file=\"./Dataset/Test/csv/test.csv\")\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 데이터 로드 확인 \n",
    "for X, y in train_loader:\n",
    "    print(f\"Shape of X [N, F, C]: {X.shape}\") # N , Channel, H= width W = height\n",
    "    print(f\"Shape of Tensor y: {y.shape} {y.dtype}\")       \n",
    "    break\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "# print(f'Traing dat length {n_total_steps}')\n",
    "# print(y)\n",
    "print('X type', X.dtype)\n",
    "print('y type', y.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#dataiter = iter(train_loader)\n",
    "#landmark, labels = next(dataiter)\n",
    "#print(\"landmark Shape \", landmark.shape)\n",
    "#print(landmark)\n",
    "#flatten = nn.Flatten()\n",
    "#linear1 = nn.Linear(68, 32)\n",
    "#x = flatten(landmark)\n",
    "#print(x)\n",
    "#print(x.size())\n",
    "# print(landmark.shape)\n",
    "# print('linear1', x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=136, out_features=33, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 33\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(68 * 2, num_classes),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(128, num_classes),\n",
    "            #nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # print(pred)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('batch',  batch)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.312433  [    0/  100]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.279359  [    0/  100]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.305074  [    0/  100]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.281749  [    0/  100]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.293791  [    0/  100]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.291111  [    0/  100]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.282101  [    0/  100]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.288735  [    0/  100]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.275069  [    0/  100]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.288075  [    0/  100]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.290006  [    0/  100]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.287282  [    0/  100]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.277202  [    0/  100]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.283883  [    0/  100]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.273298  [    0/  100]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.272709  [    0/  100]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.257846  [    0/  100]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.260524  [    0/  100]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.263370  [    0/  100]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.257184  [    0/  100]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.268278  [    0/  100]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.255053  [    0/  100]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.256462  [    0/  100]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.265922  [    0/  100]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.273917  [    0/  100]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.263677  [    0/  100]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.254824  [    0/  100]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.253816  [    0/  100]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.248763  [    0/  100]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.251949  [    0/  100]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.251483  [    0/  100]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.249468  [    0/  100]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.220826  [    0/  100]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.250883  [    0/  100]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.247343  [    0/  100]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.232378  [    0/  100]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.242698  [    0/  100]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.251223  [    0/  100]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.227118  [    0/  100]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.243065  [    0/  100]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.219263  [    0/  100]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.225428  [    0/  100]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.226439  [    0/  100]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.233888  [    0/  100]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.227717  [    0/  100]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.234032  [    0/  100]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.237214  [    0/  100]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.215522  [    0/  100]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.222179  [    0/  100]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.224653  [    0/  100]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.216768  [    0/  100]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.200670  [    0/  100]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.217233  [    0/  100]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.219338  [    0/  100]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.212418  [    0/  100]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.207215  [    0/  100]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.197289  [    0/  100]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.209041  [    0/  100]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.200057  [    0/  100]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.188611  [    0/  100]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.203616  [    0/  100]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.202407  [    0/  100]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.205156  [    0/  100]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.187881  [    0/  100]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.208038  [    0/  100]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.202609  [    0/  100]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.203669  [    0/  100]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.197979  [    0/  100]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.185153  [    0/  100]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.194772  [    0/  100]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.190902  [    0/  100]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.181171  [    0/  100]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.196728  [    0/  100]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.192781  [    0/  100]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.190134  [    0/  100]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.171110  [    0/  100]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.173497  [    0/  100]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.186352  [    0/  100]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.190157  [    0/  100]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.170967  [    0/  100]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.185718  [    0/  100]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.181574  [    0/  100]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.174395  [    0/  100]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.174740  [    0/  100]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.180035  [    0/  100]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.180021  [    0/  100]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.175846  [    0/  100]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.178593  [    0/  100]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.167280  [    0/  100]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.176033  [    0/  100]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.162939  [    0/  100]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.171161  [    0/  100]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.171978  [    0/  100]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.168728  [    0/  100]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.162176  [    0/  100]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.166876  [    0/  100]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.149237  [    0/  100]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.150400  [    0/  100]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.170044  [    0/  100]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.161249  [    0/  100]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.150339  [    0/  100]\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.159577  [    0/  100]\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.154182  [    0/  100]\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.153659  [    0/  100]\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.150318  [    0/  100]\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.162237  [    0/  100]\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.163139  [    0/  100]\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.149065  [    0/  100]\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.150898  [    0/  100]\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.154241  [    0/  100]\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.162971  [    0/  100]\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.158023  [    0/  100]\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.146326  [    0/  100]\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.138303  [    0/  100]\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.140895  [    0/  100]\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.151227  [    0/  100]\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.156943  [    0/  100]\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.149170  [    0/  100]\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.140483  [    0/  100]\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.152113  [    0/  100]\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.136797  [    0/  100]\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.135650  [    0/  100]\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.142041  [    0/  100]\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.139979  [    0/  100]\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.132951  [    0/  100]\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.143901  [    0/  100]\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.127166  [    0/  100]\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.136208  [    0/  100]\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.137965  [    0/  100]\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.136569  [    0/  100]\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.134247  [    0/  100]\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.124426  [    0/  100]\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.133181  [    0/  100]\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.126006  [    0/  100]\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.123094  [    0/  100]\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.138696  [    0/  100]\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.130980  [    0/  100]\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.126743  [    0/  100]\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.125457  [    0/  100]\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.128813  [    0/  100]\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.122606  [    0/  100]\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.123010  [    0/  100]\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.135241  [    0/  100]\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.122149  [    0/  100]\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.118146  [    0/  100]\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.125415  [    0/  100]\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.120031  [    0/  100]\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.118682  [    0/  100]\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.127171  [    0/  100]\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.115272  [    0/  100]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.126272  [    0/  100]\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.130298  [    0/  100]\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.115976  [    0/  100]\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.119787  [    0/  100]\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.115406  [    0/  100]\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.119751  [    0/  100]\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.106037  [    0/  100]\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.105844  [    0/  100]\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.120056  [    0/  100]\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.111148  [    0/  100]\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.112725  [    0/  100]\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.108328  [    0/  100]\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.117779  [    0/  100]\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.098728  [    0/  100]\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.108516  [    0/  100]\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.119125  [    0/  100]\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.097832  [    0/  100]\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.109895  [    0/  100]\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.100742  [    0/  100]\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.117338  [    0/  100]\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.109895  [    0/  100]\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.115430  [    0/  100]\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.101055  [    0/  100]\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.095936  [    0/  100]\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.101635  [    0/  100]\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.109875  [    0/  100]\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.107476  [    0/  100]\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.103254  [    0/  100]\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.095181  [    0/  100]\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.103284  [    0/  100]\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.089687  [    0/  100]\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.107472  [    0/  100]\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.106374  [    0/  100]\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.094582  [    0/  100]\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.101398  [    0/  100]\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.095344  [    0/  100]\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.098675  [    0/  100]\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.101078  [    0/  100]\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.102911  [    0/  100]\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.098307  [    0/  100]\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.089404  [    0/  100]\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.100676  [    0/  100]\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.098821  [    0/  100]\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.093847  [    0/  100]\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.087742  [    0/  100]\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.098991  [    0/  100]\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.089675  [    0/  100]\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.089561  [    0/  100]\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.093548  [    0/  100]\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.089673  [    0/  100]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.088338  [    0/  100]\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.090496  [    0/  100]\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.084056  [    0/  100]\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.090762  [    0/  100]\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.089479  [    0/  100]\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.090022  [    0/  100]\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.089200  [    0/  100]\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.089155  [    0/  100]\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.087676  [    0/  100]\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.101447  [    0/  100]\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.074819  [    0/  100]\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.085921  [    0/  100]\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.085336  [    0/  100]\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.080766  [    0/  100]\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.086455  [    0/  100]\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.088160  [    0/  100]\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.083121  [    0/  100]\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.092214  [    0/  100]\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.078585  [    0/  100]\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.087024  [    0/  100]\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.086313  [    0/  100]\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.085714  [    0/  100]\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.078741  [    0/  100]\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.072615  [    0/  100]\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.071418  [    0/  100]\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.078333  [    0/  100]\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.074547  [    0/  100]\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.084681  [    0/  100]\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.081186  [    0/  100]\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.086018  [    0/  100]\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.086084  [    0/  100]\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.079535  [    0/  100]\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.083237  [    0/  100]\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.077278  [    0/  100]\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.068975  [    0/  100]\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.077511  [    0/  100]\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.081440  [    0/  100]\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.079632  [    0/  100]\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.074308  [    0/  100]\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.072151  [    0/  100]\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.075700  [    0/  100]\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.072349  [    0/  100]\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.077110  [    0/  100]\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.071173  [    0/  100]\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.076605  [    0/  100]\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.073263  [    0/  100]\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.070758  [    0/  100]\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.074375  [    0/  100]\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.076384  [    0/  100]\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.072250  [    0/  100]\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.070400  [    0/  100]\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.067034  [    0/  100]\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.070474  [    0/  100]\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.067700  [    0/  100]\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.068211  [    0/  100]\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.071865  [    0/  100]\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.069470  [    0/  100]\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.073961  [    0/  100]\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.070067  [    0/  100]\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.073576  [    0/  100]\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.069113  [    0/  100]\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.066342  [    0/  100]\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.072776  [    0/  100]\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.070753  [    0/  100]\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.072844  [    0/  100]\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.067416  [    0/  100]\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.066996  [    0/  100]\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.067105  [    0/  100]\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.065528  [    0/  100]\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.069771  [    0/  100]\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.066860  [    0/  100]\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.068152  [    0/  100]\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.075374  [    0/  100]\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.065138  [    0/  100]\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.068247  [    0/  100]\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.071256  [    0/  100]\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.061934  [    0/  100]\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.064458  [    0/  100]\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.066368  [    0/  100]\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.060111  [    0/  100]\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.062499  [    0/  100]\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.060221  [    0/  100]\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.067793  [    0/  100]\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.063019  [    0/  100]\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.057748  [    0/  100]\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.060977  [    0/  100]\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.058278  [    0/  100]\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.059332  [    0/  100]\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.062887  [    0/  100]\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.067037  [    0/  100]\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.060242  [    0/  100]\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.063225  [    0/  100]\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.054086  [    0/  100]\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.063684  [    0/  100]\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.062413  [    0/  100]\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.054623  [    0/  100]\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.057840  [    0/  100]\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.058936  [    0/  100]\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.060711  [    0/  100]\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.063159  [    0/  100]\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.066977  [    0/  100]\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.059624  [    0/  100]\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.063942  [    0/  100]\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.059420  [    0/  100]\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.059948  [    0/  100]\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.053020  [    0/  100]\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.056807  [    0/  100]\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.060765  [    0/  100]\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.055677  [    0/  100]\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.058787  [    0/  100]\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.056356  [    0/  100]\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.058921  [    0/  100]\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.054524  [    0/  100]\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.056857  [    0/  100]\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.055430  [    0/  100]\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.056198  [    0/  100]\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.053399  [    0/  100]\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.055174  [    0/  100]\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.053082  [    0/  100]\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.051333  [    0/  100]\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.053805  [    0/  100]\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.050554  [    0/  100]\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.052558  [    0/  100]\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.054309  [    0/  100]\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.050319  [    0/  100]\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.051842  [    0/  100]\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.051746  [    0/  100]\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.051948  [    0/  100]\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.056511  [    0/  100]\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.055518  [    0/  100]\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.054209  [    0/  100]\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.050206  [    0/  100]\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.055733  [    0/  100]\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.048225  [    0/  100]\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.055547  [    0/  100]\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.052742  [    0/  100]\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.056966  [    0/  100]\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.048393  [    0/  100]\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.052319  [    0/  100]\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.047544  [    0/  100]\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.052037  [    0/  100]\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.051206  [    0/  100]\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.055066  [    0/  100]\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.055415  [    0/  100]\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.048993  [    0/  100]\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.050637  [    0/  100]\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.054034  [    0/  100]\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.050167  [    0/  100]\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.049978  [    0/  100]\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.047012  [    0/  100]\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.053922  [    0/  100]\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.050547  [    0/  100]\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.047821  [    0/  100]\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.050516  [    0/  100]\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.048012  [    0/  100]\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.049041  [    0/  100]\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.046659  [    0/  100]\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.048095  [    0/  100]\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.048889  [    0/  100]\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.050009  [    0/  100]\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.047130  [    0/  100]\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.048145  [    0/  100]\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.045185  [    0/  100]\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.042082  [    0/  100]\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.045078  [    0/  100]\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.046064  [    0/  100]\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.049479  [    0/  100]\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.046416  [    0/  100]\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.046023  [    0/  100]\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.046622  [    0/  100]\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.050357  [    0/  100]\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.043109  [    0/  100]\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.050727  [    0/  100]\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.048139  [    0/  100]\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.049871  [    0/  100]\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.049324  [    0/  100]\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.044636  [    0/  100]\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.046382  [    0/  100]\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.043984  [    0/  100]\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.047165  [    0/  100]\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.045909  [    0/  100]\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.048008  [    0/  100]\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.043389  [    0/  100]\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.050901  [    0/  100]\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.044540  [    0/  100]\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.048066  [    0/  100]\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.046250  [    0/  100]\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.044267  [    0/  100]\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.047849  [    0/  100]\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.045616  [    0/  100]\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.046484  [    0/  100]\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.047497  [    0/  100]\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.049968  [    0/  100]\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.046565  [    0/  100]\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.044833  [    0/  100]\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.043675  [    0/  100]\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.047523  [    0/  100]\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.040633  [    0/  100]\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.046828  [    0/  100]\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.046733  [    0/  100]\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.044632  [    0/  100]\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.039577  [    0/  100]\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.046685  [    0/  100]\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.046596  [    0/  100]\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.045159  [    0/  100]\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.041995  [    0/  100]\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.042674  [    0/  100]\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.044714  [    0/  100]\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.041727  [    0/  100]\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.040771  [    0/  100]\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.046302  [    0/  100]\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.045645  [    0/  100]\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.038985  [    0/  100]\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.043774  [    0/  100]\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.044327  [    0/  100]\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.045405  [    0/  100]\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.040684  [    0/  100]\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.047048  [    0/  100]\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.040912  [    0/  100]\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.046364  [    0/  100]\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.044352  [    0/  100]\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.043510  [    0/  100]\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.042220  [    0/  100]\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.042915  [    0/  100]\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.041150  [    0/  100]\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.042150  [    0/  100]\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.041745  [    0/  100]\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.043696  [    0/  100]\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.044944  [    0/  100]\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.040022  [    0/  100]\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.039757  [    0/  100]\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.040318  [    0/  100]\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.042595  [    0/  100]\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.039409  [    0/  100]\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.041076  [    0/  100]\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.039745  [    0/  100]\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.038721  [    0/  100]\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.042489  [    0/  100]\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.044503  [    0/  100]\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.036866  [    0/  100]\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.042376  [    0/  100]\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.039797  [    0/  100]\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.042872  [    0/  100]\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.041637  [    0/  100]\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.038020  [    0/  100]\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.039093  [    0/  100]\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.037749  [    0/  100]\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.038894  [    0/  100]\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.039884  [    0/  100]\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.036854  [    0/  100]\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.040047  [    0/  100]\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.038574  [    0/  100]\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.040826  [    0/  100]\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.039133  [    0/  100]\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.038304  [    0/  100]\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.038534  [    0/  100]\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.038539  [    0/  100]\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.037546  [    0/  100]\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.037861  [    0/  100]\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.041028  [    0/  100]\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.038696  [    0/  100]\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.036002  [    0/  100]\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.037784  [    0/  100]\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.039000  [    0/  100]\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.036817  [    0/  100]\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.039006  [    0/  100]\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.040274  [    0/  100]\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.036757  [    0/  100]\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.035843  [    0/  100]\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.036961  [    0/  100]\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.041233  [    0/  100]\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.035979  [    0/  100]\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.038485  [    0/  100]\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.040544  [    0/  100]\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.041511  [    0/  100]\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.042340  [    0/  100]\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.040796  [    0/  100]\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.041966  [    0/  100]\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.043600  [    0/  100]\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.037332  [    0/  100]\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.038327  [    0/  100]\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.039652  [    0/  100]\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.032764  [    0/  100]\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.038857  [    0/  100]\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.035404  [    0/  100]\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.038227  [    0/  100]\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.036086  [    0/  100]\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.036615  [    0/  100]\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.042939  [    0/  100]\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.036448  [    0/  100]\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.038817  [    0/  100]\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.037782  [    0/  100]\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.035239  [    0/  100]\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.041745  [    0/  100]\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.038544  [    0/  100]\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.034823  [    0/  100]\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.041998  [    0/  100]\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.036464  [    0/  100]\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.040762  [    0/  100]\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.038136  [    0/  100]\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "loss: 0.033496  [    0/  100]\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "loss: 0.037294  [    0/  100]\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "loss: 0.035316  [    0/  100]\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "loss: 0.036934  [    0/  100]\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "loss: 0.034152  [    0/  100]\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "loss: 0.035789  [    0/  100]\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "loss: 0.037676  [    0/  100]\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "loss: 0.035892  [    0/  100]\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "loss: 0.038398  [    0/  100]\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "loss: 0.035965  [    0/  100]\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "loss: 0.038156  [    0/  100]\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "loss: 0.035350  [    0/  100]\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "loss: 0.036905  [    0/  100]\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "loss: 0.035724  [    0/  100]\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "loss: 0.035471  [    0/  100]\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "loss: 0.035412  [    0/  100]\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "loss: 0.036288  [    0/  100]\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "loss: 0.035692  [    0/  100]\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "loss: 0.034111  [    0/  100]\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "loss: 0.036718  [    0/  100]\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "loss: 0.038071  [    0/  100]\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "loss: 0.037127  [    0/  100]\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "loss: 0.037586  [    0/  100]\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "loss: 0.035192  [    0/  100]\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "loss: 0.038741  [    0/  100]\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "loss: 0.036040  [    0/  100]\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "loss: 0.039667  [    0/  100]\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "loss: 0.036326  [    0/  100]\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "loss: 0.040849  [    0/  100]\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "loss: 0.037309  [    0/  100]\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "loss: 0.036033  [    0/  100]\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "loss: 0.033579  [    0/  100]\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "loss: 0.034760  [    0/  100]\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "loss: 0.034172  [    0/  100]\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "loss: 0.037303  [    0/  100]\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "loss: 0.034846  [    0/  100]\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "loss: 0.038852  [    0/  100]\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "loss: 0.037715  [    0/  100]\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "loss: 0.035258  [    0/  100]\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "loss: 0.033026  [    0/  100]\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "loss: 0.032180  [    0/  100]\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "loss: 0.034349  [    0/  100]\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "loss: 0.035086  [    0/  100]\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "loss: 0.032293  [    0/  100]\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "loss: 0.033269  [    0/  100]\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "loss: 0.035146  [    0/  100]\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "loss: 0.033574  [    0/  100]\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "loss: 0.035624  [    0/  100]\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "loss: 0.035422  [    0/  100]\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "loss: 0.032964  [    0/  100]\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "loss: 0.033839  [    0/  100]\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "loss: 0.035740  [    0/  100]\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "loss: 0.032455  [    0/  100]\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "loss: 0.037021  [    0/  100]\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "loss: 0.032730  [    0/  100]\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "loss: 0.034383  [    0/  100]\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "loss: 0.034521  [    0/  100]\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "loss: 0.036101  [    0/  100]\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "loss: 0.033628  [    0/  100]\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "loss: 0.034052  [    0/  100]\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "loss: 0.032445  [    0/  100]\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "loss: 0.036293  [    0/  100]\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "loss: 0.036017  [    0/  100]\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "loss: 0.032457  [    0/  100]\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "loss: 0.038044  [    0/  100]\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "loss: 0.038487  [    0/  100]\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "loss: 0.033554  [    0/  100]\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "loss: 0.032646  [    0/  100]\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "loss: 0.033684  [    0/  100]\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "loss: 0.033655  [    0/  100]\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "loss: 0.029839  [    0/  100]\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "loss: 0.034678  [    0/  100]\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "loss: 0.034053  [    0/  100]\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "loss: 0.033426  [    0/  100]\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "loss: 0.033045  [    0/  100]\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "loss: 0.037972  [    0/  100]\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "loss: 0.035822  [    0/  100]\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "loss: 0.035604  [    0/  100]\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "loss: 0.036501  [    0/  100]\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "loss: 0.029276  [    0/  100]\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "loss: 0.033146  [    0/  100]\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "loss: 0.029756  [    0/  100]\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "loss: 0.032917  [    0/  100]\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "loss: 0.033188  [    0/  100]\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "loss: 0.032952  [    0/  100]\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "loss: 0.031867  [    0/  100]\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "loss: 0.034430  [    0/  100]\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "loss: 0.030064  [    0/  100]\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "loss: 0.034609  [    0/  100]\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "loss: 0.034216  [    0/  100]\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "loss: 0.031898  [    0/  100]\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "loss: 0.033022  [    0/  100]\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "loss: 0.033122  [    0/  100]\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "loss: 0.036017  [    0/  100]\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "loss: 0.030168  [    0/  100]\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "loss: 0.032644  [    0/  100]\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "loss: 0.036689  [    0/  100]\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "loss: 0.035568  [    0/  100]\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "loss: 0.033255  [    0/  100]\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "loss: 0.035883  [    0/  100]\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "loss: 0.034641  [    0/  100]\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "loss: 0.033124  [    0/  100]\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "loss: 0.030522  [    0/  100]\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "loss: 0.031807  [    0/  100]\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "loss: 0.031740  [    0/  100]\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "loss: 0.035836  [    0/  100]\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "loss: 0.033465  [    0/  100]\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "loss: 0.034928  [    0/  100]\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "loss: 0.034380  [    0/  100]\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "loss: 0.034301  [    0/  100]\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "loss: 0.032129  [    0/  100]\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "loss: 0.031629  [    0/  100]\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "loss: 0.033757  [    0/  100]\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "loss: 0.033739  [    0/  100]\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "loss: 0.031769  [    0/  100]\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "loss: 0.037025  [    0/  100]\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "loss: 0.031640  [    0/  100]\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "loss: 0.033087  [    0/  100]\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "loss: 0.033741  [    0/  100]\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "loss: 0.032749  [    0/  100]\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "loss: 0.032769  [    0/  100]\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "loss: 0.031982  [    0/  100]\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "loss: 0.032215  [    0/  100]\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "loss: 0.030563  [    0/  100]\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "loss: 0.031588  [    0/  100]\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "loss: 0.031004  [    0/  100]\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "loss: 0.036164  [    0/  100]\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "loss: 0.033591  [    0/  100]\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "loss: 0.033201  [    0/  100]\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "loss: 0.031605  [    0/  100]\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "loss: 0.032513  [    0/  100]\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "loss: 0.031949  [    0/  100]\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "loss: 0.033274  [    0/  100]\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "loss: 0.032692  [    0/  100]\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "loss: 0.032003  [    0/  100]\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "loss: 0.036302  [    0/  100]\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "loss: 0.034443  [    0/  100]\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "loss: 0.034208  [    0/  100]\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "loss: 0.036187  [    0/  100]\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "loss: 0.032309  [    0/  100]\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "loss: 0.032576  [    0/  100]\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "loss: 0.033190  [    0/  100]\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "loss: 0.031730  [    0/  100]\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "loss: 0.032256  [    0/  100]\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "loss: 0.030949  [    0/  100]\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "loss: 0.031480  [    0/  100]\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "loss: 0.036945  [    0/  100]\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "loss: 0.037314  [    0/  100]\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "loss: 0.033724  [    0/  100]\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "loss: 0.030476  [    0/  100]\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "loss: 0.031851  [    0/  100]\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "loss: 0.033109  [    0/  100]\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "loss: 0.031176  [    0/  100]\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "loss: 0.033626  [    0/  100]\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "loss: 0.031389  [    0/  100]\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "loss: 0.032426  [    0/  100]\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "loss: 0.033767  [    0/  100]\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "loss: 0.029775  [    0/  100]\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "loss: 0.030768  [    0/  100]\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "loss: 0.031990  [    0/  100]\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "loss: 0.032492  [    0/  100]\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "loss: 0.033877  [    0/  100]\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "loss: 0.027776  [    0/  100]\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "loss: 0.031274  [    0/  100]\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "loss: 0.029991  [    0/  100]\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "loss: 0.032447  [    0/  100]\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "loss: 0.034502  [    0/  100]\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "loss: 0.033086  [    0/  100]\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "loss: 0.033123  [    0/  100]\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "loss: 0.030142  [    0/  100]\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "loss: 0.032164  [    0/  100]\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "loss: 0.033221  [    0/  100]\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "loss: 0.031408  [    0/  100]\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "loss: 0.032025  [    0/  100]\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "loss: 0.032241  [    0/  100]\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "loss: 0.030940  [    0/  100]\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "loss: 0.032246  [    0/  100]\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "loss: 0.032482  [    0/  100]\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "loss: 0.032043  [    0/  100]\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "loss: 0.031224  [    0/  100]\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "loss: 0.035090  [    0/  100]\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "loss: 0.031250  [    0/  100]\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "loss: 0.030810  [    0/  100]\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "loss: 0.032928  [    0/  100]\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "loss: 0.034000  [    0/  100]\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "loss: 0.031762  [    0/  100]\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "loss: 0.029706  [    0/  100]\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "loss: 0.030853  [    0/  100]\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "loss: 0.031261  [    0/  100]\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "loss: 0.031350  [    0/  100]\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "loss: 0.032673  [    0/  100]\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "loss: 0.030946  [    0/  100]\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "loss: 0.030706  [    0/  100]\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "loss: 0.031676  [    0/  100]\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "loss: 0.030871  [    0/  100]\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "loss: 0.033178  [    0/  100]\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "loss: 0.031406  [    0/  100]\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "loss: 0.027634  [    0/  100]\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "loss: 0.033539  [    0/  100]\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "loss: 0.031248  [    0/  100]\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "loss: 0.034572  [    0/  100]\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "loss: 0.032354  [    0/  100]\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "loss: 0.033272  [    0/  100]\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "loss: 0.030441  [    0/  100]\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "loss: 0.032829  [    0/  100]\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "loss: 0.027715  [    0/  100]\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "loss: 0.032138  [    0/  100]\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "loss: 0.034100  [    0/  100]\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "loss: 0.030647  [    0/  100]\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "loss: 0.028767  [    0/  100]\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "loss: 0.028761  [    0/  100]\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "loss: 0.031331  [    0/  100]\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "loss: 0.030094  [    0/  100]\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "loss: 0.032680  [    0/  100]\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "loss: 0.032659  [    0/  100]\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "loss: 0.031061  [    0/  100]\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "loss: 0.032853  [    0/  100]\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "loss: 0.030944  [    0/  100]\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "loss: 0.032481  [    0/  100]\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "loss: 0.033751  [    0/  100]\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "loss: 0.031535  [    0/  100]\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "loss: 0.033662  [    0/  100]\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "loss: 0.032401  [    0/  100]\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "loss: 0.030876  [    0/  100]\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "loss: 0.031680  [    0/  100]\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "loss: 0.030133  [    0/  100]\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "loss: 0.030472  [    0/  100]\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "loss: 0.031030  [    0/  100]\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "loss: 0.032040  [    0/  100]\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "loss: 0.031367  [    0/  100]\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "loss: 0.030697  [    0/  100]\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "loss: 0.030338  [    0/  100]\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "loss: 0.031408  [    0/  100]\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "loss: 0.030112  [    0/  100]\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "loss: 0.030552  [    0/  100]\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "loss: 0.032588  [    0/  100]\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "loss: 0.028280  [    0/  100]\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "loss: 0.032614  [    0/  100]\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "loss: 0.029991  [    0/  100]\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "loss: 0.031717  [    0/  100]\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "loss: 0.034130  [    0/  100]\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "loss: 0.030554  [    0/  100]\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "loss: 0.031878  [    0/  100]\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "loss: 0.030418  [    0/  100]\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "loss: 0.033221  [    0/  100]\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "loss: 0.033706  [    0/  100]\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "loss: 0.030255  [    0/  100]\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "loss: 0.031729  [    0/  100]\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "loss: 0.030548  [    0/  100]\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "loss: 0.030634  [    0/  100]\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "loss: 0.030353  [    0/  100]\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "loss: 0.032770  [    0/  100]\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "loss: 0.032169  [    0/  100]\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "loss: 0.030707  [    0/  100]\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "loss: 0.031421  [    0/  100]\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "loss: 0.032255  [    0/  100]\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "loss: 0.031866  [    0/  100]\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "loss: 0.029938  [    0/  100]\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "loss: 0.032583  [    0/  100]\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "loss: 0.030973  [    0/  100]\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "loss: 0.032915  [    0/  100]\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "loss: 0.033857  [    0/  100]\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "loss: 0.030947  [    0/  100]\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "loss: 0.029917  [    0/  100]\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "loss: 0.030211  [    0/  100]\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "loss: 0.030991  [    0/  100]\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "loss: 0.030718  [    0/  100]\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "loss: 0.031755  [    0/  100]\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "loss: 0.030645  [    0/  100]\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "loss: 0.032968  [    0/  100]\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "loss: 0.030413  [    0/  100]\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "loss: 0.030535  [    0/  100]\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "loss: 0.030931  [    0/  100]\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "loss: 0.033204  [    0/  100]\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "loss: 0.028704  [    0/  100]\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "loss: 0.032346  [    0/  100]\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "loss: 0.030534  [    0/  100]\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "loss: 0.028319  [    0/  100]\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "loss: 0.029790  [    0/  100]\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "loss: 0.032237  [    0/  100]\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "loss: 0.033680  [    0/  100]\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "loss: 0.030253  [    0/  100]\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "loss: 0.034137  [    0/  100]\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "loss: 0.030544  [    0/  100]\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "loss: 0.031813  [    0/  100]\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "loss: 0.032228  [    0/  100]\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "loss: 0.032854  [    0/  100]\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "loss: 0.032493  [    0/  100]\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "loss: 0.030668  [    0/  100]\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "loss: 0.030130  [    0/  100]\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "loss: 0.030997  [    0/  100]\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "loss: 0.029507  [    0/  100]\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "loss: 0.029304  [    0/  100]\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "loss: 0.030158  [    0/  100]\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "loss: 0.032411  [    0/  100]\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "loss: 0.030178  [    0/  100]\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "loss: 0.030211  [    0/  100]\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "loss: 0.029794  [    0/  100]\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "loss: 0.030810  [    0/  100]\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "loss: 0.032001  [    0/  100]\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "loss: 0.029838  [    0/  100]\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "loss: 0.028011  [    0/  100]\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "loss: 0.031214  [    0/  100]\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "loss: 0.029943  [    0/  100]\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "loss: 0.029651  [    0/  100]\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "loss: 0.030431  [    0/  100]\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "loss: 0.030342  [    0/  100]\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "loss: 0.032888  [    0/  100]\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "loss: 0.030234  [    0/  100]\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "loss: 0.030050  [    0/  100]\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "loss: 0.030428  [    0/  100]\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "loss: 0.031150  [    0/  100]\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "loss: 0.029720  [    0/  100]\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "loss: 0.033132  [    0/  100]\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "loss: 0.032563  [    0/  100]\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "loss: 0.030610  [    0/  100]\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "loss: 0.030031  [    0/  100]\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "loss: 0.029287  [    0/  100]\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "loss: 0.032559  [    0/  100]\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "loss: 0.029586  [    0/  100]\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "loss: 0.032893  [    0/  100]\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "loss: 0.031022  [    0/  100]\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "loss: 0.031565  [    0/  100]\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "loss: 0.035057  [    0/  100]\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "loss: 0.031433  [    0/  100]\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "loss: 0.030624  [    0/  100]\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "loss: 0.030708  [    0/  100]\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "loss: 0.029531  [    0/  100]\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "loss: 0.028628  [    0/  100]\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "loss: 0.032306  [    0/  100]\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "loss: 0.029851  [    0/  100]\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "loss: 0.029698  [    0/  100]\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "loss: 0.031355  [    0/  100]\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "loss: 0.031722  [    0/  100]\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "loss: 0.031160  [    0/  100]\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "loss: 0.029396  [    0/  100]\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "loss: 0.030202  [    0/  100]\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "loss: 0.033205  [    0/  100]\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "loss: 0.029389  [    0/  100]\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "loss: 0.029366  [    0/  100]\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "loss: 0.030551  [    0/  100]\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "loss: 0.032948  [    0/  100]\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "loss: 0.030628  [    0/  100]\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "loss: 0.027920  [    0/  100]\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "loss: 0.032070  [    0/  100]\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "loss: 0.029413  [    0/  100]\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "loss: 0.029845  [    0/  100]\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "loss: 0.032167  [    0/  100]\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "loss: 0.028590  [    0/  100]\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "loss: 0.032187  [    0/  100]\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "loss: 0.032180  [    0/  100]\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "loss: 0.029311  [    0/  100]\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "loss: 0.031127  [    0/  100]\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "loss: 0.031843  [    0/  100]\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "loss: 0.030961  [    0/  100]\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "loss: 0.031605  [    0/  100]\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "loss: 0.034483  [    0/  100]\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "loss: 0.030037  [    0/  100]\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "loss: 0.029781  [    0/  100]\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "loss: 0.029579  [    0/  100]\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "loss: 0.030476  [    0/  100]\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "loss: 0.030114  [    0/  100]\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "loss: 0.028705  [    0/  100]\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "loss: 0.030931  [    0/  100]\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "loss: 0.031062  [    0/  100]\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "loss: 0.028477  [    0/  100]\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "loss: 0.029776  [    0/  100]\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "loss: 0.031388  [    0/  100]\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "loss: 0.031500  [    0/  100]\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "loss: 0.029655  [    0/  100]\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "loss: 0.033198  [    0/  100]\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "loss: 0.028833  [    0/  100]\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "loss: 0.029889  [    0/  100]\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "loss: 0.028079  [    0/  100]\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "loss: 0.031741  [    0/  100]\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "loss: 0.031209  [    0/  100]\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "loss: 0.030696  [    0/  100]\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "loss: 0.029803  [    0/  100]\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "loss: 0.028353  [    0/  100]\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "loss: 0.030392  [    0/  100]\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "loss: 0.030432  [    0/  100]\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "loss: 0.031684  [    0/  100]\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "loss: 0.031470  [    0/  100]\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "loss: 0.031078  [    0/  100]\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "loss: 0.030740  [    0/  100]\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "loss: 0.031668  [    0/  100]\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "loss: 0.029201  [    0/  100]\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "loss: 0.029834  [    0/  100]\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "loss: 0.030027  [    0/  100]\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "loss: 0.027130  [    0/  100]\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "loss: 0.029851  [    0/  100]\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "loss: 0.030757  [    0/  100]\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "loss: 0.032108  [    0/  100]\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "loss: 0.031280  [    0/  100]\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "loss: 0.032568  [    0/  100]\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "loss: 0.031941  [    0/  100]\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "loss: 0.032139  [    0/  100]\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "loss: 0.031044  [    0/  100]\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "loss: 0.030636  [    0/  100]\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "loss: 0.028870  [    0/  100]\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "loss: 0.033960  [    0/  100]\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "loss: 0.030868  [    0/  100]\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "loss: 0.032207  [    0/  100]\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "loss: 0.029102  [    0/  100]\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "loss: 0.031223  [    0/  100]\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "loss: 0.032100  [    0/  100]\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "loss: 0.031523  [    0/  100]\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "loss: 0.033338  [    0/  100]\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "loss: 0.029019  [    0/  100]\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "loss: 0.032041  [    0/  100]\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "loss: 0.029514  [    0/  100]\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "loss: 0.030669  [    0/  100]\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "loss: 0.029554  [    0/  100]\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "loss: 0.031776  [    0/  100]\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "loss: 0.031400  [    0/  100]\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "loss: 0.029615  [    0/  100]\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "loss: 0.029388  [    0/  100]\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "loss: 0.030027  [    0/  100]\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "loss: 0.033602  [    0/  100]\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "loss: 0.031793  [    0/  100]\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "loss: 0.031391  [    0/  100]\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "loss: 0.032366  [    0/  100]\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "loss: 0.031634  [    0/  100]\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "loss: 0.028881  [    0/  100]\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "loss: 0.030215  [    0/  100]\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "loss: 0.031929  [    0/  100]\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "loss: 0.030201  [    0/  100]\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "loss: 0.032712  [    0/  100]\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "loss: 0.031281  [    0/  100]\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "loss: 0.030233  [    0/  100]\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "loss: 0.030358  [    0/  100]\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "loss: 0.031079  [    0/  100]\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "loss: 0.029848  [    0/  100]\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "loss: 0.031304  [    0/  100]\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "loss: 0.029112  [    0/  100]\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "loss: 0.032738  [    0/  100]\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "loss: 0.031139  [    0/  100]\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "loss: 0.032434  [    0/  100]\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "loss: 0.028994  [    0/  100]\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "loss: 0.034216  [    0/  100]\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "loss: 0.031002  [    0/  100]\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "loss: 0.031138  [    0/  100]\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "loss: 0.030263  [    0/  100]\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "loss: 0.033060  [    0/  100]\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "loss: 0.029671  [    0/  100]\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "loss: 0.032612  [    0/  100]\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "loss: 0.030102  [    0/  100]\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "loss: 0.031547  [    0/  100]\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "loss: 0.031272  [    0/  100]\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "loss: 0.028968  [    0/  100]\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "loss: 0.030810  [    0/  100]\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "loss: 0.030613  [    0/  100]\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "loss: 0.029659  [    0/  100]\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "loss: 0.030154  [    0/  100]\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "loss: 0.031566  [    0/  100]\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "loss: 0.031106  [    0/  100]\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "loss: 0.030744  [    0/  100]\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "loss: 0.029972  [    0/  100]\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "loss: 0.032438  [    0/  100]\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "loss: 0.032130  [    0/  100]\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "loss: 0.029014  [    0/  100]\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "loss: 0.032243  [    0/  100]\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "loss: 0.031929  [    0/  100]\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "loss: 0.032320  [    0/  100]\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "loss: 0.033443  [    0/  100]\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "loss: 0.031110  [    0/  100]\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "loss: 0.031529  [    0/  100]\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "loss: 0.032837  [    0/  100]\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "loss: 0.031448  [    0/  100]\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "loss: 0.032953  [    0/  100]\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "loss: 0.030976  [    0/  100]\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "loss: 0.028463  [    0/  100]\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "loss: 0.034374  [    0/  100]\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "loss: 0.030256  [    0/  100]\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "loss: 0.031729  [    0/  100]\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "loss: 0.028300  [    0/  100]\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "loss: 0.029110  [    0/  100]\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "loss: 0.030511  [    0/  100]\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "loss: 0.032713  [    0/  100]\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "loss: 0.030508  [    0/  100]\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "loss: 0.033573  [    0/  100]\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "loss: 0.031530  [    0/  100]\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "loss: 0.029363  [    0/  100]\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "loss: 0.031888  [    0/  100]\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "loss: 0.031226  [    0/  100]\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "loss: 0.031426  [    0/  100]\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "loss: 0.027664  [    0/  100]\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "loss: 0.034369  [    0/  100]\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "loss: 0.030086  [    0/  100]\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "loss: 0.030865  [    0/  100]\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "loss: 0.030407  [    0/  100]\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "loss: 0.029735  [    0/  100]\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "loss: 0.027936  [    0/  100]\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "loss: 0.030089  [    0/  100]\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "loss: 0.032246  [    0/  100]\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "loss: 0.031622  [    0/  100]\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "loss: 0.028039  [    0/  100]\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "loss: 0.028589  [    0/  100]\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "loss: 0.029181  [    0/  100]\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "loss: 0.030914  [    0/  100]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 1000\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, criterion, optimizer)    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    print('test size', size )\n",
    "    # num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)            \n",
    "            print('pred =', pred)\n",
    "            #print('loss', loss)\n",
    "            #print('real', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size 100\n",
      "pred = tensor([[0.4853, 0.4713, 0.4870, 0.5052, 0.5067, 0.5121, 0.4746, 0.4844, 0.5185,\n",
      "         0.4949, 0.5073, 0.4779, 0.4920, 0.4993, 0.4753, 0.4864, 0.5069, 0.5249,\n",
      "         0.4912, 0.5100, 0.4936, 0.4999, 0.4842, 0.4933, 0.5186, 0.4802, 0.5037,\n",
      "         0.4860, 0.4739, 0.4987, 0.5113, 0.5123, 0.4727],\n",
      "        [0.4793, 0.4726, 0.4820, 0.5064, 0.5086, 0.5071, 0.4718, 0.5013, 0.5213,\n",
      "         0.5034, 0.5010, 0.4987, 0.4888, 0.5025, 0.4796, 0.4700, 0.5116, 0.5312,\n",
      "         0.4936, 0.5148, 0.4849, 0.4957, 0.4912, 0.4909, 0.5065, 0.4845, 0.5220,\n",
      "         0.4845, 0.4815, 0.5008, 0.5141, 0.5102, 0.4780],\n",
      "        [0.4764, 0.4658, 0.4747, 0.5013, 0.5085, 0.5033, 0.4702, 0.4924, 0.5126,\n",
      "         0.4971, 0.5031, 0.4839, 0.4809, 0.4994, 0.4748, 0.4724, 0.5042, 0.5242,\n",
      "         0.4894, 0.5040, 0.4816, 0.5008, 0.4897, 0.4895, 0.5031, 0.4774, 0.5104,\n",
      "         0.4798, 0.4701, 0.4914, 0.5139, 0.5031, 0.4727],\n",
      "        [0.4648, 0.4613, 0.4686, 0.4855, 0.5013, 0.4953, 0.4689, 0.4874, 0.5037,\n",
      "         0.4992, 0.4969, 0.4810, 0.4690, 0.4905, 0.4673, 0.4578, 0.4885, 0.5159,\n",
      "         0.4825, 0.4970, 0.4767, 0.4915, 0.4781, 0.4790, 0.4941, 0.4730, 0.5080,\n",
      "         0.4745, 0.4652, 0.4912, 0.5038, 0.4975, 0.4703],\n",
      "        [0.4707, 0.4647, 0.4752, 0.5062, 0.5104, 0.5014, 0.4686, 0.4927, 0.5092,\n",
      "         0.5000, 0.5005, 0.4911, 0.4840, 0.4936, 0.4757, 0.4683, 0.4997, 0.5197,\n",
      "         0.4882, 0.5091, 0.4816, 0.4902, 0.4863, 0.4879, 0.5007, 0.4849, 0.5161,\n",
      "         0.4784, 0.4720, 0.4924, 0.5131, 0.4991, 0.4701],\n",
      "        [0.4758, 0.4668, 0.4774, 0.5076, 0.5091, 0.5058, 0.4714, 0.4927, 0.5104,\n",
      "         0.5027, 0.5007, 0.4925, 0.4845, 0.4959, 0.4783, 0.4713, 0.4997, 0.5195,\n",
      "         0.4879, 0.5128, 0.4857, 0.4937, 0.4867, 0.4894, 0.5024, 0.4853, 0.5214,\n",
      "         0.4831, 0.4728, 0.4893, 0.5167, 0.4991, 0.4700],\n",
      "        [0.4781, 0.4615, 0.4732, 0.4953, 0.5072, 0.4997, 0.4661, 0.4947, 0.5137,\n",
      "         0.4995, 0.5009, 0.4826, 0.4704, 0.5015, 0.4689, 0.4616, 0.5046, 0.5303,\n",
      "         0.4848, 0.4982, 0.4811, 0.5032, 0.4884, 0.4814, 0.5017, 0.4773, 0.5062,\n",
      "         0.4814, 0.4671, 0.5006, 0.5041, 0.5080, 0.4749],\n",
      "        [0.4556, 0.4492, 0.4500, 0.4747, 0.4962, 0.4850, 0.4573, 0.4762, 0.4887,\n",
      "         0.4910, 0.4866, 0.4744, 0.4507, 0.4793, 0.4597, 0.4459, 0.4705, 0.4999,\n",
      "         0.4701, 0.4787, 0.4579, 0.4808, 0.4644, 0.4732, 0.4732, 0.4583, 0.5029,\n",
      "         0.4569, 0.4522, 0.4704, 0.5014, 0.4806, 0.4577],\n",
      "        [0.4816, 0.4715, 0.4810, 0.5007, 0.5049, 0.5086, 0.4749, 0.4844, 0.5051,\n",
      "         0.4986, 0.5103, 0.4797, 0.4901, 0.4974, 0.4779, 0.4914, 0.5046, 0.5204,\n",
      "         0.4901, 0.5133, 0.4974, 0.4986, 0.4838, 0.4939, 0.5170, 0.4832, 0.5100,\n",
      "         0.4792, 0.4709, 0.4948, 0.5161, 0.5052, 0.4692],\n",
      "        [0.4510, 0.4470, 0.4523, 0.4794, 0.4893, 0.4811, 0.4534, 0.4669, 0.4864,\n",
      "         0.4928, 0.4836, 0.4644, 0.4508, 0.4705, 0.4535, 0.4482, 0.4647, 0.4899,\n",
      "         0.4676, 0.4818, 0.4645, 0.4715, 0.4560, 0.4653, 0.4748, 0.4606, 0.5008,\n",
      "         0.4552, 0.4513, 0.4697, 0.4930, 0.4783, 0.4493]], device='cuda:0')\n",
      "pred = tensor([[0.4768, 0.4657, 0.4796, 0.4934, 0.5023, 0.5033, 0.4699, 0.4799, 0.5051,\n",
      "         0.4955, 0.5046, 0.4736, 0.4765, 0.4896, 0.4682, 0.4779, 0.4961, 0.5190,\n",
      "         0.4853, 0.5034, 0.4906, 0.4972, 0.4753, 0.4845, 0.5093, 0.4753, 0.5025,\n",
      "         0.4822, 0.4684, 0.4975, 0.5055, 0.5038, 0.4700],\n",
      "        [0.4990, 0.4915, 0.5031, 0.5092, 0.5127, 0.5253, 0.4923, 0.4996, 0.5215,\n",
      "         0.5024, 0.5217, 0.4878, 0.5160, 0.5175, 0.4916, 0.5138, 0.5271, 0.5408,\n",
      "         0.5039, 0.5260, 0.5122, 0.5150, 0.5020, 0.5059, 0.5396, 0.4956, 0.5133,\n",
      "         0.4986, 0.4857, 0.5162, 0.5253, 0.5260, 0.4874],\n",
      "        [0.4833, 0.4750, 0.4916, 0.5157, 0.5072, 0.5122, 0.4762, 0.4890, 0.5180,\n",
      "         0.5030, 0.5118, 0.4878, 0.5005, 0.4940, 0.4832, 0.4933, 0.5112, 0.5212,\n",
      "         0.4952, 0.5268, 0.5043, 0.4947, 0.4870, 0.4973, 0.5229, 0.4910, 0.5219,\n",
      "         0.4892, 0.4789, 0.4990, 0.5207, 0.5070, 0.4708],\n",
      "        [0.4955, 0.4921, 0.5066, 0.5170, 0.5137, 0.5272, 0.4898, 0.4972, 0.5213,\n",
      "         0.5027, 0.5293, 0.4929, 0.5176, 0.5092, 0.4944, 0.5189, 0.5299, 0.5364,\n",
      "         0.5083, 0.5378, 0.5177, 0.5076, 0.5002, 0.5109, 0.5440, 0.4997, 0.5199,\n",
      "         0.4986, 0.4882, 0.5120, 0.5328, 0.5205, 0.4852],\n",
      "        [0.4635, 0.4534, 0.4588, 0.4771, 0.4934, 0.4871, 0.4607, 0.4744, 0.4868,\n",
      "         0.4885, 0.4885, 0.4639, 0.4634, 0.4843, 0.4605, 0.4651, 0.4792, 0.4994,\n",
      "         0.4734, 0.4850, 0.4672, 0.4789, 0.4665, 0.4771, 0.4874, 0.4598, 0.4975,\n",
      "         0.4555, 0.4569, 0.4787, 0.4994, 0.4899, 0.4550],\n",
      "        [0.4638, 0.4576, 0.4645, 0.4837, 0.5023, 0.4947, 0.4590, 0.4727, 0.4974,\n",
      "         0.4874, 0.4947, 0.4721, 0.4611, 0.4897, 0.4599, 0.4609, 0.4867, 0.5145,\n",
      "         0.4792, 0.4869, 0.4714, 0.4934, 0.4750, 0.4811, 0.4930, 0.4632, 0.4955,\n",
      "         0.4686, 0.4613, 0.4794, 0.4979, 0.4958, 0.4617],\n",
      "        [0.4589, 0.4536, 0.4520, 0.4812, 0.4908, 0.4864, 0.4592, 0.4736, 0.4856,\n",
      "         0.4949, 0.4911, 0.4736, 0.4583, 0.4759, 0.4638, 0.4601, 0.4746, 0.4943,\n",
      "         0.4748, 0.4918, 0.4706, 0.4710, 0.4620, 0.4726, 0.4878, 0.4653, 0.5112,\n",
      "         0.4520, 0.4551, 0.4714, 0.5062, 0.4820, 0.4558],\n",
      "        [0.4804, 0.4675, 0.4798, 0.5086, 0.5190, 0.5054, 0.4681, 0.5020, 0.5152,\n",
      "         0.5013, 0.5071, 0.5002, 0.4852, 0.5012, 0.4830, 0.4718, 0.5150, 0.5374,\n",
      "         0.4942, 0.5144, 0.4865, 0.4982, 0.4941, 0.4921, 0.5119, 0.4951, 0.5202,\n",
      "         0.4854, 0.4755, 0.5004, 0.5195, 0.5081, 0.4793],\n",
      "        [0.4713, 0.4682, 0.4795, 0.4926, 0.5019, 0.5015, 0.4708, 0.4789, 0.5013,\n",
      "         0.4941, 0.5017, 0.4731, 0.4825, 0.4908, 0.4717, 0.4845, 0.4931, 0.5137,\n",
      "         0.4840, 0.5021, 0.4848, 0.4936, 0.4765, 0.4887, 0.5040, 0.4779, 0.5016,\n",
      "         0.4775, 0.4688, 0.4888, 0.5080, 0.4986, 0.4651],\n",
      "        [0.4790, 0.4677, 0.4850, 0.5018, 0.5166, 0.5062, 0.4725, 0.4948, 0.5125,\n",
      "         0.4974, 0.5093, 0.4890, 0.4883, 0.5051, 0.4771, 0.4765, 0.5098, 0.5354,\n",
      "         0.4937, 0.5116, 0.4890, 0.5032, 0.4952, 0.4958, 0.5134, 0.4878, 0.5112,\n",
      "         0.4883, 0.4765, 0.5047, 0.5162, 0.5116, 0.4789]], device='cuda:0')\n",
      "pred = tensor([[0.4484, 0.4470, 0.4415, 0.4609, 0.4910, 0.4771, 0.4558, 0.4761, 0.4826,\n",
      "         0.4908, 0.4796, 0.4757, 0.4381, 0.4797, 0.4553, 0.4327, 0.4643, 0.5002,\n",
      "         0.4675, 0.4722, 0.4484, 0.4764, 0.4630, 0.4663, 0.4696, 0.4526, 0.5046,\n",
      "         0.4498, 0.4487, 0.4675, 0.4936, 0.4802, 0.4579],\n",
      "        [0.4772, 0.4740, 0.4778, 0.4908, 0.4993, 0.5062, 0.4770, 0.4801, 0.5062,\n",
      "         0.4966, 0.5032, 0.4788, 0.4825, 0.4973, 0.4731, 0.4834, 0.4945, 0.5169,\n",
      "         0.4867, 0.5028, 0.4871, 0.5001, 0.4819, 0.4891, 0.5111, 0.4733, 0.5060,\n",
      "         0.4779, 0.4692, 0.4875, 0.5092, 0.5050, 0.4672],\n",
      "        [0.4865, 0.4796, 0.4821, 0.4962, 0.5034, 0.5105, 0.4802, 0.4895, 0.5169,\n",
      "         0.4987, 0.5063, 0.4872, 0.4851, 0.5032, 0.4796, 0.4844, 0.5054, 0.5277,\n",
      "         0.4915, 0.5092, 0.4900, 0.5063, 0.4887, 0.4949, 0.5179, 0.4723, 0.5131,\n",
      "         0.4862, 0.4753, 0.4960, 0.5188, 0.5108, 0.4763],\n",
      "        [0.4912, 0.4864, 0.4901, 0.4968, 0.5084, 0.5203, 0.4877, 0.4892, 0.5186,\n",
      "         0.4972, 0.5161, 0.4861, 0.4931, 0.5109, 0.4810, 0.4943, 0.5121, 0.5361,\n",
      "         0.4980, 0.5114, 0.4981, 0.5170, 0.4947, 0.5000, 0.5287, 0.4775, 0.5044,\n",
      "         0.4923, 0.4793, 0.5007, 0.5184, 0.5187, 0.4806],\n",
      "        [0.4584, 0.4496, 0.4491, 0.4752, 0.4953, 0.4819, 0.4595, 0.4808, 0.4900,\n",
      "         0.4962, 0.4875, 0.4751, 0.4501, 0.4822, 0.4597, 0.4479, 0.4764, 0.5026,\n",
      "         0.4722, 0.4840, 0.4631, 0.4789, 0.4663, 0.4682, 0.4792, 0.4621, 0.5056,\n",
      "         0.4561, 0.4545, 0.4773, 0.4982, 0.4852, 0.4598],\n",
      "        [0.4786, 0.4749, 0.4765, 0.4871, 0.5018, 0.5033, 0.4777, 0.4909, 0.5100,\n",
      "         0.5005, 0.5035, 0.4870, 0.4763, 0.5010, 0.4762, 0.4763, 0.5007, 0.5261,\n",
      "         0.4883, 0.5004, 0.4844, 0.4994, 0.4829, 0.4857, 0.5086, 0.4741, 0.5075,\n",
      "         0.4783, 0.4711, 0.4977, 0.5097, 0.5095, 0.4779],\n",
      "        [0.4848, 0.4770, 0.4792, 0.4891, 0.5013, 0.5096, 0.4784, 0.4839, 0.5024,\n",
      "         0.4954, 0.5085, 0.4765, 0.4871, 0.5022, 0.4781, 0.4944, 0.5045, 0.5211,\n",
      "         0.4912, 0.5040, 0.4918, 0.4998, 0.4819, 0.4905, 0.5153, 0.4744, 0.5048,\n",
      "         0.4751, 0.4692, 0.4932, 0.5156, 0.5087, 0.4719],\n",
      "        [0.5005, 0.4794, 0.4985, 0.5213, 0.5168, 0.5224, 0.4779, 0.4980, 0.5304,\n",
      "         0.5058, 0.5200, 0.4970, 0.4998, 0.5086, 0.4873, 0.4923, 0.5258, 0.5417,\n",
      "         0.4983, 0.5248, 0.5092, 0.5127, 0.4994, 0.5016, 0.5310, 0.4921, 0.5196,\n",
      "         0.5015, 0.4823, 0.5112, 0.5234, 0.5189, 0.4822],\n",
      "        [0.4754, 0.4641, 0.4696, 0.4987, 0.5095, 0.4985, 0.4686, 0.5013, 0.5081,\n",
      "         0.5053, 0.4988, 0.4986, 0.4796, 0.4961, 0.4817, 0.4640, 0.5043, 0.5240,\n",
      "         0.4872, 0.5099, 0.4796, 0.4880, 0.4861, 0.4859, 0.5000, 0.4851, 0.5262,\n",
      "         0.4729, 0.4706, 0.4956, 0.5188, 0.5004, 0.4734],\n",
      "        [0.4801, 0.4577, 0.4744, 0.5013, 0.5051, 0.5018, 0.4627, 0.4778, 0.5036,\n",
      "         0.4930, 0.5027, 0.4787, 0.4734, 0.4877, 0.4696, 0.4731, 0.4958, 0.5151,\n",
      "         0.4806, 0.5026, 0.4882, 0.4880, 0.4753, 0.4872, 0.5044, 0.4707, 0.5074,\n",
      "         0.4734, 0.4655, 0.4888, 0.5094, 0.4976, 0.4643]], device='cuda:0')\n",
      "pred = tensor([[0.4844, 0.4665, 0.4857, 0.5138, 0.5193, 0.5095, 0.4719, 0.4973, 0.5227,\n",
      "         0.4972, 0.5089, 0.4917, 0.4941, 0.5028, 0.4830, 0.4758, 0.5142, 0.5378,\n",
      "         0.4941, 0.5216, 0.4943, 0.5051, 0.4965, 0.4993, 0.5202, 0.4939, 0.5188,\n",
      "         0.4955, 0.4774, 0.5054, 0.5197, 0.5142, 0.4797],\n",
      "        [0.4906, 0.4760, 0.4894, 0.5062, 0.5153, 0.5143, 0.4777, 0.4980, 0.5236,\n",
      "         0.5017, 0.5119, 0.4945, 0.4876, 0.5090, 0.4808, 0.4796, 0.5164, 0.5390,\n",
      "         0.4946, 0.5132, 0.4944, 0.5095, 0.4970, 0.4953, 0.5204, 0.4844, 0.5149,\n",
      "         0.4926, 0.4779, 0.5042, 0.5160, 0.5156, 0.4822],\n",
      "        [0.4860, 0.4783, 0.4885, 0.5082, 0.5108, 0.5157, 0.4837, 0.4975, 0.5256,\n",
      "         0.5041, 0.5121, 0.4935, 0.4969, 0.5050, 0.4824, 0.4812, 0.5117, 0.5358,\n",
      "         0.4981, 0.5153, 0.4948, 0.5096, 0.4964, 0.4971, 0.5194, 0.4870, 0.5172,\n",
      "         0.4941, 0.4796, 0.5033, 0.5212, 0.5145, 0.4835],\n",
      "        [0.4968, 0.5028, 0.5095, 0.5113, 0.5138, 0.5324, 0.5016, 0.4991, 0.5276,\n",
      "         0.5047, 0.5238, 0.4994, 0.5199, 0.5203, 0.4974, 0.5139, 0.5245, 0.5476,\n",
      "         0.5098, 0.5307, 0.5136, 0.5189, 0.5072, 0.5104, 0.5419, 0.4957, 0.5163,\n",
      "         0.5054, 0.4916, 0.5144, 0.5308, 0.5302, 0.4907],\n",
      "        [0.4707, 0.4583, 0.4764, 0.5057, 0.5107, 0.4991, 0.4629, 0.4914, 0.5108,\n",
      "         0.4952, 0.5001, 0.4799, 0.4812, 0.4976, 0.4690, 0.4677, 0.5051, 0.5244,\n",
      "         0.4869, 0.5025, 0.4790, 0.4984, 0.4877, 0.4864, 0.4979, 0.4823, 0.5028,\n",
      "         0.4807, 0.4726, 0.4954, 0.5073, 0.5033, 0.4682],\n",
      "        [0.4899, 0.4775, 0.4944, 0.5159, 0.5138, 0.5170, 0.4747, 0.4921, 0.5206,\n",
      "         0.4990, 0.5131, 0.4907, 0.5014, 0.5047, 0.4840, 0.4937, 0.5203, 0.5342,\n",
      "         0.4976, 0.5215, 0.5008, 0.5037, 0.4949, 0.4990, 0.5248, 0.4921, 0.5113,\n",
      "         0.4921, 0.4823, 0.5030, 0.5170, 0.5165, 0.4751],\n",
      "        [0.4759, 0.4663, 0.4808, 0.5072, 0.5164, 0.5040, 0.4693, 0.5051, 0.5251,\n",
      "         0.5006, 0.5050, 0.4933, 0.4802, 0.5054, 0.4767, 0.4627, 0.5151, 0.5414,\n",
      "         0.4947, 0.5078, 0.4796, 0.5084, 0.4982, 0.4887, 0.5025, 0.4862, 0.5096,\n",
      "         0.4928, 0.4803, 0.5041, 0.5123, 0.5126, 0.4840],\n",
      "        [0.4783, 0.4701, 0.4737, 0.4831, 0.4961, 0.5030, 0.4730, 0.4745, 0.4989,\n",
      "         0.4920, 0.5027, 0.4741, 0.4675, 0.4904, 0.4692, 0.4781, 0.4903, 0.5152,\n",
      "         0.4814, 0.4961, 0.4860, 0.4970, 0.4721, 0.4839, 0.5095, 0.4645, 0.5003,\n",
      "         0.4747, 0.4644, 0.4899, 0.5021, 0.5036, 0.4690],\n",
      "        [0.4991, 0.4895, 0.5142, 0.5269, 0.5194, 0.5264, 0.4883, 0.5074, 0.5321,\n",
      "         0.5063, 0.5256, 0.4955, 0.5253, 0.5155, 0.4944, 0.5139, 0.5381, 0.5445,\n",
      "         0.5093, 0.5402, 0.5206, 0.5141, 0.5065, 0.5088, 0.5407, 0.5075, 0.5181,\n",
      "         0.5074, 0.4929, 0.5224, 0.5267, 0.5310, 0.4854],\n",
      "        [0.4907, 0.4838, 0.5007, 0.5127, 0.5194, 0.5212, 0.4862, 0.5026, 0.5221,\n",
      "         0.5018, 0.5232, 0.4966, 0.5113, 0.5146, 0.4893, 0.5001, 0.5273, 0.5433,\n",
      "         0.5065, 0.5277, 0.5067, 0.5107, 0.5057, 0.5060, 0.5293, 0.4978, 0.5150,\n",
      "         0.4972, 0.4860, 0.5132, 0.5243, 0.5217, 0.4858]], device='cuda:0')\n",
      "pred = tensor([[0.4676, 0.4567, 0.4743, 0.5040, 0.5088, 0.4943, 0.4626, 0.4910, 0.5080,\n",
      "         0.4990, 0.4994, 0.4827, 0.4799, 0.4878, 0.4701, 0.4648, 0.4970, 0.5162,\n",
      "         0.4846, 0.5076, 0.4807, 0.4901, 0.4821, 0.4841, 0.4946, 0.4835, 0.5123,\n",
      "         0.4788, 0.4699, 0.4932, 0.5091, 0.4970, 0.4656],\n",
      "        [0.4872, 0.4691, 0.4913, 0.5156, 0.5151, 0.5142, 0.4718, 0.4898, 0.5199,\n",
      "         0.4961, 0.5096, 0.4870, 0.4980, 0.5013, 0.4808, 0.4867, 0.5133, 0.5329,\n",
      "         0.4921, 0.5152, 0.4971, 0.5031, 0.4913, 0.5007, 0.5178, 0.4872, 0.5106,\n",
      "         0.4926, 0.4789, 0.5022, 0.5184, 0.5112, 0.4715],\n",
      "        [0.4706, 0.4586, 0.4608, 0.4850, 0.4998, 0.4965, 0.4662, 0.4804, 0.4982,\n",
      "         0.4955, 0.4934, 0.4795, 0.4606, 0.4903, 0.4652, 0.4596, 0.4853, 0.5101,\n",
      "         0.4774, 0.4887, 0.4718, 0.4898, 0.4744, 0.4787, 0.4903, 0.4612, 0.5080,\n",
      "         0.4645, 0.4600, 0.4781, 0.5076, 0.4903, 0.4630],\n",
      "        [0.4690, 0.4657, 0.4730, 0.4912, 0.5013, 0.4988, 0.4708, 0.4864, 0.4995,\n",
      "         0.4961, 0.5001, 0.4814, 0.4851, 0.4928, 0.4742, 0.4781, 0.4958, 0.5137,\n",
      "         0.4870, 0.5056, 0.4832, 0.4877, 0.4811, 0.4902, 0.5014, 0.4773, 0.5132,\n",
      "         0.4700, 0.4704, 0.4904, 0.5141, 0.4976, 0.4662],\n",
      "        [0.4661, 0.4608, 0.4632, 0.4779, 0.4965, 0.4918, 0.4674, 0.4837, 0.5006,\n",
      "         0.4958, 0.4928, 0.4753, 0.4606, 0.4933, 0.4642, 0.4542, 0.4835, 0.5157,\n",
      "         0.4780, 0.4877, 0.4705, 0.4943, 0.4749, 0.4727, 0.4911, 0.4665, 0.4997,\n",
      "         0.4725, 0.4605, 0.4899, 0.4961, 0.5003, 0.4699],\n",
      "        [0.4580, 0.4464, 0.4551, 0.4805, 0.5006, 0.4828, 0.4537, 0.4818, 0.4953,\n",
      "         0.4941, 0.4880, 0.4767, 0.4470, 0.4810, 0.4589, 0.4404, 0.4796, 0.5109,\n",
      "         0.4723, 0.4876, 0.4653, 0.4811, 0.4689, 0.4697, 0.4809, 0.4667, 0.5039,\n",
      "         0.4656, 0.4555, 0.4861, 0.4931, 0.4898, 0.4631],\n",
      "        [0.5087, 0.4884, 0.5035, 0.5224, 0.5198, 0.5302, 0.4914, 0.5027, 0.5280,\n",
      "         0.5073, 0.5229, 0.5021, 0.5141, 0.5151, 0.4966, 0.5069, 0.5265, 0.5448,\n",
      "         0.5040, 0.5356, 0.5167, 0.5145, 0.5020, 0.5112, 0.5418, 0.4953, 0.5304,\n",
      "         0.5066, 0.4901, 0.5139, 0.5393, 0.5216, 0.4899],\n",
      "        [0.4530, 0.4427, 0.4400, 0.4684, 0.4877, 0.4740, 0.4482, 0.4701, 0.4815,\n",
      "         0.4917, 0.4788, 0.4658, 0.4339, 0.4721, 0.4511, 0.4376, 0.4641, 0.4916,\n",
      "         0.4631, 0.4726, 0.4548, 0.4710, 0.4564, 0.4586, 0.4691, 0.4507, 0.5013,\n",
      "         0.4468, 0.4457, 0.4661, 0.4913, 0.4757, 0.4508],\n",
      "        [0.4543, 0.4442, 0.4516, 0.4771, 0.4951, 0.4798, 0.4529, 0.4741, 0.4907,\n",
      "         0.4899, 0.4852, 0.4658, 0.4441, 0.4746, 0.4531, 0.4432, 0.4702, 0.4969,\n",
      "         0.4685, 0.4799, 0.4589, 0.4792, 0.4620, 0.4684, 0.4763, 0.4610, 0.4968,\n",
      "         0.4607, 0.4516, 0.4750, 0.4904, 0.4816, 0.4561],\n",
      "        [0.4523, 0.4433, 0.4457, 0.4740, 0.4966, 0.4773, 0.4469, 0.4704, 0.4861,\n",
      "         0.4875, 0.4836, 0.4699, 0.4405, 0.4731, 0.4545, 0.4399, 0.4709, 0.5007,\n",
      "         0.4689, 0.4795, 0.4563, 0.4777, 0.4630, 0.4690, 0.4772, 0.4595, 0.5022,\n",
      "         0.4566, 0.4502, 0.4707, 0.4928, 0.4796, 0.4548]], device='cuda:0')\n",
      "pred = tensor([[0.4978, 0.4839, 0.5024, 0.5245, 0.5159, 0.5235, 0.4842, 0.5031, 0.5283,\n",
      "         0.5062, 0.5206, 0.4981, 0.5167, 0.5113, 0.4935, 0.5057, 0.5304, 0.5402,\n",
      "         0.5039, 0.5375, 0.5113, 0.5074, 0.5047, 0.5064, 0.5340, 0.5019, 0.5242,\n",
      "         0.5004, 0.4883, 0.5136, 0.5293, 0.5229, 0.4829],\n",
      "        [0.4701, 0.4649, 0.4673, 0.4795, 0.5050, 0.4980, 0.4680, 0.4881, 0.5052,\n",
      "         0.4924, 0.5005, 0.4794, 0.4681, 0.5018, 0.4678, 0.4579, 0.4991, 0.5295,\n",
      "         0.4897, 0.4900, 0.4749, 0.5034, 0.4867, 0.4833, 0.5007, 0.4686, 0.5010,\n",
      "         0.4759, 0.4645, 0.4948, 0.5045, 0.5078, 0.4756],\n",
      "        [0.4676, 0.4606, 0.4666, 0.4912, 0.5008, 0.4941, 0.4685, 0.4845, 0.4967,\n",
      "         0.4973, 0.4987, 0.4796, 0.4749, 0.4878, 0.4699, 0.4716, 0.4895, 0.5089,\n",
      "         0.4805, 0.5013, 0.4781, 0.4856, 0.4748, 0.4834, 0.4945, 0.4756, 0.5095,\n",
      "         0.4687, 0.4646, 0.4846, 0.5106, 0.4908, 0.4648],\n",
      "        [0.4501, 0.4417, 0.4447, 0.4685, 0.4892, 0.4776, 0.4520, 0.4640, 0.4831,\n",
      "         0.4874, 0.4814, 0.4605, 0.4388, 0.4722, 0.4476, 0.4397, 0.4584, 0.4890,\n",
      "         0.4628, 0.4701, 0.4538, 0.4767, 0.4545, 0.4654, 0.4685, 0.4476, 0.4927,\n",
      "         0.4523, 0.4467, 0.4640, 0.4895, 0.4769, 0.4483],\n",
      "        [0.4575, 0.4528, 0.4585, 0.4879, 0.4913, 0.4836, 0.4576, 0.4786, 0.4915,\n",
      "         0.4978, 0.4870, 0.4738, 0.4659, 0.4772, 0.4641, 0.4585, 0.4771, 0.4936,\n",
      "         0.4736, 0.4949, 0.4705, 0.4724, 0.4662, 0.4731, 0.4838, 0.4723, 0.5135,\n",
      "         0.4599, 0.4593, 0.4773, 0.4999, 0.4835, 0.4528],\n",
      "        [0.4614, 0.4576, 0.4540, 0.4734, 0.4901, 0.4866, 0.4613, 0.4767, 0.4959,\n",
      "         0.4953, 0.4858, 0.4786, 0.4464, 0.4827, 0.4601, 0.4442, 0.4731, 0.5028,\n",
      "         0.4716, 0.4806, 0.4610, 0.4786, 0.4631, 0.4683, 0.4814, 0.4562, 0.5068,\n",
      "         0.4600, 0.4556, 0.4759, 0.4930, 0.4883, 0.4623],\n",
      "        [0.4825, 0.4677, 0.4850, 0.5118, 0.5144, 0.5069, 0.4689, 0.4972, 0.5208,\n",
      "         0.4986, 0.5080, 0.4906, 0.4854, 0.5029, 0.4788, 0.4751, 0.5149, 0.5364,\n",
      "         0.4939, 0.5122, 0.4892, 0.5033, 0.4952, 0.4923, 0.5122, 0.4874, 0.5115,\n",
      "         0.4908, 0.4801, 0.5027, 0.5158, 0.5107, 0.4784],\n",
      "        [0.4649, 0.4575, 0.4668, 0.4902, 0.5029, 0.4894, 0.4667, 0.4961, 0.5089,\n",
      "         0.5006, 0.4971, 0.4855, 0.4681, 0.4914, 0.4690, 0.4514, 0.4931, 0.5177,\n",
      "         0.4832, 0.4999, 0.4752, 0.4901, 0.4838, 0.4779, 0.4919, 0.4805, 0.5147,\n",
      "         0.4735, 0.4648, 0.4945, 0.5043, 0.4993, 0.4733],\n",
      "        [0.4996, 0.4927, 0.5100, 0.5245, 0.5160, 0.5297, 0.4956, 0.4991, 0.5254,\n",
      "         0.5040, 0.5273, 0.4958, 0.5277, 0.5139, 0.4986, 0.5236, 0.5310, 0.5402,\n",
      "         0.5099, 0.5411, 0.5182, 0.5119, 0.5028, 0.5143, 0.5416, 0.5001, 0.5189,\n",
      "         0.5038, 0.4950, 0.5150, 0.5367, 0.5228, 0.4865],\n",
      "        [0.4622, 0.4626, 0.4618, 0.4770, 0.4933, 0.4923, 0.4695, 0.4739, 0.4909,\n",
      "         0.4920, 0.4938, 0.4719, 0.4651, 0.4836, 0.4659, 0.4688, 0.4778, 0.5044,\n",
      "         0.4777, 0.4905, 0.4726, 0.4840, 0.4661, 0.4782, 0.4928, 0.4624, 0.5013,\n",
      "         0.4627, 0.4608, 0.4787, 0.5053, 0.4903, 0.4619]], device='cuda:0')\n",
      "pred = tensor([[0.4749, 0.4606, 0.4783, 0.4990, 0.5187, 0.5038, 0.4634, 0.4914, 0.5135,\n",
      "         0.4926, 0.5034, 0.4887, 0.4732, 0.5017, 0.4703, 0.4591, 0.5036, 0.5371,\n",
      "         0.4872, 0.5029, 0.4793, 0.5045, 0.4934, 0.4910, 0.5039, 0.4797, 0.5077,\n",
      "         0.4883, 0.4717, 0.4981, 0.5108, 0.5065, 0.4766],\n",
      "        [0.4820, 0.4655, 0.4733, 0.4938, 0.5092, 0.5047, 0.4694, 0.4918, 0.5137,\n",
      "         0.4956, 0.5046, 0.4898, 0.4676, 0.4998, 0.4746, 0.4635, 0.5026, 0.5285,\n",
      "         0.4872, 0.5014, 0.4819, 0.5033, 0.4859, 0.4890, 0.5077, 0.4713, 0.5101,\n",
      "         0.4843, 0.4683, 0.4940, 0.5097, 0.5077, 0.4745],\n",
      "        [0.4574, 0.4510, 0.4516, 0.4724, 0.4904, 0.4815, 0.4551, 0.4706, 0.4887,\n",
      "         0.4880, 0.4831, 0.4663, 0.4486, 0.4775, 0.4553, 0.4501, 0.4730, 0.4987,\n",
      "         0.4707, 0.4769, 0.4603, 0.4790, 0.4597, 0.4674, 0.4778, 0.4539, 0.4955,\n",
      "         0.4560, 0.4534, 0.4746, 0.4920, 0.4868, 0.4540],\n",
      "        [0.4926, 0.4864, 0.4966, 0.5098, 0.5128, 0.5207, 0.4866, 0.4995, 0.5250,\n",
      "         0.5046, 0.5153, 0.5012, 0.5030, 0.5110, 0.4906, 0.4922, 0.5188, 0.5380,\n",
      "         0.5010, 0.5250, 0.5020, 0.5077, 0.4995, 0.5042, 0.5274, 0.4905, 0.5236,\n",
      "         0.4947, 0.4856, 0.5059, 0.5250, 0.5180, 0.4827],\n",
      "        [0.4534, 0.4366, 0.4435, 0.4748, 0.5001, 0.4752, 0.4443, 0.4756, 0.4895,\n",
      "         0.4874, 0.4821, 0.4683, 0.4350, 0.4769, 0.4502, 0.4322, 0.4747, 0.5038,\n",
      "         0.4676, 0.4728, 0.4529, 0.4785, 0.4641, 0.4639, 0.4693, 0.4548, 0.4951,\n",
      "         0.4551, 0.4501, 0.4754, 0.4883, 0.4827, 0.4557],\n",
      "        [0.4457, 0.4353, 0.4459, 0.4847, 0.4943, 0.4746, 0.4450, 0.4700, 0.4807,\n",
      "         0.4894, 0.4802, 0.4628, 0.4485, 0.4676, 0.4512, 0.4422, 0.4675, 0.4889,\n",
      "         0.4654, 0.4813, 0.4586, 0.4673, 0.4589, 0.4652, 0.4650, 0.4647, 0.5006,\n",
      "         0.4516, 0.4498, 0.4680, 0.4942, 0.4723, 0.4468],\n",
      "        [0.4852, 0.4815, 0.4906, 0.5098, 0.5099, 0.5160, 0.4836, 0.4966, 0.5152,\n",
      "         0.5042, 0.5131, 0.4989, 0.5060, 0.5043, 0.4885, 0.4922, 0.5116, 0.5286,\n",
      "         0.4980, 0.5242, 0.4985, 0.4976, 0.4932, 0.5015, 0.5192, 0.4888, 0.5228,\n",
      "         0.4883, 0.4832, 0.5002, 0.5272, 0.5104, 0.4784],\n",
      "        [0.4508, 0.4379, 0.4437, 0.4741, 0.4866, 0.4757, 0.4467, 0.4644, 0.4807,\n",
      "         0.4884, 0.4789, 0.4583, 0.4400, 0.4668, 0.4461, 0.4406, 0.4621, 0.4847,\n",
      "         0.4631, 0.4728, 0.4561, 0.4688, 0.4515, 0.4609, 0.4641, 0.4479, 0.4950,\n",
      "         0.4484, 0.4484, 0.4669, 0.4891, 0.4736, 0.4457],\n",
      "        [0.4684, 0.4534, 0.4698, 0.5002, 0.5001, 0.4946, 0.4560, 0.4787, 0.5052,\n",
      "         0.4934, 0.4944, 0.4741, 0.4697, 0.4858, 0.4624, 0.4623, 0.4918, 0.5097,\n",
      "         0.4796, 0.4984, 0.4772, 0.4848, 0.4741, 0.4788, 0.4922, 0.4715, 0.5037,\n",
      "         0.4731, 0.4656, 0.4855, 0.5001, 0.4958, 0.4604],\n",
      "        [0.4694, 0.4665, 0.4690, 0.4862, 0.4939, 0.4962, 0.4717, 0.4820, 0.5003,\n",
      "         0.4968, 0.4978, 0.4807, 0.4676, 0.4898, 0.4682, 0.4667, 0.4878, 0.5109,\n",
      "         0.4815, 0.4992, 0.4810, 0.4912, 0.4764, 0.4791, 0.4991, 0.4706, 0.5115,\n",
      "         0.4717, 0.4640, 0.4871, 0.5045, 0.4952, 0.4671]], device='cuda:0')\n",
      "pred = tensor([[0.4721, 0.4612, 0.4762, 0.5065, 0.5083, 0.5005, 0.4677, 0.4902, 0.5083,\n",
      "         0.5024, 0.5032, 0.4875, 0.4845, 0.4895, 0.4760, 0.4709, 0.4981, 0.5143,\n",
      "         0.4875, 0.5108, 0.4879, 0.4875, 0.4805, 0.4883, 0.4999, 0.4842, 0.5195,\n",
      "         0.4784, 0.4718, 0.4918, 0.5181, 0.4963, 0.4672],\n",
      "        [0.4803, 0.4702, 0.4863, 0.5030, 0.5071, 0.5071, 0.4733, 0.4881, 0.5121,\n",
      "         0.4973, 0.5092, 0.4780, 0.4876, 0.4989, 0.4744, 0.4860, 0.5082, 0.5273,\n",
      "         0.4911, 0.5090, 0.4930, 0.5035, 0.4876, 0.4895, 0.5144, 0.4837, 0.5019,\n",
      "         0.4866, 0.4758, 0.5014, 0.5095, 0.5091, 0.4747],\n",
      "        [0.4584, 0.4549, 0.4574, 0.4805, 0.4946, 0.4879, 0.4618, 0.4779, 0.4904,\n",
      "         0.4931, 0.4882, 0.4735, 0.4620, 0.4816, 0.4623, 0.4584, 0.4791, 0.5035,\n",
      "         0.4761, 0.4880, 0.4644, 0.4798, 0.4666, 0.4753, 0.4837, 0.4645, 0.5045,\n",
      "         0.4603, 0.4600, 0.4777, 0.5016, 0.4864, 0.4607],\n",
      "        [0.4825, 0.4800, 0.4943, 0.5082, 0.5076, 0.5129, 0.4821, 0.4909, 0.5138,\n",
      "         0.5020, 0.5111, 0.4822, 0.5088, 0.5032, 0.4839, 0.5011, 0.5115, 0.5255,\n",
      "         0.4962, 0.5197, 0.5006, 0.4986, 0.4900, 0.4961, 0.5238, 0.4958, 0.5094,\n",
      "         0.4883, 0.4791, 0.5057, 0.5158, 0.5150, 0.4744],\n",
      "        [0.4769, 0.4763, 0.4788, 0.4941, 0.5034, 0.5090, 0.4815, 0.4900, 0.5092,\n",
      "         0.5017, 0.5079, 0.4916, 0.4851, 0.4963, 0.4791, 0.4777, 0.4975, 0.5188,\n",
      "         0.4899, 0.5101, 0.4882, 0.4975, 0.4846, 0.4931, 0.5077, 0.4777, 0.5171,\n",
      "         0.4809, 0.4711, 0.4904, 0.5179, 0.5025, 0.4748],\n",
      "        [0.4773, 0.4679, 0.4733, 0.4900, 0.4967, 0.5035, 0.4724, 0.4785, 0.5033,\n",
      "         0.4955, 0.5001, 0.4745, 0.4740, 0.4925, 0.4686, 0.4754, 0.4904, 0.5133,\n",
      "         0.4816, 0.4966, 0.4825, 0.4964, 0.4747, 0.4812, 0.5031, 0.4677, 0.5015,\n",
      "         0.4760, 0.4656, 0.4870, 0.5059, 0.5016, 0.4668],\n",
      "        [0.4472, 0.4384, 0.4453, 0.4869, 0.4973, 0.4764, 0.4434, 0.4758, 0.4891,\n",
      "         0.4914, 0.4808, 0.4790, 0.4435, 0.4701, 0.4551, 0.4335, 0.4706, 0.4924,\n",
      "         0.4682, 0.4839, 0.4541, 0.4677, 0.4625, 0.4705, 0.4645, 0.4628, 0.5135,\n",
      "         0.4549, 0.4531, 0.4673, 0.4979, 0.4712, 0.4489],\n",
      "        [0.4605, 0.4477, 0.4553, 0.4809, 0.4941, 0.4849, 0.4520, 0.4639, 0.4853,\n",
      "         0.4847, 0.4885, 0.4635, 0.4552, 0.4744, 0.4564, 0.4586, 0.4726, 0.4957,\n",
      "         0.4683, 0.4838, 0.4678, 0.4757, 0.4587, 0.4745, 0.4869, 0.4575, 0.4955,\n",
      "         0.4565, 0.4523, 0.4706, 0.4969, 0.4823, 0.4490],\n",
      "        [0.4905, 0.4774, 0.4934, 0.5114, 0.5193, 0.5117, 0.4759, 0.5065, 0.5322,\n",
      "         0.4987, 0.5097, 0.4942, 0.4982, 0.5135, 0.4829, 0.4821, 0.5277, 0.5536,\n",
      "         0.5016, 0.5210, 0.4932, 0.5173, 0.5072, 0.4994, 0.5280, 0.4926, 0.5145,\n",
      "         0.5023, 0.4857, 0.5172, 0.5187, 0.5278, 0.4905],\n",
      "        [0.4684, 0.4572, 0.4639, 0.4924, 0.4987, 0.4945, 0.4655, 0.4815, 0.4928,\n",
      "         0.4957, 0.4953, 0.4776, 0.4751, 0.4876, 0.4718, 0.4734, 0.4894, 0.5042,\n",
      "         0.4809, 0.5017, 0.4773, 0.4781, 0.4735, 0.4826, 0.4949, 0.4743, 0.5123,\n",
      "         0.4615, 0.4649, 0.4813, 0.5114, 0.4914, 0.4598]], device='cuda:0')\n",
      "pred = tensor([[0.4561, 0.4505, 0.4438, 0.4674, 0.4896, 0.4822, 0.4585, 0.4754, 0.4848,\n",
      "         0.4916, 0.4846, 0.4714, 0.4465, 0.4815, 0.4583, 0.4445, 0.4696, 0.4991,\n",
      "         0.4696, 0.4770, 0.4562, 0.4779, 0.4638, 0.4677, 0.4750, 0.4539, 0.5039,\n",
      "         0.4516, 0.4495, 0.4682, 0.4979, 0.4825, 0.4592],\n",
      "        [0.4872, 0.4784, 0.4861, 0.4999, 0.5117, 0.5133, 0.4811, 0.5005, 0.5233,\n",
      "         0.5018, 0.5095, 0.4959, 0.4864, 0.5115, 0.4823, 0.4763, 0.5144, 0.5413,\n",
      "         0.4967, 0.5099, 0.4901, 0.5104, 0.4955, 0.4932, 0.5179, 0.4822, 0.5116,\n",
      "         0.4923, 0.4795, 0.5072, 0.5147, 0.5192, 0.4860],\n",
      "        [0.4641, 0.4514, 0.4614, 0.4868, 0.4942, 0.4900, 0.4595, 0.4714, 0.4907,\n",
      "         0.4922, 0.4944, 0.4664, 0.4628, 0.4761, 0.4611, 0.4676, 0.4810, 0.4967,\n",
      "         0.4743, 0.4905, 0.4756, 0.4785, 0.4636, 0.4766, 0.4879, 0.4653, 0.5017,\n",
      "         0.4598, 0.4573, 0.4764, 0.5017, 0.4840, 0.4532],\n",
      "        [0.4923, 0.4793, 0.4947, 0.5169, 0.5159, 0.5189, 0.4756, 0.4968, 0.5245,\n",
      "         0.4994, 0.5135, 0.4893, 0.5024, 0.5134, 0.4846, 0.4937, 0.5264, 0.5459,\n",
      "         0.5026, 0.5172, 0.5002, 0.5132, 0.5008, 0.4987, 0.5277, 0.4911, 0.5072,\n",
      "         0.4982, 0.4841, 0.5098, 0.5199, 0.5242, 0.4814],\n",
      "        [0.4626, 0.4603, 0.4539, 0.4706, 0.4892, 0.4875, 0.4653, 0.4751, 0.4877,\n",
      "         0.4948, 0.4882, 0.4756, 0.4540, 0.4820, 0.4651, 0.4585, 0.4751, 0.5014,\n",
      "         0.4741, 0.4879, 0.4674, 0.4786, 0.4650, 0.4722, 0.4865, 0.4581, 0.5084,\n",
      "         0.4570, 0.4570, 0.4772, 0.5020, 0.4878, 0.4603],\n",
      "        [0.4683, 0.4604, 0.4750, 0.5041, 0.5064, 0.4999, 0.4688, 0.4866, 0.5035,\n",
      "         0.4978, 0.5022, 0.4806, 0.4844, 0.4902, 0.4700, 0.4741, 0.4951, 0.5121,\n",
      "         0.4857, 0.5079, 0.4847, 0.4902, 0.4781, 0.4860, 0.4966, 0.4806, 0.5085,\n",
      "         0.4784, 0.4703, 0.4907, 0.5115, 0.4963, 0.4662],\n",
      "        [0.4386, 0.4354, 0.4325, 0.4635, 0.4842, 0.4648, 0.4430, 0.4698, 0.4715,\n",
      "         0.4914, 0.4712, 0.4657, 0.4324, 0.4607, 0.4488, 0.4301, 0.4537, 0.4789,\n",
      "         0.4576, 0.4721, 0.4452, 0.4578, 0.4465, 0.4558, 0.4559, 0.4531, 0.5046,\n",
      "         0.4381, 0.4436, 0.4618, 0.4872, 0.4662, 0.4453],\n",
      "        [0.4506, 0.4368, 0.4369, 0.4673, 0.4944, 0.4732, 0.4471, 0.4768, 0.4846,\n",
      "         0.4926, 0.4793, 0.4750, 0.4309, 0.4750, 0.4519, 0.4235, 0.4654, 0.4949,\n",
      "         0.4647, 0.4690, 0.4498, 0.4707, 0.4599, 0.4626, 0.4606, 0.4518, 0.5063,\n",
      "         0.4448, 0.4440, 0.4662, 0.4921, 0.4758, 0.4519],\n",
      "        [0.4678, 0.4666, 0.4764, 0.4968, 0.4979, 0.4969, 0.4696, 0.4868, 0.5026,\n",
      "         0.5029, 0.4981, 0.4803, 0.4851, 0.4894, 0.4719, 0.4764, 0.4948, 0.5084,\n",
      "         0.4850, 0.5074, 0.4862, 0.4824, 0.4773, 0.4811, 0.4994, 0.4821, 0.5145,\n",
      "         0.4725, 0.4691, 0.4924, 0.5073, 0.4984, 0.4634],\n",
      "        [0.4609, 0.4496, 0.4639, 0.4916, 0.5025, 0.4870, 0.4542, 0.4780, 0.4934,\n",
      "         0.4910, 0.4943, 0.4693, 0.4649, 0.4832, 0.4606, 0.4638, 0.4900, 0.5086,\n",
      "         0.4786, 0.4931, 0.4748, 0.4829, 0.4718, 0.4783, 0.4897, 0.4745, 0.5004,\n",
      "         0.4635, 0.4611, 0.4861, 0.4990, 0.4912, 0.4586]], device='cuda:0')\n",
      "pred = tensor([[0.4699, 0.4583, 0.4639, 0.4849, 0.4945, 0.4932, 0.4667, 0.4771, 0.4897,\n",
      "         0.4941, 0.4959, 0.4696, 0.4704, 0.4874, 0.4665, 0.4753, 0.4835, 0.5019,\n",
      "         0.4762, 0.4928, 0.4775, 0.4826, 0.4683, 0.4785, 0.4914, 0.4651, 0.5021,\n",
      "         0.4599, 0.4603, 0.4786, 0.5080, 0.4894, 0.4581],\n",
      "        [0.4791, 0.4626, 0.4729, 0.4979, 0.5098, 0.5031, 0.4686, 0.4852, 0.5045,\n",
      "         0.4939, 0.5058, 0.4799, 0.4767, 0.4969, 0.4725, 0.4759, 0.5003, 0.5217,\n",
      "         0.4846, 0.5003, 0.4870, 0.4975, 0.4827, 0.4879, 0.5052, 0.4744, 0.5027,\n",
      "         0.4768, 0.4653, 0.4917, 0.5114, 0.5035, 0.4676],\n",
      "        [0.4553, 0.4483, 0.4571, 0.4841, 0.4982, 0.4812, 0.4534, 0.4799, 0.4937,\n",
      "         0.4923, 0.4867, 0.4696, 0.4600, 0.4800, 0.4593, 0.4522, 0.4795, 0.5023,\n",
      "         0.4731, 0.4868, 0.4640, 0.4794, 0.4686, 0.4726, 0.4817, 0.4704, 0.5013,\n",
      "         0.4617, 0.4564, 0.4820, 0.4962, 0.4883, 0.4563],\n",
      "        [0.4418, 0.4396, 0.4414, 0.4717, 0.4913, 0.4714, 0.4468, 0.4761, 0.4852,\n",
      "         0.4910, 0.4756, 0.4728, 0.4393, 0.4707, 0.4506, 0.4290, 0.4643, 0.4919,\n",
      "         0.4651, 0.4751, 0.4449, 0.4675, 0.4570, 0.4621, 0.4590, 0.4566, 0.5024,\n",
      "         0.4504, 0.4523, 0.4668, 0.4885, 0.4736, 0.4520],\n",
      "        [0.4579, 0.4501, 0.4550, 0.4855, 0.4922, 0.4871, 0.4586, 0.4719, 0.4844,\n",
      "         0.4922, 0.4889, 0.4693, 0.4646, 0.4755, 0.4622, 0.4632, 0.4742, 0.4925,\n",
      "         0.4721, 0.4925, 0.4678, 0.4727, 0.4624, 0.4753, 0.4822, 0.4635, 0.5075,\n",
      "         0.4548, 0.4576, 0.4706, 0.5062, 0.4803, 0.4534],\n",
      "        [0.5083, 0.5033, 0.5113, 0.5137, 0.5180, 0.5374, 0.5042, 0.5058, 0.5368,\n",
      "         0.5063, 0.5309, 0.5004, 0.5239, 0.5306, 0.4996, 0.5176, 0.5376, 0.5570,\n",
      "         0.5157, 0.5310, 0.5163, 0.5308, 0.5135, 0.5140, 0.5519, 0.4934, 0.5140,\n",
      "         0.5106, 0.4952, 0.5212, 0.5343, 0.5403, 0.4989],\n",
      "        [0.4859, 0.4720, 0.4849, 0.5094, 0.5135, 0.5141, 0.4771, 0.4942, 0.5168,\n",
      "         0.5012, 0.5089, 0.4949, 0.4955, 0.5043, 0.4834, 0.4834, 0.5091, 0.5299,\n",
      "         0.4922, 0.5124, 0.4921, 0.4997, 0.4934, 0.4991, 0.5114, 0.4840, 0.5198,\n",
      "         0.4856, 0.4757, 0.4966, 0.5260, 0.5048, 0.4743],\n",
      "        [0.4909, 0.4796, 0.4834, 0.4945, 0.5089, 0.5136, 0.4840, 0.4940, 0.5168,\n",
      "         0.5019, 0.5128, 0.4905, 0.4877, 0.5105, 0.4824, 0.4846, 0.5093, 0.5347,\n",
      "         0.4930, 0.5069, 0.4942, 0.5097, 0.4911, 0.4956, 0.5216, 0.4776, 0.5132,\n",
      "         0.4867, 0.4739, 0.5025, 0.5197, 0.5140, 0.4820],\n",
      "        [0.4609, 0.4531, 0.4598, 0.4881, 0.4979, 0.4906, 0.4592, 0.4785, 0.4913,\n",
      "         0.4950, 0.4930, 0.4774, 0.4660, 0.4834, 0.4630, 0.4596, 0.4831, 0.5028,\n",
      "         0.4771, 0.4910, 0.4712, 0.4789, 0.4712, 0.4775, 0.4838, 0.4695, 0.5083,\n",
      "         0.4596, 0.4582, 0.4767, 0.5030, 0.4844, 0.4570],\n",
      "        [0.4842, 0.4788, 0.4912, 0.5046, 0.5072, 0.5105, 0.4780, 0.4966, 0.5200,\n",
      "         0.5038, 0.5081, 0.4875, 0.4973, 0.5051, 0.4815, 0.4874, 0.5147, 0.5334,\n",
      "         0.4968, 0.5195, 0.4975, 0.5013, 0.4929, 0.4911, 0.5224, 0.4920, 0.5133,\n",
      "         0.4920, 0.4804, 0.5107, 0.5114, 0.5203, 0.4807]], device='cuda:0')\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test(test_loader, model, criterion)\n",
    "\n",
    "print(\"Done !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
