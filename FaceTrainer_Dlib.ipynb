{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Facial Recognition using PyTorch and OpenCV\n",
    "\n",
    "https://ritik12.medium.com/facial-recognition-using-pytorch-and-opencv-467c4e41d1f\n",
    "\n",
    "\n",
    "Machine Learning - Face Recognition CNN Pytorch.ipynb\n",
    "https://github.com/rubencg195/Pytorch-Tutorials/blob/master/Machine%20Learning%20-%20Face%20Recognition%20CNN%20Pytorch.ipynb\n",
    "\n",
    "\n",
    "\n",
    "Face Recognition Using Pytorch\n",
    "https://github.com/timesler/facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face Landmarks Detection With PyTorch\n",
    "\n",
    "https://towardsdatascience.com/face-landmarks-detection-with-pytorch-4b4852f5e9c4\n",
    "\n",
    "\n",
    "\n",
    "다중입력 deep neural network\n",
    "https://rosenfelder.ai/multi-input-neural-network-pytorch/\n",
    "\n",
    "\n",
    "\n",
    "Understanding dimensions in PyTorch\n",
    "https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.init import *\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from FaceFeatureDataset import FaceFeatureDataset\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 68])\n",
      "output_2d tensor([[[-3.1157e-01,  9.0461e-01, -2.4423e-01, -2.1884e-01, -9.1526e-01,\n",
      "          -1.6587e-01,  1.0821e+00,  1.5103e-01,  4.5544e-01,  1.4594e-01,\n",
      "          -7.0723e-01,  8.2956e-02, -6.6262e-01,  8.8002e-01,  3.4693e-01,\n",
      "          -1.9007e-03, -1.3592e+00, -3.4829e-01, -6.0125e-01,  1.7638e-01,\n",
      "          -6.6795e-01,  6.1491e-01, -7.3399e-01,  1.5171e-01, -6.4931e-01,\n",
      "          -8.2073e-01, -9.7114e-01, -8.8151e-01,  3.2684e-01,  3.9353e-01,\n",
      "          -3.6450e-01,  8.2247e-01, -1.6247e-01, -5.8801e-02,  4.9932e-02,\n",
      "           5.3720e-01,  1.0799e+00, -6.2265e-01,  4.2779e-01,  4.9235e-01,\n",
      "           4.4077e-01,  3.8501e-01, -7.9629e-01,  3.8377e-02, -4.9450e-01,\n",
      "           7.3950e-01,  1.6512e-02,  7.8983e-01,  3.7818e-01,  9.0618e-01,\n",
      "          -2.0608e-01, -4.9113e-01, -6.2648e-01, -7.4768e-01,  9.6963e-01,\n",
      "          -3.2116e-01, -4.0987e-01, -5.9507e-01, -6.9109e-01,  1.0086e+00,\n",
      "           1.2509e+00, -4.2139e-01,  3.8071e-01, -1.5466e+00, -1.3027e-01,\n",
      "          -6.7850e-01,  1.6883e-01, -1.3812e-01, -2.1588e-01,  1.0453e+00,\n",
      "           9.2717e-01,  3.4640e-01, -1.2507e+00,  2.3980e-01,  5.0247e-01,\n",
      "           3.2449e-01,  6.2154e-01,  4.3935e-01, -2.5414e-01,  1.2734e-01,\n",
      "          -1.8524e-01,  5.8356e-03,  1.8024e-01, -1.9614e+00, -7.8287e-02,\n",
      "           1.0246e+00, -1.8867e-01,  1.9161e-01, -4.2082e-01,  5.5233e-01,\n",
      "          -2.4916e-01,  6.1118e-01,  1.5946e-01,  8.2649e-01,  5.8858e-01,\n",
      "          -2.1024e-01,  1.3723e-01,  9.1949e-02, -6.2240e-01,  4.3848e-01,\n",
      "           6.4427e-01,  4.2647e-01,  5.0795e-01,  5.6213e-01,  1.0966e+00,\n",
      "          -2.1803e-01, -2.8760e-02,  5.2425e-01, -3.2269e-01,  4.7976e-01,\n",
      "          -1.6185e-01, -1.4544e-01,  8.6485e-01, -1.8187e-01,  7.9880e-01,\n",
      "           5.4358e-01, -6.7282e-01, -1.1220e+00, -8.6197e-01, -1.0438e+00,\n",
      "           4.9282e-01, -1.5583e+00, -5.7050e-01,  4.9461e-01, -5.0177e-01,\n",
      "           7.8790e-01, -1.8297e-01,  6.4758e-01],\n",
      "         [ 4.4199e-01, -7.6685e-01,  3.4717e-01, -2.5784e-01, -2.5900e-02,\n",
      "           4.1061e-01, -4.1256e-01, -9.3052e-01,  3.1070e-01, -8.6604e-01,\n",
      "          -4.1475e-01, -1.1729e-01,  7.6160e-01,  2.4904e-01,  6.8319e-01,\n",
      "           9.3921e-01,  1.2143e+00,  2.6861e-01,  6.2393e-01,  4.5660e-01,\n",
      "           5.2069e-01,  3.4904e-01, -2.2146e-01, -2.4000e-01, -2.4874e-01,\n",
      "           4.6589e-02, -4.7975e-01,  1.2322e+00,  5.2866e-02, -6.2286e-01,\n",
      "           2.8307e-01, -3.7596e-01,  1.2397e+00, -5.8912e-01,  5.5118e-01,\n",
      "          -7.5062e-01,  4.9250e-02, -4.9625e-01,  1.6342e-01,  2.0436e-01,\n",
      "           2.7047e-01,  9.2587e-02,  3.8623e-01, -3.4932e-01, -8.3764e-01,\n",
      "           7.9714e-01, -2.3271e-01,  3.0059e-01,  1.2422e-01, -5.0316e-02,\n",
      "           1.4839e+00, -5.3397e-01,  2.6146e-01,  3.4297e-01, -7.1100e-01,\n",
      "           4.3203e-01, -2.9550e-01,  4.8484e-01,  2.0955e-01, -9.9738e-01,\n",
      "           7.0560e-02,  4.2261e-01,  4.8347e-01,  7.5839e-01,  5.1214e-01,\n",
      "          -3.3140e-01,  4.5233e-01,  6.3304e-01,  9.3435e-01, -1.3150e+00,\n",
      "           2.4937e-01, -5.9344e-01,  4.0787e-01,  3.3119e-02,  7.3857e-01,\n",
      "          -1.6433e+00, -1.5370e-01, -8.3549e-01, -1.1048e+00,  9.8647e-02,\n",
      "           6.9284e-01, -5.8539e-01,  3.6065e-01,  3.0032e-01,  8.3404e-01,\n",
      "           5.0927e-02,  1.2769e+00, -6.9463e-01,  9.1466e-01,  7.7840e-01,\n",
      "           1.0340e-01,  1.2738e+00, -2.6932e-02, -3.0539e-01, -4.3361e-01,\n",
      "           1.1908e+00, -1.4740e-01,  3.5655e-01, -2.0606e-01, -3.5940e-01,\n",
      "          -3.7626e-01,  3.7509e-01, -2.6617e-01, -9.7523e-01, -8.2352e-01,\n",
      "          -9.7800e-02, -3.9318e-01,  3.5127e-01,  7.8230e-01, -8.4598e-01,\n",
      "          -2.0478e-03,  1.5678e-01,  1.1157e-01, -1.2426e+00,  1.0220e-01,\n",
      "           2.8572e-01, -1.1056e+00, -5.7907e-02,  1.4645e-02, -5.7344e-02,\n",
      "           1.3000e-01,  8.1729e-01,  3.2763e-01, -2.2413e-01,  8.3694e-01,\n",
      "           2.8414e-01, -2.0950e-01, -1.0037e+00]],\n",
      "\n",
      "        [[ 1.3705e-01,  2.0554e-02,  3.2207e-03,  1.7050e+00,  3.9401e-01,\n",
      "          -7.9987e-02,  1.2254e+00, -4.9969e-01,  1.1966e-01,  1.5940e-01,\n",
      "           3.5694e-03,  8.6426e-01,  9.1509e-02, -5.9268e-01,  7.0640e-01,\n",
      "           7.7541e-01, -3.1195e-01, -1.2403e+00,  3.9965e-01, -5.7222e-01,\n",
      "           7.2889e-01, -2.1393e-02, -5.9048e-01, -7.7605e-01,  3.1138e-01,\n",
      "          -1.4487e-01,  7.3171e-01, -5.0497e-01,  3.7751e-01,  2.8426e-01,\n",
      "          -4.3478e-01, -8.8278e-01,  4.4709e-01,  1.1538e+00, -3.3396e-04,\n",
      "          -3.6677e-02,  1.0477e+00, -3.3882e-01,  2.9917e-01, -5.2331e-01,\n",
      "          -9.3787e-02,  7.8680e-01, -9.5906e-01, -4.6334e-02,  9.2221e-01,\n",
      "           7.3399e-01,  7.2668e-01, -4.5660e-01,  4.0709e-01, -2.6447e-01,\n",
      "          -5.0287e-01,  3.8766e-01, -1.2488e+00, -3.3755e-01,  8.9597e-02,\n",
      "          -1.9534e-01,  2.6128e-01, -1.1758e+00, -1.0795e+00,  4.5429e-01,\n",
      "          -8.9898e-01,  1.1785e+00, -5.0142e-01, -9.5998e-03, -5.6583e-01,\n",
      "           8.9059e-02,  4.5167e-01, -2.4648e-01, -3.9283e-01,  4.9728e-01,\n",
      "          -1.1894e-01, -3.1316e-01,  3.6590e-01,  2.0152e-01,  3.1571e-01,\n",
      "          -6.3124e-01,  3.8668e-01,  7.7638e-01,  2.8866e-02, -5.2229e-01,\n",
      "           8.6897e-01, -4.2792e-01, -4.3327e-01,  7.6614e-01, -8.1464e-01,\n",
      "           5.4114e-01, -1.0229e+00, -1.1734e+00, -1.0215e+00, -2.4932e-01,\n",
      "          -2.0973e-01,  3.2256e-01,  2.4375e-01,  5.4686e-01,  6.5147e-01,\n",
      "           1.9799e-01,  6.1912e-01,  4.7551e-01, -1.2073e-01,  1.1471e+00,\n",
      "          -3.4841e-02, -5.8701e-01,  4.3013e-01, -3.3201e-01, -7.1752e-02,\n",
      "          -5.0707e-01,  7.2455e-02, -2.8471e-01,  4.4827e-01, -4.0544e-01,\n",
      "           6.4355e-02,  5.6078e-02, -8.8015e-01, -5.0981e-01,  6.2342e-01,\n",
      "          -4.5777e-01, -6.7309e-01, -7.0480e-01, -1.2187e+00,  5.3296e-01,\n",
      "           3.6444e-01, -2.9576e-01,  5.5951e-01,  2.4879e-01,  1.8094e-01,\n",
      "           4.3516e-02, -7.8752e-01,  2.8102e-01],\n",
      "         [-3.3815e-01, -5.1512e-02,  1.4773e+00,  2.7502e-01,  1.0749e-02,\n",
      "           8.5986e-01, -2.7861e-01,  1.9122e-01,  6.5107e-01, -1.1617e-01,\n",
      "          -5.8665e-01,  5.4325e-01, -1.8043e-01, -1.7183e-01,  4.5400e-01,\n",
      "           6.4267e-01,  1.9666e-01,  2.0116e-01,  3.5967e-01, -3.8985e-02,\n",
      "           1.2092e+00,  5.8986e-01,  2.6343e-01,  1.1911e-03, -7.0763e-01,\n",
      "           6.7696e-02, -6.3768e-02, -1.2083e+00,  5.0522e-01,  1.0031e-01,\n",
      "          -3.8442e-01, -8.6325e-01,  2.6563e-01,  7.9304e-01,  5.1826e-01,\n",
      "           2.3342e-01,  6.8810e-01, -6.2959e-01,  8.2508e-02,  1.7250e-01,\n",
      "          -8.2267e-01,  2.3891e-01,  6.7652e-02, -2.4155e-01,  6.7116e-01,\n",
      "          -5.7070e-01,  1.4121e-01, -4.7608e-01, -1.1077e-01,  3.3054e-01,\n",
      "           8.5826e-01,  7.6596e-01,  4.9991e-02,  6.3764e-02, -6.1332e-01,\n",
      "           9.2830e-01,  1.0580e-01,  2.7893e-01, -7.8708e-02, -4.0775e-01,\n",
      "          -1.2804e-01,  1.0445e+00, -1.3211e+00, -1.7501e+00,  1.3075e-01,\n",
      "          -2.4789e-01,  8.0297e-01,  1.1024e-01,  6.6330e-01, -2.5472e-01,\n",
      "           7.4788e-01,  1.1885e+00,  8.1684e-01, -9.7013e-01, -9.7272e-01,\n",
      "          -2.6608e-01, -1.3387e-02,  6.3671e-01,  3.4504e-01,  3.8485e-01,\n",
      "           4.2852e-01, -8.4924e-01, -5.3270e-02,  4.0419e-01,  4.0769e-01,\n",
      "           2.9384e-01,  2.6601e-02,  8.5564e-02, -9.8718e-01, -8.6605e-01,\n",
      "           6.9421e-01, -3.5339e-01,  4.1209e-01, -7.9750e-01,  3.5212e-01,\n",
      "          -4.2508e-01, -1.4106e-01,  1.0513e+00,  3.0177e-01,  4.3528e-01,\n",
      "          -3.0272e-01, -8.7294e-01,  2.1827e-01, -4.1765e-02,  2.7151e-01,\n",
      "          -2.4645e-02, -3.4851e-01,  9.8554e-02,  5.0505e-02, -5.5470e-01,\n",
      "           4.0191e-01,  2.7764e-01, -3.9615e-01,  1.6997e-01,  7.4603e-01,\n",
      "          -7.7796e-01,  8.2525e-01, -8.4803e-01, -1.1935e+00,  2.3858e-02,\n",
      "          -6.6475e-01, -1.6674e-01,  3.9474e-01, -2.4038e-01,  2.7442e-01,\n",
      "          -6.6962e-01, -5.2187e-01, -8.0884e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer_2d = nn.Linear(in_features=68, out_features=128)\n",
    "flatten = nn.Flatten()\n",
    "input_2d = torch.randn(2, 2, 68) # n, channel, features\n",
    "#print(input_2d)\n",
    "print(input_2d.shape)\n",
    "output_2d = linear_layer_2d(input_2d)\n",
    "print('output_2d' , output_2d)\n",
    "#print(output_2d.size())\n",
    "\n",
    "#output_1d = flatten(input_2d)\n",
    "#print(output_1d.size())\n",
    "#linear_layer_2d(output_1d)\n",
    "#print(\"1D \",  output_1d)\n",
    "\n",
    "#test_sequence = nn.Sequential(nn.Linear(64, 32), nn.ReLU(), nn.Linear(32,10))\n",
    "#out = test_sequence(input_2d)\n",
    "#print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.47850995e-01 -2.35903748e-01]\n",
      " [-3.41526145e-01 -1.45656640e-01]\n",
      " [-3.25950023e-01 -5.53465988e-02]\n",
      " [-3.08061083e-01  3.49791763e-02]\n",
      " [-2.83217955e-01  1.23039334e-01]\n",
      " [-2.37527996e-01  2.06615456e-01]\n",
      " [-1.73241092e-01  2.76440537e-01]\n",
      " [-9.49986118e-02  3.34795928e-01]\n",
      " [-1.41758445e-02  3.53849144e-01]\n",
      " [ 6.68514577e-02  3.42835724e-01]\n",
      " [ 1.48130495e-01  2.94817214e-01]\n",
      " [ 2.15642758e-01  2.30514577e-01]\n",
      " [ 2.71669597e-01  1.54569181e-01]\n",
      " [ 3.04646923e-01  6.69023600e-02]\n",
      " [ 3.28404442e-01 -2.54530310e-02]\n",
      " [ 3.52177696e-01 -1.20121240e-01]\n",
      " [ 3.64402591e-01 -2.17180935e-01]\n",
      " [-2.98935678e-01 -2.86455345e-01]\n",
      " [-2.52459047e-01 -3.18520130e-01]\n",
      " [-1.92278575e-01 -3.25049514e-01]\n",
      " [-1.36865340e-01 -3.10795002e-01]\n",
      " [-8.38593244e-02 -2.82679315e-01]\n",
      " [ 8.26950486e-02 -2.86172143e-01]\n",
      " [ 1.40688570e-01 -3.11219806e-01]\n",
      " [ 2.03213327e-01 -3.22359093e-01]\n",
      " [ 2.65627950e-01 -3.17308653e-01]\n",
      " [ 3.13929662e-01 -2.77660342e-01]\n",
      " [ 3.49282739e-03 -2.03445627e-01]\n",
      " [ 3.03655715e-03 -1.36373901e-01]\n",
      " [ 2.51735308e-04 -6.70050902e-02]\n",
      " [-4.84590467e-03  2.34798704e-03]\n",
      " [-6.28866266e-02  3.43341046e-02]\n",
      " [-3.51957427e-02  4.37741786e-02]\n",
      " [-2.86348913e-03  5.09329014e-02]\n",
      " [ 2.72660805e-02  4.18861638e-02]\n",
      " [ 5.50985655e-02  3.05108746e-02]\n",
      " [-2.27820453e-01 -2.00393336e-01]\n",
      " [-1.92986580e-01 -2.20972698e-01]\n",
      " [-1.46730217e-01 -2.20658029e-01]\n",
      " [-1.05288292e-01 -1.92621009e-01]\n",
      " [-1.49326238e-01 -1.79043036e-01]\n",
      " [-1.95582601e-01 -1.79357705e-01]\n",
      " [ 1.05162425e-01 -1.88876446e-01]\n",
      " [ 1.44684868e-01 -2.18675613e-01]\n",
      " [ 1.95582601e-01 -2.20642295e-01]\n",
      " [ 2.37087460e-01 -2.01856548e-01]\n",
      " [ 1.97627950e-01 -1.81308653e-01]\n",
      " [ 1.49043036e-01 -1.79326238e-01]\n",
      " [-1.16805183e-01  1.40361870e-01]\n",
      " [-7.73299398e-02  1.17501157e-01]\n",
      " [-3.79176307e-02  1.03891717e-01]\n",
      " [-5.60111060e-03  1.13363258e-01]\n",
      " [ 2.45284590e-02  1.04316520e-01]\n",
      " [ 6.37362332e-02  1.20773716e-01]\n",
      " [ 1.12195280e-01  1.37293845e-01]\n",
      " [ 6.79685331e-02  1.78625636e-01]\n",
      " [ 2.85562240e-02  1.92235076e-01]\n",
      " [-6.15178158e-03  1.94311893e-01]\n",
      " [-4.08283202e-02  1.91763073e-01]\n",
      " [-8.00203609e-02  1.72993059e-01]\n",
      " [-9.60055530e-02  1.42816289e-01]\n",
      " [-4.04349838e-02  1.33942619e-01]\n",
      " [-5.75844516e-03  1.36491439e-01]\n",
      " [ 2.43239241e-02  1.34383156e-01]\n",
      " [ 8.90356317e-02  1.41762147e-01]\n",
      " [ 2.66052753e-02  1.39024526e-01]\n",
      " [-3.49282739e-03  1.43445627e-01]\n",
      " [-3.81536326e-02  1.38583989e-01]]\n",
      "imagefile\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testload = pd.read_csv(\"./outimg/Train/facefeature.csv\")\n",
    "landmarks = np.array(testload.iloc[0, 1:])\n",
    "landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "print(landmarks)\n",
    "\n",
    "dataiter = iter(testload)\n",
    "landmark = next(dataiter)\n",
    "print(landmark)\n",
    "# print(landmarks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, F, C]: torch.Size([10, 68, 2])\n",
      "Shape of Tensor y: torch.Size([10, 33]) torch.float32\n",
      "X type torch.float32\n",
      "y type torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader \n",
    "\n",
    "training_data = FaceFeatureDataset(feature_file=\"./outimg/Train/facefeature.csv\", label_file=\"./Dataset/Train/csv/train.csv\")\n",
    "test_data = FaceFeatureDataset(feature_file=\"./outimg/Test/facefeature.csv\", label_file=\"./Dataset/Test/csv/test.csv\")\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 데이터 로드 확인 \n",
    "for X, y in train_loader:\n",
    "    print(f\"Shape of X [N, F, C]: {X.shape}\") # N , Channel, H= width W = height\n",
    "    print(f\"Shape of Tensor y: {y.shape} {y.dtype}\")       \n",
    "    break\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "# print(f'Traing dat length {n_total_steps}')\n",
    "# print(y)\n",
    "print('X type', X.dtype)\n",
    "print('y type', y.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#dataiter = iter(train_loader)\n",
    "#landmark, labels = next(dataiter)\n",
    "#print(\"landmark Shape \", landmark.shape)\n",
    "#print(landmark)\n",
    "#flatten = nn.Flatten()\n",
    "#linear1 = nn.Linear(68, 32)\n",
    "#x = flatten(landmark)\n",
    "#print(x)\n",
    "#print(x.size())\n",
    "# print(landmark.shape)\n",
    "# print('linear1', x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=136, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=33, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 33\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(68 * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # print(pred)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('batch',  batch)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.233010  [    0/  100]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.216255  [    0/  100]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.225929  [    0/  100]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.232366  [    0/  100]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.229129  [    0/  100]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.219246  [    0/  100]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.232491  [    0/  100]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.218484  [    0/  100]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.231237  [    0/  100]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.237416  [    0/  100]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.243295  [    0/  100]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.207508  [    0/  100]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.241346  [    0/  100]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.223628  [    0/  100]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.213715  [    0/  100]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.227021  [    0/  100]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.224255  [    0/  100]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.211187  [    0/  100]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.211268  [    0/  100]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.220079  [    0/  100]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.219710  [    0/  100]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.207772  [    0/  100]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.215237  [    0/  100]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.226433  [    0/  100]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.217018  [    0/  100]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.220444  [    0/  100]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.214775  [    0/  100]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.231430  [    0/  100]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.228406  [    0/  100]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.241002  [    0/  100]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.224351  [    0/  100]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.219623  [    0/  100]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.245781  [    0/  100]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.239706  [    0/  100]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.218363  [    0/  100]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.232079  [    0/  100]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.235209  [    0/  100]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.229703  [    0/  100]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.217584  [    0/  100]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.223663  [    0/  100]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.232202  [    0/  100]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.222426  [    0/  100]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.219598  [    0/  100]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.200557  [    0/  100]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.218036  [    0/  100]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.218475  [    0/  100]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.231324  [    0/  100]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.223489  [    0/  100]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.229052  [    0/  100]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.216720  [    0/  100]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.209708  [    0/  100]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.217577  [    0/  100]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.222612  [    0/  100]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.220869  [    0/  100]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.216838  [    0/  100]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.206695  [    0/  100]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.208718  [    0/  100]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.206787  [    0/  100]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.208561  [    0/  100]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.217118  [    0/  100]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.225796  [    0/  100]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.189603  [    0/  100]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.212204  [    0/  100]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.212702  [    0/  100]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.208204  [    0/  100]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.211880  [    0/  100]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.206032  [    0/  100]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.202831  [    0/  100]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.203441  [    0/  100]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.209529  [    0/  100]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.198549  [    0/  100]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.216456  [    0/  100]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.217137  [    0/  100]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.220838  [    0/  100]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.214690  [    0/  100]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.211003  [    0/  100]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.205744  [    0/  100]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.209857  [    0/  100]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.203297  [    0/  100]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.213666  [    0/  100]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.199350  [    0/  100]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.205262  [    0/  100]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.202683  [    0/  100]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.196099  [    0/  100]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.215376  [    0/  100]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.187227  [    0/  100]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.206111  [    0/  100]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.181309  [    0/  100]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.199175  [    0/  100]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.199112  [    0/  100]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.201295  [    0/  100]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.205100  [    0/  100]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.216559  [    0/  100]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.194775  [    0/  100]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.220470  [    0/  100]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.199040  [    0/  100]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.197804  [    0/  100]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.193504  [    0/  100]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.211163  [    0/  100]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.195485  [    0/  100]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.198931  [    0/  100]\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.193498  [    0/  100]\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.197006  [    0/  100]\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.202975  [    0/  100]\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.205459  [    0/  100]\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.203833  [    0/  100]\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.195482  [    0/  100]\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.217189  [    0/  100]\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.200373  [    0/  100]\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.203740  [    0/  100]\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.207229  [    0/  100]\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.195689  [    0/  100]\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.207097  [    0/  100]\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.193306  [    0/  100]\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.197393  [    0/  100]\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.200820  [    0/  100]\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.198567  [    0/  100]\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.199878  [    0/  100]\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.206990  [    0/  100]\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.203190  [    0/  100]\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.195235  [    0/  100]\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.187670  [    0/  100]\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.197102  [    0/  100]\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.202604  [    0/  100]\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.203023  [    0/  100]\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.215357  [    0/  100]\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.206918  [    0/  100]\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.198944  [    0/  100]\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.202096  [    0/  100]\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.198775  [    0/  100]\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.189317  [    0/  100]\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.200405  [    0/  100]\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.182982  [    0/  100]\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.188346  [    0/  100]\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.193927  [    0/  100]\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.188941  [    0/  100]\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.192009  [    0/  100]\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.200461  [    0/  100]\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.198184  [    0/  100]\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.192940  [    0/  100]\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.202951  [    0/  100]\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.179154  [    0/  100]\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.195600  [    0/  100]\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.195354  [    0/  100]\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.202776  [    0/  100]\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.197096  [    0/  100]\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.194563  [    0/  100]\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.178282  [    0/  100]\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.194549  [    0/  100]\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.181310  [    0/  100]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.198829  [    0/  100]\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.198704  [    0/  100]\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.203813  [    0/  100]\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.200051  [    0/  100]\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.185910  [    0/  100]\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.194836  [    0/  100]\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.193350  [    0/  100]\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.192667  [    0/  100]\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.201646  [    0/  100]\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.199540  [    0/  100]\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.192470  [    0/  100]\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.196731  [    0/  100]\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.181535  [    0/  100]\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.185854  [    0/  100]\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.183788  [    0/  100]\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.186574  [    0/  100]\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.200146  [    0/  100]\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.188772  [    0/  100]\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.182779  [    0/  100]\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.193804  [    0/  100]\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.189034  [    0/  100]\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.198445  [    0/  100]\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.187898  [    0/  100]\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.165242  [    0/  100]\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.190042  [    0/  100]\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.177746  [    0/  100]\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.183682  [    0/  100]\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.188916  [    0/  100]\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.184042  [    0/  100]\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.180546  [    0/  100]\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.195630  [    0/  100]\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.189985  [    0/  100]\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.185443  [    0/  100]\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.177414  [    0/  100]\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.202555  [    0/  100]\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.177069  [    0/  100]\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.190894  [    0/  100]\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.182833  [    0/  100]\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.193719  [    0/  100]\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.193019  [    0/  100]\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.193355  [    0/  100]\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.185096  [    0/  100]\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.188377  [    0/  100]\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.195712  [    0/  100]\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.178595  [    0/  100]\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.193440  [    0/  100]\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.185414  [    0/  100]\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.180563  [    0/  100]\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.187254  [    0/  100]\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.191136  [    0/  100]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 200\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, criterion, optimizer)    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    print('test size', size )\n",
    "    # num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)            \n",
    "            print('pred =', pred)\n",
    "            #print('loss', loss)\n",
    "            #print('real', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size 100\n",
      "pred = tensor([[0.0000, 0.2925, 0.2273, 0.0000, 0.2239, 0.0000, 0.1286, 0.1859, 0.3306,\n",
      "         0.1947, 0.4036, 0.0000, 0.0000, 0.0000, 0.2584, 0.1199, 0.0000, 0.2126,\n",
      "         0.0000, 0.1994, 0.1438, 0.0000, 0.1357, 0.2535, 0.1694, 0.0000, 0.1264,\n",
      "         0.0000, 0.0000, 0.1993, 0.0000, 0.1827, 0.1842],\n",
      "        [0.0000, 0.2956, 0.2309, 0.0000, 0.2246, 0.0000, 0.1271, 0.1859, 0.3324,\n",
      "         0.1936, 0.4054, 0.0000, 0.0000, 0.0000, 0.2544, 0.1177, 0.0000, 0.2116,\n",
      "         0.0000, 0.2039, 0.1441, 0.0000, 0.1381, 0.2554, 0.1680, 0.0000, 0.1258,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1813, 0.1844],\n",
      "        [0.0000, 0.2944, 0.2292, 0.0000, 0.2248, 0.0000, 0.1283, 0.1855, 0.3303,\n",
      "         0.1919, 0.4042, 0.0000, 0.0000, 0.0000, 0.2574, 0.1164, 0.0000, 0.2114,\n",
      "         0.0000, 0.2013, 0.1436, 0.0000, 0.1383, 0.2536, 0.1687, 0.0000, 0.1258,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1823, 0.1834],\n",
      "        [0.0000, 0.2950, 0.2309, 0.0000, 0.2248, 0.0000, 0.1289, 0.1830, 0.3296,\n",
      "         0.1909, 0.4033, 0.0000, 0.0000, 0.0000, 0.2557, 0.1153, 0.0000, 0.2107,\n",
      "         0.0000, 0.2008, 0.1441, 0.0000, 0.1391, 0.2523, 0.1668, 0.0000, 0.1267,\n",
      "         0.0000, 0.0000, 0.2005, 0.0000, 0.1815, 0.1828],\n",
      "        [0.0000, 0.2954, 0.2309, 0.0000, 0.2243, 0.0000, 0.1282, 0.1854, 0.3318,\n",
      "         0.1937, 0.4046, 0.0000, 0.0000, 0.0000, 0.2556, 0.1174, 0.0000, 0.2133,\n",
      "         0.0000, 0.2011, 0.1439, 0.0000, 0.1383, 0.2549, 0.1674, 0.0000, 0.1270,\n",
      "         0.0000, 0.0000, 0.2005, 0.0000, 0.1823, 0.1842],\n",
      "        [0.0000, 0.2963, 0.2320, 0.0000, 0.2248, 0.0000, 0.1290, 0.1861, 0.3327,\n",
      "         0.1941, 0.4063, 0.0000, 0.0000, 0.0000, 0.2569, 0.1163, 0.0000, 0.2131,\n",
      "         0.0000, 0.2026, 0.1434, 0.0000, 0.1382, 0.2569, 0.1687, 0.0000, 0.1273,\n",
      "         0.0000, 0.0000, 0.2004, 0.0000, 0.1829, 0.1840],\n",
      "        [0.0000, 0.2932, 0.2284, 0.0000, 0.2257, 0.0000, 0.1279, 0.1853, 0.3295,\n",
      "         0.1900, 0.4044, 0.0000, 0.0000, 0.0000, 0.2542, 0.1161, 0.0000, 0.2097,\n",
      "         0.0000, 0.2028, 0.1428, 0.0000, 0.1374, 0.2532, 0.1694, 0.0000, 0.1245,\n",
      "         0.0000, 0.0000, 0.1993, 0.0000, 0.1803, 0.1844],\n",
      "        [0.0000, 0.2934, 0.2325, 0.0000, 0.2239, 0.0000, 0.1301, 0.1806, 0.3263,\n",
      "         0.1897, 0.4008, 0.0000, 0.0000, 0.0000, 0.2566, 0.1130, 0.0000, 0.2089,\n",
      "         0.0000, 0.1988, 0.1424, 0.0000, 0.1412, 0.2501, 0.1661, 0.0000, 0.1257,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1828, 0.1800],\n",
      "        [0.0000, 0.2960, 0.2290, 0.0000, 0.2247, 0.0000, 0.1294, 0.1856, 0.3323,\n",
      "         0.1943, 0.4055, 0.0000, 0.0000, 0.0000, 0.2597, 0.1178, 0.0000, 0.2146,\n",
      "         0.0000, 0.1998, 0.1454, 0.0000, 0.1368, 0.2548, 0.1685, 0.0000, 0.1295,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1833, 0.1844],\n",
      "        [0.0000, 0.2935, 0.2325, 0.0000, 0.2218, 0.0000, 0.1291, 0.1785, 0.3258,\n",
      "         0.1920, 0.3994, 0.0000, 0.0000, 0.0000, 0.2560, 0.1154, 0.0000, 0.2082,\n",
      "         0.0000, 0.1984, 0.1444, 0.0000, 0.1403, 0.2490, 0.1652, 0.0000, 0.1269,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1821, 0.1789]], device='cuda:0')\n",
      "pred = tensor([[0.0000, 0.2941, 0.2287, 0.0000, 0.2245, 0.0000, 0.1290, 0.1840, 0.3302,\n",
      "         0.1930, 0.4037, 0.0000, 0.0000, 0.0000, 0.2579, 0.1172, 0.0000, 0.2120,\n",
      "         0.0000, 0.2001, 0.1454, 0.0000, 0.1364, 0.2532, 0.1687, 0.0000, 0.1287,\n",
      "         0.0000, 0.0000, 0.1998, 0.0000, 0.1818, 0.1837],\n",
      "        [0.0000, 0.2974, 0.2279, 0.0000, 0.2271, 0.0000, 0.1300, 0.1903, 0.3364,\n",
      "         0.1953, 0.4098, 0.0000, 0.0000, 0.0000, 0.2613, 0.1204, 0.0000, 0.2171,\n",
      "         0.0000, 0.2023, 0.1438, 0.0000, 0.1347, 0.2590, 0.1704, 0.0000, 0.1288,\n",
      "         0.0000, 0.0000, 0.2008, 0.0000, 0.1840, 0.1869],\n",
      "        [0.0000, 0.2971, 0.2300, 0.0000, 0.2242, 0.0000, 0.1286, 0.1859, 0.3337,\n",
      "         0.1965, 0.4066, 0.0000, 0.0000, 0.0000, 0.2590, 0.1190, 0.0000, 0.2152,\n",
      "         0.0000, 0.2014, 0.1465, 0.0000, 0.1362, 0.2562, 0.1687, 0.0000, 0.1308,\n",
      "         0.0000, 0.0000, 0.2011, 0.0000, 0.1832, 0.1848],\n",
      "        [0.0000, 0.2992, 0.2292, 0.0000, 0.2257, 0.0000, 0.1289, 0.1895, 0.3379,\n",
      "         0.1981, 0.4098, 0.0000, 0.0000, 0.0000, 0.2616, 0.1213, 0.0000, 0.2186,\n",
      "         0.0000, 0.2028, 0.1467, 0.0000, 0.1351, 0.2595, 0.1693, 0.0000, 0.1316,\n",
      "         0.0000, 0.0000, 0.2022, 0.0000, 0.1841, 0.1869],\n",
      "        [0.0000, 0.2924, 0.2292, 0.0000, 0.2228, 0.0000, 0.1293, 0.1809, 0.3266,\n",
      "         0.1914, 0.3998, 0.0000, 0.0000, 0.0000, 0.2575, 0.1169, 0.0000, 0.2100,\n",
      "         0.0000, 0.1967, 0.1439, 0.0000, 0.1390, 0.2490, 0.1659, 0.0000, 0.1259,\n",
      "         0.0000, 0.0000, 0.2000, 0.0000, 0.1823, 0.1809],\n",
      "        [0.0000, 0.2918, 0.2291, 0.0000, 0.2236, 0.0000, 0.1288, 0.1820, 0.3270,\n",
      "         0.1909, 0.3999, 0.0000, 0.0000, 0.0000, 0.2569, 0.1153, 0.0000, 0.2095,\n",
      "         0.0000, 0.1980, 0.1443, 0.0000, 0.1388, 0.2500, 0.1670, 0.0000, 0.1261,\n",
      "         0.0000, 0.0000, 0.1990, 0.0000, 0.1818, 0.1817],\n",
      "        [0.0000, 0.2954, 0.2325, 0.0000, 0.2225, 0.0000, 0.1291, 0.1807, 0.3278,\n",
      "         0.1927, 0.4014, 0.0000, 0.0000, 0.0000, 0.2572, 0.1157, 0.0000, 0.2113,\n",
      "         0.0000, 0.1986, 0.1445, 0.0000, 0.1406, 0.2507, 0.1656, 0.0000, 0.1282,\n",
      "         0.0000, 0.0000, 0.2012, 0.0000, 0.1835, 0.1801],\n",
      "        [0.0000, 0.2954, 0.2301, 0.0000, 0.2255, 0.0000, 0.1269, 0.1880, 0.3332,\n",
      "         0.1932, 0.4060, 0.0000, 0.0000, 0.0000, 0.2535, 0.1175, 0.0000, 0.2144,\n",
      "         0.0000, 0.2034, 0.1438, 0.0000, 0.1374, 0.2573, 0.1692, 0.0000, 0.1272,\n",
      "         0.0000, 0.0000, 0.1998, 0.0000, 0.1813, 0.1862],\n",
      "        [0.0000, 0.2946, 0.2298, 0.0000, 0.2237, 0.0000, 0.1291, 0.1835, 0.3307,\n",
      "         0.1941, 0.4032, 0.0000, 0.0000, 0.0000, 0.2587, 0.1183, 0.0000, 0.2118,\n",
      "         0.0000, 0.2000, 0.1452, 0.0000, 0.1373, 0.2534, 0.1674, 0.0000, 0.1277,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1823, 0.1825],\n",
      "        [0.0000, 0.2952, 0.2281, 0.0000, 0.2257, 0.0000, 0.1280, 0.1869, 0.3330,\n",
      "         0.1922, 0.4053, 0.0000, 0.0000, 0.0000, 0.2555, 0.1163, 0.0000, 0.2140,\n",
      "         0.0000, 0.2008, 0.1457, 0.0000, 0.1362, 0.2561, 0.1687, 0.0000, 0.1279,\n",
      "         0.0000, 0.0000, 0.1993, 0.0000, 0.1806, 0.1869]], device='cuda:0')\n",
      "pred = tensor([[0.0000, 0.2940, 0.2340, 0.0000, 0.2243, 0.0000, 0.1297, 0.1797, 0.3257,\n",
      "         0.1878, 0.4006, 0.0000, 0.0000, 0.0000, 0.2543, 0.1113, 0.0000, 0.2073,\n",
      "         0.0000, 0.2001, 0.1418, 0.0000, 0.1430, 0.2494, 0.1649, 0.0000, 0.1243,\n",
      "         0.0000, 0.0000, 0.2004, 0.0000, 0.1819, 0.1795],\n",
      "        [0.0000, 0.2951, 0.2303, 0.0000, 0.2246, 0.0000, 0.1298, 0.1839, 0.3311,\n",
      "         0.1938, 0.4043, 0.0000, 0.0000, 0.0000, 0.2603, 0.1173, 0.0000, 0.2115,\n",
      "         0.0000, 0.2007, 0.1445, 0.0000, 0.1380, 0.2533, 0.1676, 0.0000, 0.1275,\n",
      "         0.0000, 0.0000, 0.2009, 0.0000, 0.1833, 0.1824],\n",
      "        [0.0000, 0.2963, 0.2296, 0.0000, 0.2252, 0.0000, 0.1296, 0.1854, 0.3316,\n",
      "         0.1926, 0.4061, 0.0000, 0.0000, 0.0000, 0.2606, 0.1156, 0.0000, 0.2118,\n",
      "         0.0000, 0.2016, 0.1446, 0.0000, 0.1381, 0.2545, 0.1690, 0.0000, 0.1273,\n",
      "         0.0000, 0.0000, 0.2005, 0.0000, 0.1832, 0.1833],\n",
      "        [0.0000, 0.2959, 0.2282, 0.0000, 0.2257, 0.0000, 0.1292, 0.1875, 0.3331,\n",
      "         0.1934, 0.4062, 0.0000, 0.0000, 0.0000, 0.2611, 0.1175, 0.0000, 0.2134,\n",
      "         0.0000, 0.2008, 0.1445, 0.0000, 0.1370, 0.2550, 0.1688, 0.0000, 0.1273,\n",
      "         0.0000, 0.0000, 0.2000, 0.0000, 0.1838, 0.1846],\n",
      "        [0.0000, 0.2941, 0.2324, 0.0000, 0.2242, 0.0000, 0.1295, 0.1807, 0.3268,\n",
      "         0.1897, 0.4013, 0.0000, 0.0000, 0.0000, 0.2553, 0.1138, 0.0000, 0.2094,\n",
      "         0.0000, 0.1997, 0.1430, 0.0000, 0.1409, 0.2502, 0.1659, 0.0000, 0.1264,\n",
      "         0.0000, 0.0000, 0.2004, 0.0000, 0.1822, 0.1806],\n",
      "        [0.0000, 0.2962, 0.2317, 0.0000, 0.2261, 0.0000, 0.1297, 0.1852, 0.3320,\n",
      "         0.1925, 0.4065, 0.0000, 0.0000, 0.0000, 0.2576, 0.1169, 0.0000, 0.2115,\n",
      "         0.0000, 0.2034, 0.1423, 0.0000, 0.1390, 0.2543, 0.1676, 0.0000, 0.1259,\n",
      "         0.0000, 0.0000, 0.2015, 0.0000, 0.1830, 0.1833],\n",
      "        [0.0000, 0.2957, 0.2293, 0.0000, 0.2254, 0.0000, 0.1303, 0.1854, 0.3316,\n",
      "         0.1934, 0.4052, 0.0000, 0.0000, 0.0000, 0.2615, 0.1180, 0.0000, 0.2140,\n",
      "         0.0000, 0.1996, 0.1440, 0.0000, 0.1376, 0.2538, 0.1681, 0.0000, 0.1286,\n",
      "         0.0000, 0.0000, 0.2009, 0.0000, 0.1844, 0.1831],\n",
      "        [0.0000, 0.2953, 0.2286, 0.0000, 0.2262, 0.0000, 0.1289, 0.1895, 0.3344,\n",
      "         0.1950, 0.4085, 0.0000, 0.0000, 0.0000, 0.2583, 0.1170, 0.0000, 0.2151,\n",
      "         0.0000, 0.2026, 0.1441, 0.0000, 0.1353, 0.2584, 0.1716, 0.0000, 0.1285,\n",
      "         0.0000, 0.0000, 0.1989, 0.0000, 0.1830, 0.1874],\n",
      "        [0.0000, 0.2968, 0.2326, 0.0000, 0.2257, 0.0000, 0.1286, 0.1858, 0.3324,\n",
      "         0.1926, 0.4063, 0.0000, 0.0000, 0.0000, 0.2544, 0.1156, 0.0000, 0.2137,\n",
      "         0.0000, 0.2029, 0.1433, 0.0000, 0.1392, 0.2560, 0.1676, 0.0000, 0.1279,\n",
      "         0.0000, 0.0000, 0.2008, 0.0000, 0.1823, 0.1847],\n",
      "        [0.0000, 0.2939, 0.2290, 0.0000, 0.2240, 0.0000, 0.1290, 0.1840, 0.3296,\n",
      "         0.1929, 0.4031, 0.0000, 0.0000, 0.0000, 0.2573, 0.1156, 0.0000, 0.2126,\n",
      "         0.0000, 0.1981, 0.1452, 0.0000, 0.1375, 0.2524, 0.1681, 0.0000, 0.1283,\n",
      "         0.0000, 0.0000, 0.1991, 0.0000, 0.1825, 0.1842]], device='cuda:0')\n",
      "pred = tensor([[0.0000, 0.2948, 0.2277, 0.0000, 0.2252, 0.0000, 0.1278, 0.1877, 0.3323,\n",
      "         0.1927, 0.4057, 0.0000, 0.0000, 0.0000, 0.2554, 0.1164, 0.0000, 0.2147,\n",
      "         0.0000, 0.2007, 0.1450, 0.0000, 0.1359, 0.2565, 0.1703, 0.0000, 0.1283,\n",
      "         0.0000, 0.0000, 0.1987, 0.0000, 0.1812, 0.1869],\n",
      "        [0.0000, 0.2951, 0.2291, 0.0000, 0.2262, 0.0000, 0.1284, 0.1878, 0.3326,\n",
      "         0.1927, 0.4067, 0.0000, 0.0000, 0.0000, 0.2571, 0.1162, 0.0000, 0.2128,\n",
      "         0.0000, 0.2028, 0.1437, 0.0000, 0.1373, 0.2558, 0.1698, 0.0000, 0.1267,\n",
      "         0.0000, 0.0000, 0.1996, 0.0000, 0.1823, 0.1858],\n",
      "        [0.0000, 0.2970, 0.2296, 0.0000, 0.2260, 0.0000, 0.1283, 0.1877, 0.3333,\n",
      "         0.1926, 0.4079, 0.0000, 0.0000, 0.0000, 0.2580, 0.1161, 0.0000, 0.2126,\n",
      "         0.0000, 0.2036, 0.1438, 0.0000, 0.1382, 0.2560, 0.1693, 0.0000, 0.1260,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1826, 0.1853],\n",
      "        [0.0000, 0.2990, 0.2302, 0.0000, 0.2271, 0.0000, 0.1299, 0.1902, 0.3390,\n",
      "         0.1969, 0.4112, 0.0000, 0.0000, 0.0000, 0.2621, 0.1193, 0.0000, 0.2166,\n",
      "         0.0000, 0.2043, 0.1447, 0.0000, 0.1363, 0.2598, 0.1688, 0.0000, 0.1284,\n",
      "         0.0000, 0.0000, 0.2020, 0.0000, 0.1842, 0.1874],\n",
      "        [0.0000, 0.2924, 0.2271, 0.0000, 0.2237, 0.0000, 0.1270, 0.1848, 0.3298,\n",
      "         0.1923, 0.4017, 0.0000, 0.0000, 0.0000, 0.2546, 0.1187, 0.0000, 0.2116,\n",
      "         0.0000, 0.1997, 0.1451, 0.0000, 0.1363, 0.2530, 0.1682, 0.0000, 0.1261,\n",
      "         0.0000, 0.0000, 0.1991, 0.0000, 0.1801, 0.1843],\n",
      "        [0.0000, 0.2941, 0.2273, 0.0000, 0.2241, 0.0000, 0.1272, 0.1875, 0.3331,\n",
      "         0.1957, 0.4049, 0.0000, 0.0000, 0.0000, 0.2571, 0.1204, 0.0000, 0.2144,\n",
      "         0.0000, 0.2010, 0.1457, 0.0000, 0.1350, 0.2560, 0.1695, 0.0000, 0.1279,\n",
      "         0.0000, 0.0000, 0.1996, 0.0000, 0.1818, 0.1860],\n",
      "        [0.0000, 0.2941, 0.2279, 0.0000, 0.2250, 0.0000, 0.1256, 0.1865, 0.3313,\n",
      "         0.1912, 0.4044, 0.0000, 0.0000, 0.0000, 0.2531, 0.1178, 0.0000, 0.2096,\n",
      "         0.0000, 0.2047, 0.1445, 0.0000, 0.1373, 0.2543, 0.1691, 0.0000, 0.1238,\n",
      "         0.0000, 0.0000, 0.1998, 0.0000, 0.1792, 0.1849],\n",
      "        [0.0000, 0.2940, 0.2303, 0.0000, 0.2251, 0.0000, 0.1301, 0.1839, 0.3296,\n",
      "         0.1925, 0.4040, 0.0000, 0.0000, 0.0000, 0.2595, 0.1155, 0.0000, 0.2113,\n",
      "         0.0000, 0.2003, 0.1435, 0.0000, 0.1384, 0.2525, 0.1684, 0.0000, 0.1276,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1836, 0.1826],\n",
      "        [0.0000, 0.2973, 0.2268, 0.0000, 0.2259, 0.0000, 0.1283, 0.1900, 0.3379,\n",
      "         0.1978, 0.4097, 0.0000, 0.0000, 0.0000, 0.2593, 0.1228, 0.0000, 0.2174,\n",
      "         0.0000, 0.2031, 0.1468, 0.0000, 0.1325, 0.2597, 0.1704, 0.0000, 0.1302,\n",
      "         0.0000, 0.0000, 0.2011, 0.0000, 0.1818, 0.1887],\n",
      "        [0.0000, 0.2962, 0.2278, 0.0000, 0.2259, 0.0000, 0.1277, 0.1892, 0.3361,\n",
      "         0.1957, 0.4072, 0.0000, 0.0000, 0.0000, 0.2580, 0.1209, 0.0000, 0.2164,\n",
      "         0.0000, 0.2024, 0.1458, 0.0000, 0.1352, 0.2578, 0.1688, 0.0000, 0.1286,\n",
      "         0.0000, 0.0000, 0.2009, 0.0000, 0.1820, 0.1874]], device='cuda:0')\n",
      "pred = tensor([[0.0000, 0.2942, 0.2292, 0.0000, 0.2231, 0.0000, 0.1272, 0.1837, 0.3302,\n",
      "         0.1934, 0.4024, 0.0000, 0.0000, 0.0000, 0.2548, 0.1179, 0.0000, 0.2118,\n",
      "         0.0000, 0.2003, 0.1460, 0.0000, 0.1370, 0.2536, 0.1677, 0.0000, 0.1278,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1806, 0.1836],\n",
      "        [0.0000, 0.2943, 0.2271, 0.0000, 0.2243, 0.0000, 0.1282, 0.1864, 0.3321,\n",
      "         0.1942, 0.4045, 0.0000, 0.0000, 0.0000, 0.2575, 0.1175, 0.0000, 0.2141,\n",
      "         0.0000, 0.1989, 0.1463, 0.0000, 0.1354, 0.2550, 0.1693, 0.0000, 0.1286,\n",
      "         0.0000, 0.0000, 0.1989, 0.0000, 0.1817, 0.1863],\n",
      "        [0.0000, 0.2948, 0.2317, 0.0000, 0.2247, 0.0000, 0.1304, 0.1829, 0.3283,\n",
      "         0.1907, 0.4037, 0.0000, 0.0000, 0.0000, 0.2587, 0.1132, 0.0000, 0.2102,\n",
      "         0.0000, 0.1997, 0.1424, 0.0000, 0.1405, 0.2517, 0.1675, 0.0000, 0.1259,\n",
      "         0.0000, 0.0000, 0.2001, 0.0000, 0.1838, 0.1815],\n",
      "        [0.0000, 0.2957, 0.2306, 0.0000, 0.2243, 0.0000, 0.1290, 0.1838, 0.3314,\n",
      "         0.1938, 0.4040, 0.0000, 0.0000, 0.0000, 0.2578, 0.1185, 0.0000, 0.2129,\n",
      "         0.0000, 0.2005, 0.1447, 0.0000, 0.1382, 0.2533, 0.1665, 0.0000, 0.1280,\n",
      "         0.0000, 0.0000, 0.2015, 0.0000, 0.1827, 0.1829],\n",
      "        [0.0000, 0.2944, 0.2302, 0.0000, 0.2249, 0.0000, 0.1282, 0.1831, 0.3284,\n",
      "         0.1897, 0.4036, 0.0000, 0.0000, 0.0000, 0.2555, 0.1153, 0.0000, 0.2080,\n",
      "         0.0000, 0.2026, 0.1428, 0.0000, 0.1397, 0.2511, 0.1675, 0.0000, 0.1237,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1812, 0.1820],\n",
      "        [0.0000, 0.2931, 0.2312, 0.0000, 0.2241, 0.0000, 0.1280, 0.1818, 0.3268,\n",
      "         0.1892, 0.4013, 0.0000, 0.0000, 0.0000, 0.2522, 0.1138, 0.0000, 0.2088,\n",
      "         0.0000, 0.2005, 0.1431, 0.0000, 0.1398, 0.2510, 0.1671, 0.0000, 0.1253,\n",
      "         0.0000, 0.0000, 0.1993, 0.0000, 0.1804, 0.1821],\n",
      "        [0.0000, 0.2984, 0.2299, 0.0000, 0.2277, 0.0000, 0.1307, 0.1912, 0.3375,\n",
      "         0.1961, 0.4120, 0.0000, 0.0000, 0.0000, 0.2615, 0.1162, 0.0000, 0.2180,\n",
      "         0.0000, 0.2031, 0.1443, 0.0000, 0.1354, 0.2612, 0.1718, 0.0000, 0.1307,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1848, 0.1886],\n",
      "        [0.0000, 0.2925, 0.2323, 0.0000, 0.2227, 0.0000, 0.1291, 0.1784, 0.3237,\n",
      "         0.1888, 0.3986, 0.0000, 0.0000, 0.0000, 0.2550, 0.1127, 0.0000, 0.2073,\n",
      "         0.0000, 0.1983, 0.1431, 0.0000, 0.1417, 0.2478, 0.1657, 0.0000, 0.1259,\n",
      "         0.0000, 0.0000, 0.1997, 0.0000, 0.1820, 0.1785],\n",
      "        [0.0000, 0.2914, 0.2308, 0.0000, 0.2228, 0.0000, 0.1286, 0.1800, 0.3248,\n",
      "         0.1899, 0.3990, 0.0000, 0.0000, 0.0000, 0.2544, 0.1150, 0.0000, 0.2076,\n",
      "         0.0000, 0.1989, 0.1428, 0.0000, 0.1397, 0.2491, 0.1668, 0.0000, 0.1251,\n",
      "         0.0000, 0.0000, 0.1992, 0.0000, 0.1813, 0.1798],\n",
      "        [0.0000, 0.2917, 0.2310, 0.0000, 0.2226, 0.0000, 0.1281, 0.1798, 0.3246,\n",
      "         0.1896, 0.3983, 0.0000, 0.0000, 0.0000, 0.2541, 0.1136, 0.0000, 0.2080,\n",
      "         0.0000, 0.1984, 0.1438, 0.0000, 0.1404, 0.2489, 0.1665, 0.0000, 0.1260,\n",
      "         0.0000, 0.0000, 0.1990, 0.0000, 0.1812, 0.1798]], device='cuda:0')\n",
      "pred = tensor([[0.0000, 0.2970, 0.2284, 0.0000, 0.2255, 0.0000, 0.1281, 0.1893, 0.3363,\n",
      "         0.1968, 0.4083, 0.0000, 0.0000, 0.0000, 0.2587, 0.1202, 0.0000, 0.2168,\n",
      "         0.0000, 0.2025, 0.1460, 0.0000, 0.1349, 0.2589, 0.1698, 0.0000, 0.1298,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1827, 0.1874],\n",
      "        [0.0000, 0.2943, 0.2285, 0.0000, 0.2259, 0.0000, 0.1287, 0.1844, 0.3290,\n",
      "         0.1885, 0.4038, 0.0000, 0.0000, 0.0000, 0.2559, 0.1149, 0.0000, 0.2100,\n",
      "         0.0000, 0.2011, 0.1431, 0.0000, 0.1387, 0.2519, 0.1680, 0.0000, 0.1248,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1809, 0.1836],\n",
      "        [0.0000, 0.2954, 0.2315, 0.0000, 0.2242, 0.0000, 0.1292, 0.1834, 0.3301,\n",
      "         0.1927, 0.4034, 0.0000, 0.0000, 0.0000, 0.2575, 0.1162, 0.0000, 0.2125,\n",
      "         0.0000, 0.2000, 0.1441, 0.0000, 0.1394, 0.2528, 0.1667, 0.0000, 0.1280,\n",
      "         0.0000, 0.0000, 0.2009, 0.0000, 0.1831, 0.1823],\n",
      "        [0.0000, 0.2915, 0.2313, 0.0000, 0.2222, 0.0000, 0.1297, 0.1781, 0.3236,\n",
      "         0.1895, 0.3979, 0.0000, 0.0000, 0.0000, 0.2567, 0.1133, 0.0000, 0.2061,\n",
      "         0.0000, 0.1969, 0.1432, 0.0000, 0.1411, 0.2469, 0.1654, 0.0000, 0.1245,\n",
      "         0.0000, 0.0000, 0.1994, 0.0000, 0.1821, 0.1783],\n",
      "        [0.0000, 0.2943, 0.2323, 0.0000, 0.2227, 0.0000, 0.1288, 0.1810, 0.3284,\n",
      "         0.1935, 0.4021, 0.0000, 0.0000, 0.0000, 0.2556, 0.1175, 0.0000, 0.2098,\n",
      "         0.0000, 0.2007, 0.1438, 0.0000, 0.1389, 0.2518, 0.1665, 0.0000, 0.1269,\n",
      "         0.0000, 0.0000, 0.2010, 0.0000, 0.1821, 0.1806],\n",
      "        [0.0000, 0.2945, 0.2330, 0.0000, 0.2239, 0.0000, 0.1290, 0.1808, 0.3261,\n",
      "         0.1895, 0.4026, 0.0000, 0.0000, 0.0000, 0.2555, 0.1134, 0.0000, 0.2063,\n",
      "         0.0000, 0.2022, 0.1414, 0.0000, 0.1422, 0.2493, 0.1665, 0.0000, 0.1232,\n",
      "         0.0000, 0.0000, 0.2009, 0.0000, 0.1825, 0.1793],\n",
      "        [0.0000, 0.2938, 0.2282, 0.0000, 0.2248, 0.0000, 0.1271, 0.1868, 0.3319,\n",
      "         0.1933, 0.4044, 0.0000, 0.0000, 0.0000, 0.2550, 0.1179, 0.0000, 0.2127,\n",
      "         0.0000, 0.2021, 0.1450, 0.0000, 0.1360, 0.2558, 0.1696, 0.0000, 0.1271,\n",
      "         0.0000, 0.0000, 0.1991, 0.0000, 0.1807, 0.1855],\n",
      "        [0.0000, 0.2949, 0.2315, 0.0000, 0.2254, 0.0000, 0.1282, 0.1841, 0.3300,\n",
      "         0.1906, 0.4045, 0.0000, 0.0000, 0.0000, 0.2531, 0.1157, 0.0000, 0.2102,\n",
      "         0.0000, 0.2031, 0.1425, 0.0000, 0.1392, 0.2533, 0.1674, 0.0000, 0.1251,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1809, 0.1835],\n",
      "        [0.0000, 0.2985, 0.2280, 0.0000, 0.2253, 0.0000, 0.1286, 0.1902, 0.3381,\n",
      "         0.1985, 0.4093, 0.0000, 0.0000, 0.0000, 0.2624, 0.1215, 0.0000, 0.2188,\n",
      "         0.0000, 0.2019, 0.1471, 0.0000, 0.1346, 0.2599, 0.1697, 0.0000, 0.1312,\n",
      "         0.0000, 0.0000, 0.2015, 0.0000, 0.1841, 0.1873],\n",
      "        [0.0000, 0.2950, 0.2314, 0.0000, 0.2238, 0.0000, 0.1303, 0.1815, 0.3284,\n",
      "         0.1918, 0.4020, 0.0000, 0.0000, 0.0000, 0.2594, 0.1153, 0.0000, 0.2113,\n",
      "         0.0000, 0.1981, 0.1441, 0.0000, 0.1401, 0.2508, 0.1659, 0.0000, 0.1277,\n",
      "         0.0000, 0.0000, 0.2010, 0.0000, 0.1836, 0.1808]], device='cuda:0')\n",
      "pred = tensor([[0.0000, 0.2934, 0.2282, 0.0000, 0.2251, 0.0000, 0.1278, 0.1857, 0.3301,\n",
      "         0.1901, 0.4032, 0.0000, 0.0000, 0.0000, 0.2545, 0.1137, 0.0000, 0.2115,\n",
      "         0.0000, 0.1999, 0.1449, 0.0000, 0.1376, 0.2541, 0.1689, 0.0000, 0.1261,\n",
      "         0.0000, 0.0000, 0.1981, 0.0000, 0.1803, 0.1856],\n",
      "        [0.0000, 0.2936, 0.2293, 0.0000, 0.2252, 0.0000, 0.1281, 0.1857, 0.3295,\n",
      "         0.1909, 0.4041, 0.0000, 0.0000, 0.0000, 0.2566, 0.1144, 0.0000, 0.2107,\n",
      "         0.0000, 0.2016, 0.1433, 0.0000, 0.1388, 0.2532, 0.1694, 0.0000, 0.1258,\n",
      "         0.0000, 0.0000, 0.1989, 0.0000, 0.1823, 0.1838],\n",
      "        [0.0000, 0.2922, 0.2305, 0.0000, 0.2227, 0.0000, 0.1290, 0.1794, 0.3250,\n",
      "         0.1901, 0.3992, 0.0000, 0.0000, 0.0000, 0.2561, 0.1154, 0.0000, 0.2073,\n",
      "         0.0000, 0.1985, 0.1434, 0.0000, 0.1400, 0.2480, 0.1661, 0.0000, 0.1250,\n",
      "         0.0000, 0.0000, 0.2000, 0.0000, 0.1816, 0.1793],\n",
      "        [0.0000, 0.2976, 0.2317, 0.0000, 0.2265, 0.0000, 0.1297, 0.1888, 0.3360,\n",
      "         0.1956, 0.4094, 0.0000, 0.0000, 0.0000, 0.2591, 0.1172, 0.0000, 0.2154,\n",
      "         0.0000, 0.2035, 0.1435, 0.0000, 0.1376, 0.2586, 0.1689, 0.0000, 0.1281,\n",
      "         0.0000, 0.0000, 0.2010, 0.0000, 0.1841, 0.1863],\n",
      "        [0.0000, 0.2899, 0.2293, 0.0000, 0.2225, 0.0000, 0.1276, 0.1788, 0.3228,\n",
      "         0.1879, 0.3966, 0.0000, 0.0000, 0.0000, 0.2521, 0.1141, 0.0000, 0.2066,\n",
      "         0.0000, 0.1975, 0.1437, 0.0000, 0.1398, 0.2470, 0.1664, 0.0000, 0.1248,\n",
      "         0.0000, 0.0000, 0.1984, 0.0000, 0.1797, 0.1799],\n",
      "        [0.0000, 0.2926, 0.2309, 0.0000, 0.2215, 0.0000, 0.1277, 0.1789, 0.3251,\n",
      "         0.1907, 0.3982, 0.0000, 0.0000, 0.0000, 0.2536, 0.1156, 0.0000, 0.2083,\n",
      "         0.0000, 0.1980, 0.1449, 0.0000, 0.1398, 0.2488, 0.1656, 0.0000, 0.1263,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1806, 0.1796],\n",
      "        [0.0000, 0.2979, 0.2312, 0.0000, 0.2253, 0.0000, 0.1292, 0.1874, 0.3353,\n",
      "         0.1955, 0.4074, 0.0000, 0.0000, 0.0000, 0.2588, 0.1181, 0.0000, 0.2159,\n",
      "         0.0000, 0.2016, 0.1449, 0.0000, 0.1378, 0.2574, 0.1674, 0.0000, 0.1287,\n",
      "         0.0000, 0.0000, 0.2015, 0.0000, 0.1836, 0.1856],\n",
      "        [0.0000, 0.2910, 0.2309, 0.0000, 0.2214, 0.0000, 0.1292, 0.1778, 0.3231,\n",
      "         0.1904, 0.3974, 0.0000, 0.0000, 0.0000, 0.2559, 0.1149, 0.0000, 0.2068,\n",
      "         0.0000, 0.1968, 0.1433, 0.0000, 0.1402, 0.2469, 0.1659, 0.0000, 0.1255,\n",
      "         0.0000, 0.0000, 0.1994, 0.0000, 0.1820, 0.1781],\n",
      "        [0.0000, 0.2914, 0.2286, 0.0000, 0.2224, 0.0000, 0.1273, 0.1824, 0.3277,\n",
      "         0.1931, 0.4001, 0.0000, 0.0000, 0.0000, 0.2546, 0.1183, 0.0000, 0.2096,\n",
      "         0.0000, 0.1992, 0.1447, 0.0000, 0.1371, 0.2510, 0.1677, 0.0000, 0.1258,\n",
      "         0.0000, 0.0000, 0.1990, 0.0000, 0.1807, 0.1822],\n",
      "        [0.0000, 0.2961, 0.2331, 0.0000, 0.2256, 0.0000, 0.1302, 0.1840, 0.3307,\n",
      "         0.1920, 0.4059, 0.0000, 0.0000, 0.0000, 0.2578, 0.1149, 0.0000, 0.2106,\n",
      "         0.0000, 0.2033, 0.1421, 0.0000, 0.1398, 0.2543, 0.1678, 0.0000, 0.1263,\n",
      "         0.0000, 0.0000, 0.2011, 0.0000, 0.1833, 0.1820]], device='cuda:0')\n",
      "pred = tensor([[0.0000, 0.2966, 0.2312, 0.0000, 0.2241, 0.0000, 0.1291, 0.1842, 0.3318,\n",
      "         0.1942, 0.4053, 0.0000, 0.0000, 0.0000, 0.2571, 0.1168, 0.0000, 0.2134,\n",
      "         0.0000, 0.2006, 0.1451, 0.0000, 0.1378, 0.2549, 0.1675, 0.0000, 0.1289,\n",
      "         0.0000, 0.0000, 0.2009, 0.0000, 0.1826, 0.1839],\n",
      "        [0.0000, 0.2936, 0.2278, 0.0000, 0.2245, 0.0000, 0.1279, 0.1857, 0.3315,\n",
      "         0.1939, 0.4038, 0.0000, 0.0000, 0.0000, 0.2571, 0.1191, 0.0000, 0.2126,\n",
      "         0.0000, 0.2010, 0.1453, 0.0000, 0.1358, 0.2544, 0.1690, 0.0000, 0.1276,\n",
      "         0.0000, 0.0000, 0.1997, 0.0000, 0.1814, 0.1846],\n",
      "        [0.0000, 0.2940, 0.2317, 0.0000, 0.2233, 0.0000, 0.1290, 0.1813, 0.3277,\n",
      "         0.1914, 0.4011, 0.0000, 0.0000, 0.0000, 0.2559, 0.1162, 0.0000, 0.2099,\n",
      "         0.0000, 0.1996, 0.1432, 0.0000, 0.1400, 0.2511, 0.1659, 0.0000, 0.1258,\n",
      "         0.0000, 0.0000, 0.2007, 0.0000, 0.1822, 0.1804],\n",
      "        [0.0000, 0.2959, 0.2283, 0.0000, 0.2246, 0.0000, 0.1285, 0.1870, 0.3345,\n",
      "         0.1966, 0.4067, 0.0000, 0.0000, 0.0000, 0.2587, 0.1224, 0.0000, 0.2148,\n",
      "         0.0000, 0.2020, 0.1449, 0.0000, 0.1348, 0.2566, 0.1688, 0.0000, 0.1280,\n",
      "         0.0000, 0.0000, 0.2014, 0.0000, 0.1824, 0.1853],\n",
      "        [0.0000, 0.2970, 0.2336, 0.0000, 0.2256, 0.0000, 0.1300, 0.1856, 0.3328,\n",
      "         0.1940, 0.4065, 0.0000, 0.0000, 0.0000, 0.2594, 0.1155, 0.0000, 0.2126,\n",
      "         0.0000, 0.2029, 0.1427, 0.0000, 0.1404, 0.2554, 0.1672, 0.0000, 0.1270,\n",
      "         0.0000, 0.0000, 0.2015, 0.0000, 0.1847, 0.1827],\n",
      "        [0.0000, 0.2941, 0.2293, 0.0000, 0.2241, 0.0000, 0.1294, 0.1837, 0.3292,\n",
      "         0.1924, 0.4035, 0.0000, 0.0000, 0.0000, 0.2593, 0.1163, 0.0000, 0.2102,\n",
      "         0.0000, 0.2002, 0.1438, 0.0000, 0.1381, 0.2525, 0.1685, 0.0000, 0.1260,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1828, 0.1820],\n",
      "        [0.0000, 0.2932, 0.2332, 0.0000, 0.2220, 0.0000, 0.1282, 0.1789, 0.3253,\n",
      "         0.1906, 0.3991, 0.0000, 0.0000, 0.0000, 0.2531, 0.1137, 0.0000, 0.2078,\n",
      "         0.0000, 0.1994, 0.1440, 0.0000, 0.1410, 0.2495, 0.1656, 0.0000, 0.1264,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1812, 0.1793],\n",
      "        [0.0000, 0.2915, 0.2286, 0.0000, 0.2214, 0.0000, 0.1283, 0.1797, 0.3249,\n",
      "         0.1917, 0.3978, 0.0000, 0.0000, 0.0000, 0.2570, 0.1163, 0.0000, 0.2093,\n",
      "         0.0000, 0.1956, 0.1454, 0.0000, 0.1388, 0.2477, 0.1663, 0.0000, 0.1269,\n",
      "         0.0000, 0.0000, 0.1991, 0.0000, 0.1820, 0.1802],\n",
      "        [0.0000, 0.2947, 0.2259, 0.0000, 0.2264, 0.0000, 0.1268, 0.1886, 0.3330,\n",
      "         0.1915, 0.4070, 0.0000, 0.0000, 0.0000, 0.2548, 0.1188, 0.0000, 0.2119,\n",
      "         0.0000, 0.2045, 0.1443, 0.0000, 0.1348, 0.2564, 0.1711, 0.0000, 0.1252,\n",
      "         0.0000, 0.0000, 0.1996, 0.0000, 0.1794, 0.1870],\n",
      "        [0.0000, 0.2951, 0.2312, 0.0000, 0.2231, 0.0000, 0.1286, 0.1833, 0.3300,\n",
      "         0.1941, 0.4027, 0.0000, 0.0000, 0.0000, 0.2574, 0.1180, 0.0000, 0.2126,\n",
      "         0.0000, 0.1995, 0.1443, 0.0000, 0.1392, 0.2525, 0.1663, 0.0000, 0.1275,\n",
      "         0.0000, 0.0000, 0.2011, 0.0000, 0.1833, 0.1820]], device='cuda:0')\n",
      "pred = tensor([[0.0000, 0.2941, 0.2324, 0.0000, 0.2239, 0.0000, 0.1295, 0.1802, 0.3252,\n",
      "         0.1882, 0.4007, 0.0000, 0.0000, 0.0000, 0.2566, 0.1125, 0.0000, 0.2078,\n",
      "         0.0000, 0.1995, 0.1420, 0.0000, 0.1427, 0.2486, 0.1657, 0.0000, 0.1246,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1830, 0.1789],\n",
      "        [0.0000, 0.2964, 0.2293, 0.0000, 0.2267, 0.0000, 0.1283, 0.1877, 0.3331,\n",
      "         0.1917, 0.4080, 0.0000, 0.0000, 0.0000, 0.2568, 0.1166, 0.0000, 0.2116,\n",
      "         0.0000, 0.2044, 0.1429, 0.0000, 0.1379, 0.2555, 0.1692, 0.0000, 0.1247,\n",
      "         0.0000, 0.0000, 0.2007, 0.0000, 0.1819, 0.1856],\n",
      "        [0.0000, 0.2937, 0.2295, 0.0000, 0.2225, 0.0000, 0.1291, 0.1807, 0.3272,\n",
      "         0.1925, 0.4006, 0.0000, 0.0000, 0.0000, 0.2581, 0.1173, 0.0000, 0.2107,\n",
      "         0.0000, 0.1975, 0.1452, 0.0000, 0.1384, 0.2498, 0.1666, 0.0000, 0.1280,\n",
      "         0.0000, 0.0000, 0.2005, 0.0000, 0.1825, 0.1806],\n",
      "        [0.0000, 0.2941, 0.2260, 0.0000, 0.2255, 0.0000, 0.1276, 0.1885, 0.3335,\n",
      "         0.1940, 0.4059, 0.0000, 0.0000, 0.0000, 0.2572, 0.1201, 0.0000, 0.2142,\n",
      "         0.0000, 0.2018, 0.1451, 0.0000, 0.1342, 0.2569, 0.1706, 0.0000, 0.1272,\n",
      "         0.0000, 0.0000, 0.1993, 0.0000, 0.1809, 0.1868],\n",
      "        [0.0000, 0.2955, 0.2336, 0.0000, 0.2244, 0.0000, 0.1309, 0.1810, 0.3276,\n",
      "         0.1905, 0.4030, 0.0000, 0.0000, 0.0000, 0.2580, 0.1134, 0.0000, 0.2100,\n",
      "         0.0000, 0.1998, 0.1422, 0.0000, 0.1415, 0.2510, 0.1660, 0.0000, 0.1268,\n",
      "         0.0000, 0.0000, 0.2011, 0.0000, 0.1838, 0.1800],\n",
      "        [0.0000, 0.2951, 0.2293, 0.0000, 0.2232, 0.0000, 0.1279, 0.1840, 0.3310,\n",
      "         0.1938, 0.4028, 0.0000, 0.0000, 0.0000, 0.2571, 0.1180, 0.0000, 0.2129,\n",
      "         0.0000, 0.1995, 0.1462, 0.0000, 0.1374, 0.2538, 0.1673, 0.0000, 0.1282,\n",
      "         0.0000, 0.0000, 0.2004, 0.0000, 0.1817, 0.1833],\n",
      "        [0.0000, 0.2931, 0.2340, 0.0000, 0.2214, 0.0000, 0.1283, 0.1768, 0.3236,\n",
      "         0.1897, 0.3974, 0.0000, 0.0000, 0.0000, 0.2528, 0.1141, 0.0000, 0.2068,\n",
      "         0.0000, 0.1988, 0.1434, 0.0000, 0.1425, 0.2474, 0.1639, 0.0000, 0.1257,\n",
      "         0.0000, 0.0000, 0.2009, 0.0000, 0.1814, 0.1773],\n",
      "        [0.0000, 0.2925, 0.2333, 0.0000, 0.2235, 0.0000, 0.1294, 0.1786, 0.3237,\n",
      "         0.1876, 0.3993, 0.0000, 0.0000, 0.0000, 0.2531, 0.1108, 0.0000, 0.2065,\n",
      "         0.0000, 0.1987, 0.1420, 0.0000, 0.1425, 0.2478, 0.1654, 0.0000, 0.1246,\n",
      "         0.0000, 0.0000, 0.1994, 0.0000, 0.1816, 0.1793],\n",
      "        [0.0000, 0.2972, 0.2317, 0.0000, 0.2245, 0.0000, 0.1296, 0.1831, 0.3323,\n",
      "         0.1942, 0.4058, 0.0000, 0.0000, 0.0000, 0.2568, 0.1186, 0.0000, 0.2126,\n",
      "         0.0000, 0.2019, 0.1449, 0.0000, 0.1373, 0.2547, 0.1667, 0.0000, 0.1285,\n",
      "         0.0000, 0.0000, 0.2021, 0.0000, 0.1818, 0.1832],\n",
      "        [0.0000, 0.2932, 0.2286, 0.0000, 0.2226, 0.0000, 0.1273, 0.1816, 0.3280,\n",
      "         0.1922, 0.4004, 0.0000, 0.0000, 0.0000, 0.2543, 0.1185, 0.0000, 0.2107,\n",
      "         0.0000, 0.1988, 0.1458, 0.0000, 0.1374, 0.2506, 0.1665, 0.0000, 0.1272,\n",
      "         0.0000, 0.0000, 0.2002, 0.0000, 0.1803, 0.1824]], device='cuda:0')\n",
      "pred = tensor([[0.0000, 0.2941, 0.2308, 0.0000, 0.2233, 0.0000, 0.1296, 0.1825, 0.3288,\n",
      "         0.1931, 0.4018, 0.0000, 0.0000, 0.0000, 0.2590, 0.1168, 0.0000, 0.2116,\n",
      "         0.0000, 0.1980, 0.1437, 0.0000, 0.1396, 0.2511, 0.1661, 0.0000, 0.1267,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1838, 0.1813],\n",
      "        [0.0000, 0.2936, 0.2277, 0.0000, 0.2242, 0.0000, 0.1287, 0.1845, 0.3294,\n",
      "         0.1919, 0.4025, 0.0000, 0.0000, 0.0000, 0.2581, 0.1164, 0.0000, 0.2127,\n",
      "         0.0000, 0.1982, 0.1451, 0.0000, 0.1373, 0.2526, 0.1684, 0.0000, 0.1279,\n",
      "         0.0000, 0.0000, 0.1992, 0.0000, 0.1822, 0.1837],\n",
      "        [0.0000, 0.2924, 0.2297, 0.0000, 0.2226, 0.0000, 0.1278, 0.1805, 0.3267,\n",
      "         0.1912, 0.3997, 0.0000, 0.0000, 0.0000, 0.2539, 0.1174, 0.0000, 0.2087,\n",
      "         0.0000, 0.1993, 0.1445, 0.0000, 0.1382, 0.2502, 0.1665, 0.0000, 0.1258,\n",
      "         0.0000, 0.0000, 0.1999, 0.0000, 0.1801, 0.1810],\n",
      "        [0.0000, 0.2920, 0.2328, 0.0000, 0.2221, 0.0000, 0.1278, 0.1781, 0.3241,\n",
      "         0.1890, 0.3976, 0.0000, 0.0000, 0.0000, 0.2520, 0.1148, 0.0000, 0.2056,\n",
      "         0.0000, 0.1998, 0.1426, 0.0000, 0.1418, 0.2479, 0.1644, 0.0000, 0.1232,\n",
      "         0.0000, 0.0000, 0.2003, 0.0000, 0.1803, 0.1782],\n",
      "        [0.0000, 0.2946, 0.2314, 0.0000, 0.2220, 0.0000, 0.1290, 0.1808, 0.3276,\n",
      "         0.1928, 0.4005, 0.0000, 0.0000, 0.0000, 0.2581, 0.1159, 0.0000, 0.2111,\n",
      "         0.0000, 0.1974, 0.1450, 0.0000, 0.1402, 0.2504, 0.1656, 0.0000, 0.1278,\n",
      "         0.0000, 0.0000, 0.2007, 0.0000, 0.1833, 0.1801],\n",
      "        [0.0000, 0.2990, 0.2259, 0.0000, 0.2280, 0.0000, 0.1280, 0.1920, 0.3384,\n",
      "         0.1940, 0.4118, 0.0000, 0.0000, 0.0000, 0.2618, 0.1206, 0.0000, 0.2156,\n",
      "         0.0000, 0.2048, 0.1445, 0.0000, 0.1343, 0.2573, 0.1701, 0.0000, 0.1274,\n",
      "         0.0000, 0.0000, 0.2006, 0.0000, 0.1839, 0.1877],\n",
      "        [0.0000, 0.2963, 0.2309, 0.0000, 0.2256, 0.0000, 0.1302, 0.1868, 0.3332,\n",
      "         0.1939, 0.4069, 0.0000, 0.0000, 0.0000, 0.2591, 0.1156, 0.0000, 0.2145,\n",
      "         0.0000, 0.2005, 0.1436, 0.0000, 0.1379, 0.2563, 0.1685, 0.0000, 0.1279,\n",
      "         0.0000, 0.0000, 0.2001, 0.0000, 0.1838, 0.1853],\n",
      "        [0.0000, 0.2965, 0.2298, 0.0000, 0.2270, 0.0000, 0.1304, 0.1872, 0.3329,\n",
      "         0.1919, 0.4078, 0.0000, 0.0000, 0.0000, 0.2599, 0.1150, 0.0000, 0.2136,\n",
      "         0.0000, 0.2015, 0.1431, 0.0000, 0.1382, 0.2553, 0.1689, 0.0000, 0.1271,\n",
      "         0.0000, 0.0000, 0.2004, 0.0000, 0.1837, 0.1855],\n",
      "        [0.0000, 0.2945, 0.2317, 0.0000, 0.2237, 0.0000, 0.1290, 0.1821, 0.3287,\n",
      "         0.1919, 0.4020, 0.0000, 0.0000, 0.0000, 0.2561, 0.1156, 0.0000, 0.2109,\n",
      "         0.0000, 0.1997, 0.1439, 0.0000, 0.1398, 0.2517, 0.1662, 0.0000, 0.1270,\n",
      "         0.0000, 0.0000, 0.2005, 0.0000, 0.1824, 0.1816],\n",
      "        [0.0000, 0.2957, 0.2289, 0.0000, 0.2257, 0.0000, 0.1283, 0.1869, 0.3338,\n",
      "         0.1943, 0.4070, 0.0000, 0.0000, 0.0000, 0.2562, 0.1199, 0.0000, 0.2137,\n",
      "         0.0000, 0.2035, 0.1447, 0.0000, 0.1352, 0.2568, 0.1694, 0.0000, 0.1280,\n",
      "         0.0000, 0.0000, 0.2008, 0.0000, 0.1810, 0.1859]], device='cuda:0')\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test(test_loader, model, criterion)\n",
    "\n",
    "print(\"Done !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
