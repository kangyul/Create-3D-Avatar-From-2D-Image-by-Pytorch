{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Facial Recognition using PyTorch and OpenCV\n",
    "\n",
    "https://ritik12.medium.com/facial-recognition-using-pytorch-and-opencv-467c4e41d1f\n",
    "\n",
    "\n",
    "Machine Learning - Face Recognition CNN Pytorch.ipynb\n",
    "https://github.com/rubencg195/Pytorch-Tutorials/blob/master/Machine%20Learning%20-%20Face%20Recognition%20CNN%20Pytorch.ipynb\n",
    "\n",
    "\n",
    "\n",
    "Face Recognition Using Pytorch\n",
    "https://github.com/timesler/facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face Landmarks Detection With PyTorch\n",
    "\n",
    "https://towardsdatascience.com/face-landmarks-detection-with-pytorch-4b4852f5e9c4\n",
    "\n",
    "\n",
    "\n",
    "다중입력 deep neural network\n",
    "https://rosenfelder.ai/multi-input-neural-network-pytorch/\n",
    "\n",
    "\n",
    "\n",
    "Understanding dimensions in PyTorch\n",
    "https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습하기 \n",
    "https://github.com/deeplearningzerotoall/PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.init import *\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from FaceFeatureDataset import FaceFeatureDataset\n",
    "import dlib_index\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, F, C]: torch.Size([30, 68, 2])\n",
      "Shape of Tensor y: torch.Size([30, 33]) torch.float32\n",
      "X type torch.float32\n",
      "y type torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader \n",
    "training_data = FaceFeatureDataset(feature_file=\"./outimg/Train/facefeature.csv\", label_file=\"./Dataset/Train/csv/train.csv\")\n",
    "test_data = FaceFeatureDataset(feature_file=\"./outimg/Test/facefeature.csv\", label_file=\"./Dataset/Test/csv/test.csv\")\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 데이터 로드 확인 \n",
    "for X, y in train_loader:\n",
    "    print(f\"Shape of X [N, F, C]: {X.shape}\") # N , Channel, H= width W = height\n",
    "    print(f\"Shape of Tensor y: {y.shape} {y.dtype}\")       \n",
    "    break\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "# print(f'Traing dat length {n_total_steps}')\n",
    "# print(y)\n",
    "print('X type', X.dtype)\n",
    "print('y type', y.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=136, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=33, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 33\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(68 * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "            #nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)        \n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('batch',  batch)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.272626  [    0/  100]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.275498  [    0/  100]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.272230  [    0/  100]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.267463  [    0/  100]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.265215  [    0/  100]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.263594  [    0/  100]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.264567  [    0/  100]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.264601  [    0/  100]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.259139  [    0/  100]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.253483  [    0/  100]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.247438  [    0/  100]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.247878  [    0/  100]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.249509  [    0/  100]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.245293  [    0/  100]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.243359  [    0/  100]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.249737  [    0/  100]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.249707  [    0/  100]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.235237  [    0/  100]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.240251  [    0/  100]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.236808  [    0/  100]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.234239  [    0/  100]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.228278  [    0/  100]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.234498  [    0/  100]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.231152  [    0/  100]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.238976  [    0/  100]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.230102  [    0/  100]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.229282  [    0/  100]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.218113  [    0/  100]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.227583  [    0/  100]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.216126  [    0/  100]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.220039  [    0/  100]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.219874  [    0/  100]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.212439  [    0/  100]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.206021  [    0/  100]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.215054  [    0/  100]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.211899  [    0/  100]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.210050  [    0/  100]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.199587  [    0/  100]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.204394  [    0/  100]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.204604  [    0/  100]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.206476  [    0/  100]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.209785  [    0/  100]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.197927  [    0/  100]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.191195  [    0/  100]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.207223  [    0/  100]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.194270  [    0/  100]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.195181  [    0/  100]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.191233  [    0/  100]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.195982  [    0/  100]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.191078  [    0/  100]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.184965  [    0/  100]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.187381  [    0/  100]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.184991  [    0/  100]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.182541  [    0/  100]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.182189  [    0/  100]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.184238  [    0/  100]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.180169  [    0/  100]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.171941  [    0/  100]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.175600  [    0/  100]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.172292  [    0/  100]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.169534  [    0/  100]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.172531  [    0/  100]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.169945  [    0/  100]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.167139  [    0/  100]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.164685  [    0/  100]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.163205  [    0/  100]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.160532  [    0/  100]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.156055  [    0/  100]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.162263  [    0/  100]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.159540  [    0/  100]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.152827  [    0/  100]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.153662  [    0/  100]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.149040  [    0/  100]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.153938  [    0/  100]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.144396  [    0/  100]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.137525  [    0/  100]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.151920  [    0/  100]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.147637  [    0/  100]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.135739  [    0/  100]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.140194  [    0/  100]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.140134  [    0/  100]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.138819  [    0/  100]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.134349  [    0/  100]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.128478  [    0/  100]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.139397  [    0/  100]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.129751  [    0/  100]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.129094  [    0/  100]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.124591  [    0/  100]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.128159  [    0/  100]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.127943  [    0/  100]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.121044  [    0/  100]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.121065  [    0/  100]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.117017  [    0/  100]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.115394  [    0/  100]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.114236  [    0/  100]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.114285  [    0/  100]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.115822  [    0/  100]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.114174  [    0/  100]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.107510  [    0/  100]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.106858  [    0/  100]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.105600  [    0/  100]\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.103439  [    0/  100]\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.102406  [    0/  100]\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.103110  [    0/  100]\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.106017  [    0/  100]\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.097230  [    0/  100]\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.098670  [    0/  100]\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.096453  [    0/  100]\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.095197  [    0/  100]\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.092533  [    0/  100]\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.084184  [    0/  100]\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.090454  [    0/  100]\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.090296  [    0/  100]\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.085951  [    0/  100]\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.085868  [    0/  100]\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.085289  [    0/  100]\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.080799  [    0/  100]\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.086156  [    0/  100]\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.079746  [    0/  100]\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.079266  [    0/  100]\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.078826  [    0/  100]\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.081055  [    0/  100]\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.076778  [    0/  100]\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.072880  [    0/  100]\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.074140  [    0/  100]\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.073733  [    0/  100]\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.070325  [    0/  100]\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.067890  [    0/  100]\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.067831  [    0/  100]\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.066985  [    0/  100]\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.065219  [    0/  100]\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.062871  [    0/  100]\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.064011  [    0/  100]\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.064122  [    0/  100]\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.061172  [    0/  100]\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.058468  [    0/  100]\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.058974  [    0/  100]\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.055080  [    0/  100]\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.056136  [    0/  100]\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.055826  [    0/  100]\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.056992  [    0/  100]\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.054107  [    0/  100]\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.053735  [    0/  100]\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.053806  [    0/  100]\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.052664  [    0/  100]\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.050536  [    0/  100]\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.048328  [    0/  100]\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.051997  [    0/  100]\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.048123  [    0/  100]\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.048630  [    0/  100]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.049495  [    0/  100]\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.046426  [    0/  100]\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.047864  [    0/  100]\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.044410  [    0/  100]\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.043012  [    0/  100]\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.045962  [    0/  100]\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.043627  [    0/  100]\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.041466  [    0/  100]\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.041856  [    0/  100]\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.040904  [    0/  100]\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.040603  [    0/  100]\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.040656  [    0/  100]\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.039956  [    0/  100]\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.042326  [    0/  100]\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.040982  [    0/  100]\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.039581  [    0/  100]\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.039686  [    0/  100]\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.038598  [    0/  100]\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.038937  [    0/  100]\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.039508  [    0/  100]\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.035908  [    0/  100]\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.037463  [    0/  100]\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.038194  [    0/  100]\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.037154  [    0/  100]\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.039114  [    0/  100]\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.036967  [    0/  100]\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.035364  [    0/  100]\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.037042  [    0/  100]\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.035801  [    0/  100]\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.036809  [    0/  100]\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.035839  [    0/  100]\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.035124  [    0/  100]\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.035387  [    0/  100]\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.034782  [    0/  100]\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.035060  [    0/  100]\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.033802  [    0/  100]\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.034591  [    0/  100]\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.032226  [    0/  100]\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.032481  [    0/  100]\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.033731  [    0/  100]\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.031118  [    0/  100]\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.034130  [    0/  100]\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.035641  [    0/  100]\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.032709  [    0/  100]\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.032984  [    0/  100]\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.033963  [    0/  100]\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.031790  [    0/  100]\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.033904  [    0/  100]\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.031557  [    0/  100]\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.032237  [    0/  100]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.033373  [    0/  100]\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.031795  [    0/  100]\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.033435  [    0/  100]\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.031274  [    0/  100]\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.033009  [    0/  100]\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.032163  [    0/  100]\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.031258  [    0/  100]\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.031157  [    0/  100]\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.030698  [    0/  100]\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.030857  [    0/  100]\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.032927  [    0/  100]\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.030628  [    0/  100]\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.030923  [    0/  100]\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.031585  [    0/  100]\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.032242  [    0/  100]\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.031713  [    0/  100]\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.030337  [    0/  100]\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.031660  [    0/  100]\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.032105  [    0/  100]\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.031401  [    0/  100]\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.031534  [    0/  100]\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.031192  [    0/  100]\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.029935  [    0/  100]\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.030504  [    0/  100]\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.031225  [    0/  100]\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.032002  [    0/  100]\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.029739  [    0/  100]\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.031779  [    0/  100]\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.032074  [    0/  100]\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.030781  [    0/  100]\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.032144  [    0/  100]\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.030254  [    0/  100]\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.031434  [    0/  100]\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.031886  [    0/  100]\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.030515  [    0/  100]\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.030476  [    0/  100]\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.031550  [    0/  100]\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.030575  [    0/  100]\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.032296  [    0/  100]\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.030958  [    0/  100]\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.030652  [    0/  100]\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.030408  [    0/  100]\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.031545  [    0/  100]\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.030396  [    0/  100]\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.031633  [    0/  100]\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.029532  [    0/  100]\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.030147  [    0/  100]\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.031103  [    0/  100]\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.028592  [    0/  100]\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.030709  [    0/  100]\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.030379  [    0/  100]\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.030348  [    0/  100]\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.032133  [    0/  100]\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.031136  [    0/  100]\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.029799  [    0/  100]\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.031785  [    0/  100]\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.030653  [    0/  100]\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.030044  [    0/  100]\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.028866  [    0/  100]\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.031372  [    0/  100]\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.031261  [    0/  100]\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.032638  [    0/  100]\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.031382  [    0/  100]\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.032083  [    0/  100]\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.030517  [    0/  100]\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.030583  [    0/  100]\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.030361  [    0/  100]\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.030570  [    0/  100]\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.031625  [    0/  100]\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.030656  [    0/  100]\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.030448  [    0/  100]\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.030911  [    0/  100]\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.029265  [    0/  100]\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.030267  [    0/  100]\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.029284  [    0/  100]\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.029890  [    0/  100]\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.030502  [    0/  100]\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.031958  [    0/  100]\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.031259  [    0/  100]\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.029772  [    0/  100]\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.030486  [    0/  100]\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.031307  [    0/  100]\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.028961  [    0/  100]\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.030312  [    0/  100]\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.030143  [    0/  100]\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.029661  [    0/  100]\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.030323  [    0/  100]\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.029343  [    0/  100]\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.029316  [    0/  100]\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.029973  [    0/  100]\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.029536  [    0/  100]\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.031174  [    0/  100]\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.029898  [    0/  100]\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.030636  [    0/  100]\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.032039  [    0/  100]\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.029421  [    0/  100]\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.029836  [    0/  100]\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.030753  [    0/  100]\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.031031  [    0/  100]\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.030810  [    0/  100]\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.029737  [    0/  100]\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.031673  [    0/  100]\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.029525  [    0/  100]\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.029874  [    0/  100]\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.029916  [    0/  100]\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.030280  [    0/  100]\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.029933  [    0/  100]\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.029667  [    0/  100]\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.030431  [    0/  100]\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.031622  [    0/  100]\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.031252  [    0/  100]\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.030189  [    0/  100]\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.030520  [    0/  100]\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.028456  [    0/  100]\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.029759  [    0/  100]\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.030402  [    0/  100]\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.029490  [    0/  100]\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.030610  [    0/  100]\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.031868  [    0/  100]\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.030536  [    0/  100]\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.029930  [    0/  100]\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.030860  [    0/  100]\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.030048  [    0/  100]\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.031553  [    0/  100]\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.031511  [    0/  100]\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.030945  [    0/  100]\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.029083  [    0/  100]\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.029942  [    0/  100]\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.029922  [    0/  100]\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.030787  [    0/  100]\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.031000  [    0/  100]\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.031872  [    0/  100]\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.031809  [    0/  100]\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.030156  [    0/  100]\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.030948  [    0/  100]\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.030652  [    0/  100]\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.031151  [    0/  100]\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.030832  [    0/  100]\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.031777  [    0/  100]\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.028585  [    0/  100]\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.031486  [    0/  100]\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.030657  [    0/  100]\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.031375  [    0/  100]\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.030614  [    0/  100]\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.031544  [    0/  100]\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.029712  [    0/  100]\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.029573  [    0/  100]\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.029733  [    0/  100]\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.030462  [    0/  100]\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.029783  [    0/  100]\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.031504  [    0/  100]\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.031122  [    0/  100]\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.030139  [    0/  100]\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.029965  [    0/  100]\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.030521  [    0/  100]\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.029915  [    0/  100]\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.029539  [    0/  100]\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.029829  [    0/  100]\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.030347  [    0/  100]\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.030456  [    0/  100]\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.029856  [    0/  100]\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.029988  [    0/  100]\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.030174  [    0/  100]\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.029076  [    0/  100]\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.030286  [    0/  100]\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.030517  [    0/  100]\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.030326  [    0/  100]\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.030143  [    0/  100]\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.031147  [    0/  100]\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.029373  [    0/  100]\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.031938  [    0/  100]\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.030630  [    0/  100]\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.029480  [    0/  100]\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.032404  [    0/  100]\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.030839  [    0/  100]\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.031031  [    0/  100]\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.030815  [    0/  100]\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.030337  [    0/  100]\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.030887  [    0/  100]\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.031513  [    0/  100]\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.029958  [    0/  100]\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.030420  [    0/  100]\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.029837  [    0/  100]\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.030719  [    0/  100]\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.030742  [    0/  100]\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.030576  [    0/  100]\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.030899  [    0/  100]\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.028444  [    0/  100]\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.031643  [    0/  100]\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.029979  [    0/  100]\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.031117  [    0/  100]\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.031759  [    0/  100]\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.030395  [    0/  100]\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.029987  [    0/  100]\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.029850  [    0/  100]\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.031175  [    0/  100]\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.029488  [    0/  100]\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.030992  [    0/  100]\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.030067  [    0/  100]\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.030759  [    0/  100]\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.031281  [    0/  100]\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.031649  [    0/  100]\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.029794  [    0/  100]\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.030985  [    0/  100]\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.029907  [    0/  100]\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.029721  [    0/  100]\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.031000  [    0/  100]\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.030717  [    0/  100]\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.029430  [    0/  100]\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.030478  [    0/  100]\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.029480  [    0/  100]\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.029759  [    0/  100]\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.030274  [    0/  100]\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.029753  [    0/  100]\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.031760  [    0/  100]\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.030139  [    0/  100]\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.029545  [    0/  100]\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.031426  [    0/  100]\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.030838  [    0/  100]\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.030611  [    0/  100]\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.030635  [    0/  100]\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.030283  [    0/  100]\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.029745  [    0/  100]\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.031274  [    0/  100]\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.030749  [    0/  100]\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.031646  [    0/  100]\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.030840  [    0/  100]\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.031080  [    0/  100]\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.029926  [    0/  100]\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.032083  [    0/  100]\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.030712  [    0/  100]\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.029616  [    0/  100]\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.030523  [    0/  100]\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.031597  [    0/  100]\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.031595  [    0/  100]\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.030337  [    0/  100]\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.029772  [    0/  100]\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.031012  [    0/  100]\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.029243  [    0/  100]\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.030462  [    0/  100]\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.032000  [    0/  100]\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.030042  [    0/  100]\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.029881  [    0/  100]\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.029848  [    0/  100]\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.028833  [    0/  100]\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.030712  [    0/  100]\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.029656  [    0/  100]\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.031988  [    0/  100]\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.029571  [    0/  100]\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.029086  [    0/  100]\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.029805  [    0/  100]\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.030990  [    0/  100]\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.030026  [    0/  100]\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.029503  [    0/  100]\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.029351  [    0/  100]\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.029143  [    0/  100]\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.030655  [    0/  100]\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.030528  [    0/  100]\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.031293  [    0/  100]\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.029645  [    0/  100]\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.031354  [    0/  100]\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.029071  [    0/  100]\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.031316  [    0/  100]\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.030505  [    0/  100]\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.031619  [    0/  100]\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.029924  [    0/  100]\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.030146  [    0/  100]\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.030942  [    0/  100]\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.030626  [    0/  100]\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.031968  [    0/  100]\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.031590  [    0/  100]\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.030437  [    0/  100]\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.031191  [    0/  100]\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.032207  [    0/  100]\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.029656  [    0/  100]\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.030746  [    0/  100]\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.030725  [    0/  100]\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.031168  [    0/  100]\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.030556  [    0/  100]\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.030660  [    0/  100]\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.029275  [    0/  100]\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.031347  [    0/  100]\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.030129  [    0/  100]\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.030964  [    0/  100]\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.029746  [    0/  100]\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.029928  [    0/  100]\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.028635  [    0/  100]\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.029667  [    0/  100]\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.031957  [    0/  100]\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.029227  [    0/  100]\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.029858  [    0/  100]\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.030924  [    0/  100]\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.029974  [    0/  100]\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.032491  [    0/  100]\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.029526  [    0/  100]\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.031413  [    0/  100]\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.030909  [    0/  100]\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.030106  [    0/  100]\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.030773  [    0/  100]\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.029929  [    0/  100]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, criterion, optimizer)    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    print('test size', size )\n",
    "    # num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            print(\"X Shape\", X.shape)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)            \n",
    "            print('pred =', pred)\n",
    "            #print('loss', loss)\n",
    "            #print('real', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size 100\n",
      "X Shape torch.Size([30, 68, 2])\n",
      "pred = tensor([[0.4941, 0.4877, 0.4989, 0.5138, 0.5229, 0.5231, 0.4762, 0.5017, 0.5232,\n",
      "         0.5143, 0.5257, 0.5050, 0.4974, 0.5171, 0.4836, 0.4960, 0.5171, 0.5363,\n",
      "         0.4978, 0.5257, 0.5018, 0.5084, 0.5014, 0.4994, 0.5232, 0.4900, 0.5280,\n",
      "         0.4912, 0.4849, 0.5077, 0.5306, 0.5226, 0.4856],\n",
      "        [0.4928, 0.4861, 0.4959, 0.5126, 0.5210, 0.5211, 0.4745, 0.4998, 0.5213,\n",
      "         0.5128, 0.5228, 0.5011, 0.4950, 0.5148, 0.4828, 0.4933, 0.5147, 0.5347,\n",
      "         0.4956, 0.5224, 0.4982, 0.5057, 0.4990, 0.4986, 0.5211, 0.4881, 0.5258,\n",
      "         0.4901, 0.4831, 0.5069, 0.5250, 0.5197, 0.4841],\n",
      "        [0.4943, 0.4875, 0.4981, 0.5138, 0.5231, 0.5247, 0.4763, 0.5029, 0.5227,\n",
      "         0.5149, 0.5257, 0.5032, 0.4993, 0.5166, 0.4834, 0.4955, 0.5173, 0.5366,\n",
      "         0.4979, 0.5254, 0.5002, 0.5082, 0.5016, 0.4994, 0.5226, 0.4907, 0.5288,\n",
      "         0.4904, 0.4851, 0.5082, 0.5291, 0.5217, 0.4847],\n",
      "        [0.4962, 0.4893, 0.5015, 0.5160, 0.5259, 0.5243, 0.4783, 0.5037, 0.5254,\n",
      "         0.5171, 0.5284, 0.5089, 0.4998, 0.5200, 0.4858, 0.4988, 0.5209, 0.5373,\n",
      "         0.5013, 0.5289, 0.5048, 0.5117, 0.5051, 0.5018, 0.5272, 0.4916, 0.5298,\n",
      "         0.4940, 0.4871, 0.5098, 0.5339, 0.5247, 0.4867],\n",
      "        [0.4933, 0.4870, 0.4968, 0.5123, 0.5211, 0.5221, 0.4756, 0.4996, 0.5216,\n",
      "         0.5124, 0.5230, 0.5030, 0.4950, 0.5154, 0.4828, 0.4940, 0.5144, 0.5352,\n",
      "         0.4961, 0.5234, 0.4992, 0.5050, 0.4995, 0.4981, 0.5209, 0.4890, 0.5263,\n",
      "         0.4894, 0.4829, 0.5071, 0.5262, 0.5200, 0.4857],\n",
      "        [0.4812, 0.4763, 0.4836, 0.5009, 0.5075, 0.5091, 0.4666, 0.4890, 0.5112,\n",
      "         0.5024, 0.5077, 0.4889, 0.4838, 0.5050, 0.4736, 0.4805, 0.5023, 0.5256,\n",
      "         0.4854, 0.5092, 0.4889, 0.4932, 0.4873, 0.4898, 0.5073, 0.4787, 0.5159,\n",
      "         0.4776, 0.4722, 0.4959, 0.5113, 0.5075, 0.4761],\n",
      "        [0.4912, 0.4848, 0.4938, 0.5104, 0.5186, 0.5197, 0.4739, 0.4981, 0.5194,\n",
      "         0.5110, 0.5201, 0.4992, 0.4934, 0.5133, 0.4810, 0.4911, 0.5125, 0.5335,\n",
      "         0.4942, 0.5206, 0.4968, 0.5034, 0.4973, 0.4967, 0.5186, 0.4870, 0.5242,\n",
      "         0.4871, 0.4810, 0.5050, 0.5230, 0.5171, 0.4826],\n",
      "        [0.4769, 0.4715, 0.4797, 0.4965, 0.5044, 0.5054, 0.4626, 0.4869, 0.5073,\n",
      "         0.4990, 0.5042, 0.4852, 0.4817, 0.5023, 0.4704, 0.4767, 0.4992, 0.5209,\n",
      "         0.4826, 0.5052, 0.4854, 0.4899, 0.4834, 0.4875, 0.5029, 0.4769, 0.5129,\n",
      "         0.4737, 0.4689, 0.4934, 0.5075, 0.5034, 0.4725],\n",
      "        [0.4910, 0.4846, 0.4939, 0.5101, 0.5179, 0.5180, 0.4731, 0.4968, 0.5186,\n",
      "         0.5100, 0.5193, 0.4991, 0.4916, 0.5129, 0.4813, 0.4899, 0.5115, 0.5334,\n",
      "         0.4934, 0.5194, 0.4971, 0.5027, 0.4960, 0.4962, 0.5180, 0.4856, 0.5233,\n",
      "         0.4878, 0.4806, 0.5051, 0.5221, 0.5165, 0.4835],\n",
      "        [0.4814, 0.4763, 0.4840, 0.5005, 0.5081, 0.5105, 0.4677, 0.4894, 0.5117,\n",
      "         0.5023, 0.5083, 0.4900, 0.4851, 0.5054, 0.4734, 0.4815, 0.5033, 0.5252,\n",
      "         0.4863, 0.5102, 0.4892, 0.4935, 0.4884, 0.4900, 0.5072, 0.4802, 0.5167,\n",
      "         0.4765, 0.4720, 0.4960, 0.5126, 0.5078, 0.4762],\n",
      "        [0.4893, 0.4821, 0.4918, 0.5085, 0.5174, 0.5172, 0.4721, 0.4967, 0.5173,\n",
      "         0.5093, 0.5181, 0.4978, 0.4918, 0.5119, 0.4801, 0.4890, 0.5114, 0.5310,\n",
      "         0.4931, 0.5184, 0.4957, 0.5021, 0.4959, 0.4961, 0.5171, 0.4858, 0.5227,\n",
      "         0.4859, 0.4791, 0.5044, 0.5207, 0.5148, 0.4815],\n",
      "        [0.4884, 0.4824, 0.4917, 0.5080, 0.5159, 0.5167, 0.4722, 0.4959, 0.5178,\n",
      "         0.5093, 0.5173, 0.4976, 0.4913, 0.5117, 0.4793, 0.4888, 0.5107, 0.5313,\n",
      "         0.4924, 0.5181, 0.4960, 0.5018, 0.4952, 0.4952, 0.5163, 0.4848, 0.5224,\n",
      "         0.4852, 0.4789, 0.5025, 0.5212, 0.5155, 0.4812],\n",
      "        [0.4866, 0.4809, 0.4892, 0.5059, 0.5136, 0.5151, 0.4705, 0.4942, 0.5156,\n",
      "         0.5068, 0.5148, 0.4943, 0.4897, 0.5095, 0.4777, 0.4861, 0.5079, 0.5295,\n",
      "         0.4903, 0.5155, 0.4931, 0.4986, 0.4924, 0.4933, 0.5129, 0.4837, 0.5208,\n",
      "         0.4825, 0.4771, 0.5016, 0.5177, 0.5128, 0.4799],\n",
      "        [0.4910, 0.4842, 0.4939, 0.5100, 0.5189, 0.5194, 0.4734, 0.4983, 0.5192,\n",
      "         0.5107, 0.5205, 0.4989, 0.4942, 0.5131, 0.4810, 0.4908, 0.5130, 0.5328,\n",
      "         0.4943, 0.5205, 0.4973, 0.5040, 0.4968, 0.4964, 0.5183, 0.4872, 0.5249,\n",
      "         0.4876, 0.4807, 0.5061, 0.5232, 0.5169, 0.4828],\n",
      "        [0.4895, 0.4839, 0.4928, 0.5086, 0.5168, 0.5176, 0.4726, 0.4962, 0.5181,\n",
      "         0.5092, 0.5177, 0.4990, 0.4911, 0.5123, 0.4801, 0.4898, 0.5110, 0.5327,\n",
      "         0.4931, 0.5192, 0.4967, 0.5018, 0.4959, 0.4955, 0.5172, 0.4851, 0.5229,\n",
      "         0.4861, 0.4792, 0.5036, 0.5219, 0.5158, 0.4826],\n",
      "        [0.4736, 0.4682, 0.4750, 0.4930, 0.5002, 0.5004, 0.4606, 0.4824, 0.5038,\n",
      "         0.4958, 0.4984, 0.4806, 0.4770, 0.4994, 0.4679, 0.4719, 0.4955, 0.5173,\n",
      "         0.4792, 0.5008, 0.4822, 0.4859, 0.4797, 0.4853, 0.4991, 0.4735, 0.5091,\n",
      "         0.4702, 0.4650, 0.4902, 0.5018, 0.4985, 0.4697],\n",
      "        [0.4878, 0.4818, 0.4913, 0.5070, 0.5158, 0.5172, 0.4724, 0.4960, 0.5177,\n",
      "         0.5087, 0.5171, 0.4981, 0.4920, 0.5118, 0.4784, 0.4893, 0.5113, 0.5303,\n",
      "         0.4928, 0.5187, 0.4958, 0.5017, 0.4958, 0.4949, 0.5156, 0.4859, 0.5228,\n",
      "         0.4836, 0.4780, 0.5017, 0.5223, 0.5151, 0.4805],\n",
      "        [0.4818, 0.4756, 0.4844, 0.5013, 0.5095, 0.5106, 0.4668, 0.4911, 0.5120,\n",
      "         0.5040, 0.5095, 0.4894, 0.4866, 0.5063, 0.4740, 0.4819, 0.5044, 0.5252,\n",
      "         0.4869, 0.5107, 0.4896, 0.4953, 0.4886, 0.4908, 0.5084, 0.4805, 0.5174,\n",
      "         0.4782, 0.4730, 0.4974, 0.5129, 0.5083, 0.4754],\n",
      "        [0.4931, 0.4864, 0.4974, 0.5129, 0.5217, 0.5224, 0.4758, 0.5011, 0.5225,\n",
      "         0.5145, 0.5240, 0.5040, 0.4967, 0.5163, 0.4823, 0.4954, 0.5166, 0.5353,\n",
      "         0.4970, 0.5242, 0.5003, 0.5079, 0.5010, 0.4990, 0.5223, 0.4894, 0.5272,\n",
      "         0.4900, 0.4838, 0.5061, 0.5286, 0.5217, 0.4842],\n",
      "        [0.4978, 0.4910, 0.5023, 0.5169, 0.5275, 0.5271, 0.4798, 0.5048, 0.5268,\n",
      "         0.5179, 0.5299, 0.5104, 0.5009, 0.5204, 0.4864, 0.5009, 0.5224, 0.5390,\n",
      "         0.5020, 0.5309, 0.5050, 0.5126, 0.5068, 0.5027, 0.5287, 0.4939, 0.5317,\n",
      "         0.4949, 0.4872, 0.5113, 0.5354, 0.5262, 0.4879],\n",
      "        [0.4981, 0.4911, 0.5022, 0.5174, 0.5276, 0.5280, 0.4808, 0.5050, 0.5274,\n",
      "         0.5185, 0.5297, 0.5091, 0.5024, 0.5204, 0.4861, 0.5009, 0.5226, 0.5391,\n",
      "         0.5023, 0.5309, 0.5044, 0.5131, 0.5073, 0.5026, 0.5280, 0.4942, 0.5322,\n",
      "         0.4937, 0.4877, 0.5104, 0.5355, 0.5259, 0.4871],\n",
      "        [0.4792, 0.4732, 0.4808, 0.4978, 0.5066, 0.5070, 0.4655, 0.4868, 0.5099,\n",
      "         0.5006, 0.5056, 0.4881, 0.4824, 0.5041, 0.4721, 0.4791, 0.5025, 0.5217,\n",
      "         0.4849, 0.5083, 0.4881, 0.4923, 0.4866, 0.4893, 0.5059, 0.4788, 0.5143,\n",
      "         0.4749, 0.4689, 0.4942, 0.5105, 0.5055, 0.4736],\n",
      "        [0.4834, 0.4779, 0.4869, 0.5032, 0.5111, 0.5116, 0.4684, 0.4921, 0.5132,\n",
      "         0.5049, 0.5119, 0.4934, 0.4869, 0.5084, 0.4753, 0.4838, 0.5064, 0.5263,\n",
      "         0.4888, 0.5128, 0.4928, 0.4974, 0.4908, 0.4925, 0.5115, 0.4819, 0.5180,\n",
      "         0.4806, 0.4753, 0.4983, 0.5167, 0.5101, 0.4777],\n",
      "        [0.4808, 0.4748, 0.4825, 0.5000, 0.5081, 0.5095, 0.4669, 0.4895, 0.5112,\n",
      "         0.5026, 0.5077, 0.4883, 0.4853, 0.5055, 0.4731, 0.4809, 0.5040, 0.5236,\n",
      "         0.4864, 0.5100, 0.4884, 0.4939, 0.4882, 0.4904, 0.5074, 0.4802, 0.5164,\n",
      "         0.4760, 0.4710, 0.4959, 0.5114, 0.5066, 0.4742],\n",
      "        [0.4948, 0.4885, 0.4971, 0.5133, 0.5219, 0.5228, 0.4771, 0.5001, 0.5226,\n",
      "         0.5134, 0.5231, 0.5029, 0.4955, 0.5157, 0.4834, 0.4950, 0.5155, 0.5365,\n",
      "         0.4969, 0.5246, 0.4994, 0.5063, 0.5010, 0.4988, 0.5224, 0.4895, 0.5270,\n",
      "         0.4903, 0.4831, 0.5077, 0.5263, 0.5203, 0.4859],\n",
      "        [0.4815, 0.4754, 0.4844, 0.5010, 0.5093, 0.5099, 0.4673, 0.4905, 0.5122,\n",
      "         0.5035, 0.5095, 0.4899, 0.4863, 0.5066, 0.4734, 0.4820, 0.5049, 0.5242,\n",
      "         0.4868, 0.5111, 0.4900, 0.4954, 0.4888, 0.4909, 0.5084, 0.4807, 0.5172,\n",
      "         0.4778, 0.4728, 0.4966, 0.5140, 0.5084, 0.4752],\n",
      "        [0.4892, 0.4831, 0.4921, 0.5085, 0.5163, 0.5175, 0.4732, 0.4957, 0.5180,\n",
      "         0.5095, 0.5168, 0.4983, 0.4909, 0.5118, 0.4793, 0.4893, 0.5102, 0.5320,\n",
      "         0.4926, 0.5182, 0.4956, 0.5012, 0.4959, 0.4954, 0.5166, 0.4854, 0.5225,\n",
      "         0.4851, 0.4791, 0.5029, 0.5206, 0.5154, 0.4821],\n",
      "        [0.4772, 0.4715, 0.4787, 0.4965, 0.5035, 0.5032, 0.4637, 0.4846, 0.5075,\n",
      "         0.4991, 0.5022, 0.4850, 0.4789, 0.5024, 0.4705, 0.4760, 0.4992, 0.5203,\n",
      "         0.4825, 0.5045, 0.4861, 0.4900, 0.4837, 0.4879, 0.5037, 0.4759, 0.5115,\n",
      "         0.4738, 0.4676, 0.4928, 0.5061, 0.5023, 0.4725],\n",
      "        [0.4890, 0.4819, 0.4927, 0.5087, 0.5178, 0.5174, 0.4723, 0.4979, 0.5184,\n",
      "         0.5108, 0.5194, 0.4988, 0.4934, 0.5135, 0.4798, 0.4903, 0.5132, 0.5311,\n",
      "         0.4942, 0.5194, 0.4972, 0.5042, 0.4969, 0.4967, 0.5181, 0.4864, 0.5236,\n",
      "         0.4860, 0.4800, 0.5036, 0.5236, 0.5160, 0.4806],\n",
      "        [0.4903, 0.4835, 0.4942, 0.5106, 0.5192, 0.5185, 0.4728, 0.4989, 0.5195,\n",
      "         0.5120, 0.5208, 0.5004, 0.4936, 0.5146, 0.4809, 0.4919, 0.5142, 0.5325,\n",
      "         0.4949, 0.5206, 0.4980, 0.5052, 0.4981, 0.4982, 0.5200, 0.4871, 0.5242,\n",
      "         0.4881, 0.4816, 0.5046, 0.5243, 0.5176, 0.4819]], device='cuda:0')\n",
      "X Shape torch.Size([30, 68, 2])\n",
      "pred = tensor([[0.4874, 0.4811, 0.4902, 0.5066, 0.5146, 0.5152, 0.4720, 0.4941, 0.5165,\n",
      "         0.5079, 0.5151, 0.4973, 0.4891, 0.5110, 0.4780, 0.4876, 0.5096, 0.5299,\n",
      "         0.4917, 0.5169, 0.4948, 0.5001, 0.4948, 0.4948, 0.5152, 0.4847, 0.5210,\n",
      "         0.4832, 0.4771, 0.5017, 0.5196, 0.5135, 0.4805],\n",
      "        [0.4928, 0.4858, 0.4966, 0.5122, 0.5215, 0.5205, 0.4753, 0.4997, 0.5220,\n",
      "         0.5136, 0.5227, 0.5035, 0.4950, 0.5159, 0.4826, 0.4947, 0.5168, 0.5344,\n",
      "         0.4972, 0.5236, 0.5001, 0.5075, 0.5008, 0.4994, 0.5226, 0.4884, 0.5262,\n",
      "         0.4901, 0.4825, 0.5065, 0.5274, 0.5204, 0.4837],\n",
      "        [0.4899, 0.4828, 0.4930, 0.5084, 0.5184, 0.5181, 0.4733, 0.4975, 0.5193,\n",
      "         0.5106, 0.5194, 0.5011, 0.4931, 0.5141, 0.4802, 0.4919, 0.5143, 0.5307,\n",
      "         0.4954, 0.5210, 0.4976, 0.5042, 0.4984, 0.4972, 0.5190, 0.4880, 0.5238,\n",
      "         0.4857, 0.4789, 0.5042, 0.5246, 0.5164, 0.4815],\n",
      "        [0.5045, 0.4983, 0.5100, 0.5241, 0.5344, 0.5342, 0.4846, 0.5112, 0.5328,\n",
      "         0.5232, 0.5380, 0.5168, 0.5073, 0.5256, 0.4923, 0.5074, 0.5270, 0.5462,\n",
      "         0.5067, 0.5380, 0.5106, 0.5183, 0.5123, 0.5063, 0.5358, 0.4981, 0.5375,\n",
      "         0.5022, 0.4943, 0.5163, 0.5418, 0.5329, 0.4939],\n",
      "        [0.4892, 0.4826, 0.4902, 0.5077, 0.5160, 0.5168, 0.4727, 0.4949, 0.5178,\n",
      "         0.5085, 0.5162, 0.4964, 0.4902, 0.5108, 0.4795, 0.4889, 0.5103, 0.5311,\n",
      "         0.4920, 0.5179, 0.4936, 0.5003, 0.4952, 0.4954, 0.5158, 0.4855, 0.5221,\n",
      "         0.4843, 0.4774, 0.5030, 0.5186, 0.5143, 0.4812],\n",
      "        [0.4902, 0.4840, 0.4938, 0.5094, 0.5183, 0.5195, 0.4738, 0.4977, 0.5196,\n",
      "         0.5106, 0.5195, 0.5002, 0.4934, 0.5131, 0.4804, 0.4917, 0.5124, 0.5327,\n",
      "         0.4939, 0.5208, 0.4969, 0.5031, 0.4974, 0.4963, 0.5179, 0.4871, 0.5244,\n",
      "         0.4861, 0.4803, 0.5041, 0.5237, 0.5178, 0.4826],\n",
      "        [0.4924, 0.4861, 0.4952, 0.5114, 0.5194, 0.5207, 0.4747, 0.4987, 0.5201,\n",
      "         0.5120, 0.5214, 0.5006, 0.4939, 0.5144, 0.4816, 0.4920, 0.5130, 0.5349,\n",
      "         0.4946, 0.5218, 0.4979, 0.5040, 0.4980, 0.4970, 0.5192, 0.4876, 0.5251,\n",
      "         0.4882, 0.4819, 0.5061, 0.5245, 0.5184, 0.4842],\n",
      "        [0.4902, 0.4842, 0.4938, 0.5094, 0.5180, 0.5182, 0.4732, 0.4971, 0.5189,\n",
      "         0.5098, 0.5191, 0.4998, 0.4921, 0.5129, 0.4809, 0.4908, 0.5121, 0.5327,\n",
      "         0.4940, 0.5202, 0.4973, 0.5027, 0.4968, 0.4963, 0.5181, 0.4859, 0.5236,\n",
      "         0.4867, 0.4800, 0.5045, 0.5229, 0.5166, 0.4827],\n",
      "        [0.4937, 0.4866, 0.4976, 0.5137, 0.5222, 0.5217, 0.4752, 0.5013, 0.5217,\n",
      "         0.5142, 0.5245, 0.5027, 0.4966, 0.5164, 0.4835, 0.4943, 0.5164, 0.5355,\n",
      "         0.4969, 0.5241, 0.5003, 0.5077, 0.5003, 0.4993, 0.5226, 0.4890, 0.5270,\n",
      "         0.4914, 0.4842, 0.5082, 0.5276, 0.5206, 0.4844],\n",
      "        [0.5001, 0.4921, 0.5048, 0.5199, 0.5301, 0.5286, 0.4811, 0.5073, 0.5289,\n",
      "         0.5210, 0.5328, 0.5117, 0.5042, 0.5227, 0.4883, 0.5029, 0.5250, 0.5403,\n",
      "         0.5048, 0.5327, 0.5071, 0.5164, 0.5094, 0.5050, 0.5318, 0.4949, 0.5335,\n",
      "         0.4977, 0.4900, 0.5131, 0.5378, 0.5278, 0.4886],\n",
      "        [0.4966, 0.4903, 0.5008, 0.5160, 0.5254, 0.5259, 0.4786, 0.5036, 0.5252,\n",
      "         0.5159, 0.5280, 0.5073, 0.5000, 0.5186, 0.4857, 0.4983, 0.5192, 0.5386,\n",
      "         0.4997, 0.5285, 0.5032, 0.5102, 0.5040, 0.5009, 0.5262, 0.4922, 0.5303,\n",
      "         0.4935, 0.4867, 0.5099, 0.5321, 0.5242, 0.4876],\n",
      "        [0.4955, 0.4888, 0.4987, 0.5150, 0.5230, 0.5227, 0.4765, 0.5014, 0.5226,\n",
      "         0.5146, 0.5252, 0.5041, 0.4964, 0.5170, 0.4850, 0.4948, 0.5161, 0.5374,\n",
      "         0.4976, 0.5245, 0.5009, 0.5074, 0.5009, 0.4995, 0.5234, 0.4893, 0.5274,\n",
      "         0.4926, 0.4853, 0.5091, 0.5272, 0.5212, 0.4869],\n",
      "        [0.4924, 0.4855, 0.4969, 0.5121, 0.5215, 0.5208, 0.4751, 0.5005, 0.5222,\n",
      "         0.5141, 0.5233, 0.5042, 0.4959, 0.5165, 0.4819, 0.4951, 0.5168, 0.5344,\n",
      "         0.4971, 0.5239, 0.5006, 0.5082, 0.5010, 0.4992, 0.5228, 0.4891, 0.5263,\n",
      "         0.4902, 0.4832, 0.5060, 0.5287, 0.5210, 0.4834],\n",
      "        [0.5033, 0.4965, 0.5075, 0.5222, 0.5331, 0.5331, 0.4841, 0.5089, 0.5319,\n",
      "         0.5215, 0.5362, 0.5144, 0.5071, 0.5240, 0.4912, 0.5059, 0.5263, 0.5446,\n",
      "         0.5057, 0.5371, 0.5090, 0.5166, 0.5111, 0.5052, 0.5336, 0.4972, 0.5366,\n",
      "         0.4994, 0.4919, 0.5152, 0.5404, 0.5317, 0.4919],\n",
      "        [0.4959, 0.4894, 0.4996, 0.5153, 0.5244, 0.5246, 0.4788, 0.5024, 0.5248,\n",
      "         0.5154, 0.5264, 0.5067, 0.4986, 0.5181, 0.4848, 0.4975, 0.5185, 0.5373,\n",
      "         0.4992, 0.5275, 0.5022, 0.5092, 0.5037, 0.5005, 0.5252, 0.4917, 0.5293,\n",
      "         0.4923, 0.4856, 0.5087, 0.5306, 0.5232, 0.4868],\n",
      "        [0.4857, 0.4796, 0.4876, 0.5044, 0.5124, 0.5142, 0.4698, 0.4926, 0.5143,\n",
      "         0.5055, 0.5131, 0.4925, 0.4885, 0.5081, 0.4768, 0.4843, 0.5062, 0.5284,\n",
      "         0.4889, 0.5140, 0.4922, 0.4969, 0.4908, 0.4920, 0.5109, 0.4825, 0.5195,\n",
      "         0.4809, 0.4755, 0.5003, 0.5159, 0.5113, 0.4789],\n",
      "        [0.4932, 0.4866, 0.4966, 0.5131, 0.5216, 0.5226, 0.4759, 0.5007, 0.5221,\n",
      "         0.5137, 0.5235, 0.5026, 0.4962, 0.5151, 0.4826, 0.4946, 0.5154, 0.5353,\n",
      "         0.4963, 0.5236, 0.4987, 0.5063, 0.5006, 0.4989, 0.5217, 0.4897, 0.5270,\n",
      "         0.4897, 0.4837, 0.5070, 0.5265, 0.5209, 0.4845],\n",
      "        [0.4813, 0.4749, 0.4834, 0.5000, 0.5091, 0.5085, 0.4663, 0.4894, 0.5107,\n",
      "         0.5024, 0.5082, 0.4899, 0.4843, 0.5060, 0.4741, 0.4809, 0.5043, 0.5237,\n",
      "         0.4869, 0.5101, 0.4894, 0.4942, 0.4881, 0.4908, 0.5085, 0.4798, 0.5158,\n",
      "         0.4776, 0.4714, 0.4974, 0.5118, 0.5064, 0.4752],\n",
      "        [0.4853, 0.4788, 0.4876, 0.5048, 0.5131, 0.5138, 0.4700, 0.4937, 0.5151,\n",
      "         0.5067, 0.5137, 0.4937, 0.4890, 0.5093, 0.4764, 0.4858, 0.5083, 0.5276,\n",
      "         0.4901, 0.5147, 0.4917, 0.4985, 0.4928, 0.4939, 0.5127, 0.4840, 0.5201,\n",
      "         0.4813, 0.4758, 0.5002, 0.5169, 0.5116, 0.4781],\n",
      "        [0.4841, 0.4783, 0.4871, 0.5035, 0.5116, 0.5114, 0.4687, 0.4919, 0.5139,\n",
      "         0.5055, 0.5121, 0.4934, 0.4870, 0.5086, 0.4761, 0.4843, 0.5071, 0.5272,\n",
      "         0.4890, 0.5136, 0.4932, 0.4980, 0.4911, 0.4925, 0.5120, 0.4811, 0.5185,\n",
      "         0.4813, 0.4748, 0.4987, 0.5169, 0.5110, 0.4775],\n",
      "        [0.4894, 0.4824, 0.4924, 0.5084, 0.5177, 0.5182, 0.4725, 0.4972, 0.5180,\n",
      "         0.5099, 0.5186, 0.4983, 0.4926, 0.5124, 0.4801, 0.4899, 0.5121, 0.5314,\n",
      "         0.4937, 0.5195, 0.4964, 0.5026, 0.4964, 0.4962, 0.5173, 0.4862, 0.5233,\n",
      "         0.4855, 0.4791, 0.5042, 0.5220, 0.5156, 0.4810],\n",
      "        [0.4812, 0.4759, 0.4840, 0.5009, 0.5088, 0.5094, 0.4661, 0.4900, 0.5112,\n",
      "         0.5031, 0.5084, 0.4902, 0.4847, 0.5060, 0.4739, 0.4817, 0.5041, 0.5251,\n",
      "         0.4863, 0.5102, 0.4890, 0.4942, 0.4883, 0.4909, 0.5087, 0.4796, 0.5164,\n",
      "         0.4784, 0.4721, 0.4971, 0.5118, 0.5078, 0.4758],\n",
      "        [0.4790, 0.4737, 0.4811, 0.4987, 0.5056, 0.5070, 0.4650, 0.4875, 0.5093,\n",
      "         0.5007, 0.5054, 0.4860, 0.4829, 0.5033, 0.4717, 0.4781, 0.5009, 0.5231,\n",
      "         0.4836, 0.5068, 0.4870, 0.4918, 0.4851, 0.4882, 0.5046, 0.4775, 0.5145,\n",
      "         0.4752, 0.4702, 0.4941, 0.5089, 0.5052, 0.4736],\n",
      "        [0.4820, 0.4767, 0.4846, 0.5016, 0.5088, 0.5100, 0.4675, 0.4898, 0.5114,\n",
      "         0.5032, 0.5086, 0.4908, 0.4845, 0.5063, 0.4741, 0.4815, 0.5034, 0.5257,\n",
      "         0.4866, 0.5103, 0.4901, 0.4941, 0.4887, 0.4910, 0.5087, 0.4801, 0.5162,\n",
      "         0.4784, 0.4732, 0.4971, 0.5126, 0.5079, 0.4771],\n",
      "        [0.4999, 0.4925, 0.5030, 0.5191, 0.5290, 0.5292, 0.4811, 0.5057, 0.5279,\n",
      "         0.5188, 0.5314, 0.5096, 0.5032, 0.5208, 0.4880, 0.5015, 0.5225, 0.5411,\n",
      "         0.5022, 0.5321, 0.5049, 0.5131, 0.5071, 0.5030, 0.5293, 0.4947, 0.5333,\n",
      "         0.4960, 0.4885, 0.5126, 0.5349, 0.5272, 0.4889],\n",
      "        [0.4879, 0.4815, 0.4905, 0.5073, 0.5155, 0.5147, 0.4716, 0.4947, 0.5169,\n",
      "         0.5089, 0.5156, 0.4976, 0.4894, 0.5118, 0.4788, 0.4884, 0.5108, 0.5300,\n",
      "         0.4924, 0.5173, 0.4955, 0.5015, 0.4954, 0.4958, 0.5167, 0.4843, 0.5209,\n",
      "         0.4849, 0.4776, 0.5021, 0.5199, 0.5139, 0.4804],\n",
      "        [0.4845, 0.4784, 0.4872, 0.5040, 0.5120, 0.5128, 0.4694, 0.4926, 0.5139,\n",
      "         0.5055, 0.5125, 0.4933, 0.4878, 0.5086, 0.4759, 0.4845, 0.5067, 0.5272,\n",
      "         0.4889, 0.5138, 0.4919, 0.4973, 0.4914, 0.4926, 0.5115, 0.4828, 0.5191,\n",
      "         0.4806, 0.4753, 0.4995, 0.5163, 0.5107, 0.4782],\n",
      "        [0.4746, 0.4692, 0.4769, 0.4944, 0.5017, 0.5018, 0.4611, 0.4840, 0.5052,\n",
      "         0.4974, 0.5006, 0.4822, 0.4792, 0.5008, 0.4688, 0.4736, 0.4972, 0.5187,\n",
      "         0.4804, 0.5024, 0.4842, 0.4881, 0.4809, 0.4858, 0.5004, 0.4742, 0.5106,\n",
      "         0.4716, 0.4665, 0.4910, 0.5044, 0.5007, 0.4701],\n",
      "        [0.4818, 0.4760, 0.4840, 0.5009, 0.5089, 0.5104, 0.4668, 0.4906, 0.5115,\n",
      "         0.5028, 0.5096, 0.4890, 0.4861, 0.5057, 0.4738, 0.4811, 0.5036, 0.5252,\n",
      "         0.4861, 0.5103, 0.4894, 0.4946, 0.4877, 0.4901, 0.5076, 0.4805, 0.5170,\n",
      "         0.4778, 0.4729, 0.4972, 0.5129, 0.5078, 0.4759],\n",
      "        [0.4808, 0.4747, 0.4834, 0.5005, 0.5083, 0.5076, 0.4653, 0.4891, 0.5111,\n",
      "         0.5027, 0.5080, 0.4882, 0.4842, 0.5055, 0.4738, 0.4804, 0.5037, 0.5239,\n",
      "         0.4859, 0.5091, 0.4894, 0.4946, 0.4871, 0.4904, 0.5079, 0.4781, 0.5155,\n",
      "         0.4782, 0.4717, 0.4962, 0.5114, 0.5070, 0.4744]], device='cuda:0')\n",
      "X Shape torch.Size([30, 68, 2])\n",
      "pred = tensor([[0.4884, 0.4823, 0.4904, 0.5078, 0.5154, 0.5164, 0.4719, 0.4953, 0.5172,\n",
      "         0.5087, 0.5162, 0.4957, 0.4907, 0.5105, 0.4789, 0.4879, 0.5097, 0.5315,\n",
      "         0.4917, 0.5168, 0.4944, 0.5007, 0.4945, 0.4946, 0.5152, 0.4841, 0.5219,\n",
      "         0.4842, 0.4783, 0.5024, 0.5191, 0.5143, 0.4805],\n",
      "        [0.5023, 0.4950, 0.5074, 0.5224, 0.5325, 0.5322, 0.4836, 0.5093, 0.5310,\n",
      "         0.5224, 0.5354, 0.5141, 0.5069, 0.5239, 0.4899, 0.5052, 0.5267, 0.5431,\n",
      "         0.5057, 0.5358, 0.5081, 0.5175, 0.5114, 0.5055, 0.5333, 0.4976, 0.5365,\n",
      "         0.4991, 0.4924, 0.5148, 0.5405, 0.5307, 0.4907],\n",
      "        [0.4852, 0.4786, 0.4886, 0.5051, 0.5136, 0.5132, 0.4700, 0.4939, 0.5153,\n",
      "         0.5072, 0.5141, 0.4951, 0.4894, 0.5099, 0.4767, 0.4863, 0.5093, 0.5272,\n",
      "         0.4907, 0.5152, 0.4936, 0.5002, 0.4934, 0.4943, 0.5139, 0.4837, 0.5202,\n",
      "         0.4825, 0.4764, 0.5001, 0.5185, 0.5125, 0.4779],\n",
      "        [0.4888, 0.4831, 0.4914, 0.5086, 0.5156, 0.5168, 0.4726, 0.4953, 0.5176,\n",
      "         0.5095, 0.5162, 0.4975, 0.4900, 0.5116, 0.4791, 0.4888, 0.5100, 0.5319,\n",
      "         0.4921, 0.5178, 0.4950, 0.5008, 0.4955, 0.4957, 0.5165, 0.4846, 0.5219,\n",
      "         0.4853, 0.4789, 0.5026, 0.5197, 0.5152, 0.4819],\n",
      "        [0.4881, 0.4821, 0.4912, 0.5074, 0.5155, 0.5156, 0.4717, 0.4949, 0.5173,\n",
      "         0.5084, 0.5163, 0.4971, 0.4900, 0.5114, 0.4792, 0.4884, 0.5103, 0.5306,\n",
      "         0.4918, 0.5176, 0.4954, 0.5009, 0.4946, 0.4950, 0.5157, 0.4840, 0.5214,\n",
      "         0.4846, 0.4779, 0.5020, 0.5202, 0.5145, 0.4806],\n",
      "        [0.4817, 0.4761, 0.4848, 0.5020, 0.5096, 0.5100, 0.4663, 0.4913, 0.5120,\n",
      "         0.5041, 0.5102, 0.4913, 0.4858, 0.5068, 0.4744, 0.4825, 0.5050, 0.5252,\n",
      "         0.4871, 0.5108, 0.4900, 0.4958, 0.4891, 0.4917, 0.5098, 0.4806, 0.5171,\n",
      "         0.4793, 0.4734, 0.4972, 0.5135, 0.5090, 0.4757],\n",
      "        [0.4973, 0.4899, 0.5013, 0.5170, 0.5265, 0.5259, 0.4787, 0.5045, 0.5257,\n",
      "         0.5177, 0.5288, 0.5076, 0.5000, 0.5194, 0.4861, 0.4994, 0.5209, 0.5386,\n",
      "         0.5010, 0.5286, 0.5035, 0.5119, 0.5054, 0.5027, 0.5277, 0.4922, 0.5304,\n",
      "         0.4948, 0.4874, 0.5107, 0.5324, 0.5247, 0.4872],\n",
      "        [0.4789, 0.4743, 0.4815, 0.4984, 0.5052, 0.5066, 0.4648, 0.4867, 0.5090,\n",
      "         0.4999, 0.5047, 0.4872, 0.4811, 0.5034, 0.4721, 0.4783, 0.4998, 0.5232,\n",
      "         0.4836, 0.5068, 0.4866, 0.4903, 0.4850, 0.4884, 0.5046, 0.4772, 0.5134,\n",
      "         0.4751, 0.4698, 0.4945, 0.5082, 0.5050, 0.4749],\n",
      "        [0.4741, 0.4692, 0.4752, 0.4932, 0.5002, 0.5027, 0.4613, 0.4833, 0.5044,\n",
      "         0.4958, 0.4992, 0.4798, 0.4787, 0.4988, 0.4677, 0.4721, 0.4953, 0.5184,\n",
      "         0.4793, 0.5011, 0.4818, 0.4854, 0.4798, 0.4846, 0.4980, 0.4745, 0.5103,\n",
      "         0.4691, 0.4654, 0.4906, 0.5022, 0.4991, 0.4701],\n",
      "        [0.4890, 0.4820, 0.4916, 0.5084, 0.5171, 0.5173, 0.4728, 0.4965, 0.5184,\n",
      "         0.5099, 0.5180, 0.4972, 0.4926, 0.5120, 0.4795, 0.4897, 0.5120, 0.5305,\n",
      "         0.4931, 0.5192, 0.4952, 0.5023, 0.4962, 0.4961, 0.5168, 0.4864, 0.5234,\n",
      "         0.4851, 0.4787, 0.5038, 0.5212, 0.5154, 0.4804],\n",
      "        [0.4858, 0.4803, 0.4893, 0.5051, 0.5132, 0.5149, 0.4695, 0.4940, 0.5150,\n",
      "         0.5060, 0.5147, 0.4941, 0.4899, 0.5088, 0.4774, 0.4855, 0.5072, 0.5291,\n",
      "         0.4897, 0.5151, 0.4933, 0.4982, 0.4915, 0.4925, 0.5120, 0.4830, 0.5206,\n",
      "         0.4822, 0.4768, 0.5009, 0.5182, 0.5127, 0.4795],\n",
      "        [0.4867, 0.4803, 0.4901, 0.5060, 0.5146, 0.5148, 0.4707, 0.4948, 0.5161,\n",
      "         0.5080, 0.5156, 0.4959, 0.4904, 0.5108, 0.4779, 0.4871, 0.5098, 0.5295,\n",
      "         0.4916, 0.5164, 0.4951, 0.5010, 0.4939, 0.4944, 0.5147, 0.4835, 0.5211,\n",
      "         0.4836, 0.4775, 0.5014, 0.5200, 0.5135, 0.4794],\n",
      "        [0.4902, 0.4841, 0.4934, 0.5101, 0.5179, 0.5198, 0.4729, 0.4987, 0.5187,\n",
      "         0.5111, 0.5200, 0.4986, 0.4941, 0.5129, 0.4804, 0.4907, 0.5121, 0.5333,\n",
      "         0.4937, 0.5199, 0.4962, 0.5032, 0.4969, 0.4963, 0.5176, 0.4870, 0.5245,\n",
      "         0.4866, 0.4811, 0.5048, 0.5227, 0.5172, 0.4823],\n",
      "        [0.4895, 0.4834, 0.4933, 0.5093, 0.5178, 0.5187, 0.4732, 0.4978, 0.5192,\n",
      "         0.5103, 0.5194, 0.4993, 0.4936, 0.5128, 0.4799, 0.4909, 0.5126, 0.5319,\n",
      "         0.4938, 0.5201, 0.4969, 0.5034, 0.4970, 0.4961, 0.5176, 0.4867, 0.5242,\n",
      "         0.4860, 0.4803, 0.5034, 0.5236, 0.5171, 0.4816],\n",
      "        [0.4795, 0.4737, 0.4815, 0.4995, 0.5067, 0.5073, 0.4653, 0.4885, 0.5098,\n",
      "         0.5016, 0.5066, 0.4872, 0.4834, 0.5043, 0.4723, 0.4792, 0.5021, 0.5227,\n",
      "         0.4846, 0.5079, 0.4872, 0.4927, 0.4865, 0.4898, 0.5064, 0.4790, 0.5149,\n",
      "         0.4761, 0.4708, 0.4952, 0.5096, 0.5056, 0.4738],\n",
      "        [0.4943, 0.4876, 0.4982, 0.5139, 0.5231, 0.5237, 0.4769, 0.5020, 0.5237,\n",
      "         0.5145, 0.5256, 0.5044, 0.4985, 0.5169, 0.4833, 0.4965, 0.5178, 0.5361,\n",
      "         0.4979, 0.5262, 0.5006, 0.5087, 0.5022, 0.4996, 0.5233, 0.4911, 0.5289,\n",
      "         0.4906, 0.4845, 0.5078, 0.5302, 0.5224, 0.4849],\n",
      "        [0.4862, 0.4793, 0.4895, 0.5059, 0.5143, 0.5140, 0.4701, 0.4949, 0.5154,\n",
      "         0.5078, 0.5154, 0.4951, 0.4898, 0.5106, 0.4776, 0.4865, 0.5094, 0.5284,\n",
      "         0.4911, 0.5156, 0.4942, 0.5005, 0.4933, 0.4945, 0.5141, 0.4834, 0.5206,\n",
      "         0.4832, 0.4774, 0.5014, 0.5189, 0.5128, 0.4787],\n",
      "        [0.4865, 0.4797, 0.4898, 0.5058, 0.5148, 0.5146, 0.4704, 0.4950, 0.5161,\n",
      "         0.5081, 0.5158, 0.4962, 0.4904, 0.5110, 0.4777, 0.4874, 0.5103, 0.5288,\n",
      "         0.4916, 0.5167, 0.4947, 0.5009, 0.4941, 0.4945, 0.5148, 0.4842, 0.5211,\n",
      "         0.4832, 0.4769, 0.5015, 0.5202, 0.5134, 0.4787],\n",
      "        [0.4726, 0.4676, 0.4727, 0.4918, 0.4977, 0.5005, 0.4590, 0.4817, 0.5021,\n",
      "         0.4941, 0.4967, 0.4766, 0.4762, 0.4966, 0.4666, 0.4696, 0.4925, 0.5168,\n",
      "         0.4766, 0.4980, 0.4791, 0.4828, 0.4771, 0.4833, 0.4953, 0.4722, 0.5078,\n",
      "         0.4676, 0.4639, 0.4888, 0.4979, 0.4969, 0.4686],\n",
      "        [0.4837, 0.4782, 0.4872, 0.5036, 0.5115, 0.5129, 0.4690, 0.4926, 0.5142,\n",
      "         0.5053, 0.5124, 0.4935, 0.4880, 0.5083, 0.4755, 0.4847, 0.5065, 0.5270,\n",
      "         0.4886, 0.5135, 0.4918, 0.4971, 0.4912, 0.4924, 0.5111, 0.4827, 0.5191,\n",
      "         0.4801, 0.4752, 0.4983, 0.5166, 0.5113, 0.4777],\n",
      "        [0.4866, 0.4804, 0.4891, 0.5061, 0.5146, 0.5156, 0.4712, 0.4948, 0.5165,\n",
      "         0.5079, 0.5153, 0.4958, 0.4902, 0.5104, 0.4775, 0.4880, 0.5097, 0.5290,\n",
      "         0.4909, 0.5170, 0.4929, 0.4998, 0.4944, 0.4948, 0.5146, 0.4854, 0.5217,\n",
      "         0.4827, 0.4766, 0.5013, 0.5190, 0.5134, 0.4794],\n",
      "        [0.4802, 0.4740, 0.4819, 0.4993, 0.5076, 0.5067, 0.4655, 0.4880, 0.5097,\n",
      "         0.5017, 0.5063, 0.4886, 0.4823, 0.5053, 0.4729, 0.4798, 0.5029, 0.5227,\n",
      "         0.4852, 0.5086, 0.4876, 0.4927, 0.4870, 0.4904, 0.5072, 0.4789, 0.5144,\n",
      "         0.4768, 0.4704, 0.4960, 0.5098, 0.5052, 0.4744],\n",
      "        [0.4967, 0.4894, 0.5001, 0.5160, 0.5253, 0.5237, 0.4783, 0.5024, 0.5248,\n",
      "         0.5163, 0.5267, 0.5066, 0.4979, 0.5187, 0.4859, 0.4977, 0.5194, 0.5377,\n",
      "         0.4998, 0.5275, 0.5035, 0.5106, 0.5041, 0.5015, 0.5265, 0.4904, 0.5288,\n",
      "         0.4938, 0.4860, 0.5096, 0.5307, 0.5234, 0.4867],\n",
      "        [0.4826, 0.4770, 0.4857, 0.5020, 0.5103, 0.5120, 0.4682, 0.4916, 0.5132,\n",
      "         0.5041, 0.5110, 0.4918, 0.4875, 0.5071, 0.4744, 0.4834, 0.5055, 0.5260,\n",
      "         0.4876, 0.5125, 0.4908, 0.4959, 0.4899, 0.4912, 0.5093, 0.4819, 0.5184,\n",
      "         0.4784, 0.4738, 0.4974, 0.5156, 0.5100, 0.4767],\n",
      "        [0.4992, 0.4920, 0.5028, 0.5180, 0.5281, 0.5277, 0.4802, 0.5055, 0.5268,\n",
      "         0.5181, 0.5306, 0.5096, 0.5013, 0.5206, 0.4877, 0.5006, 0.5215, 0.5402,\n",
      "         0.5020, 0.5309, 0.5052, 0.5124, 0.5065, 0.5027, 0.5289, 0.4936, 0.5317,\n",
      "         0.4959, 0.4881, 0.5123, 0.5341, 0.5258, 0.4891],\n",
      "        [0.4797, 0.4735, 0.4814, 0.4990, 0.5073, 0.5071, 0.4654, 0.4882, 0.5104,\n",
      "         0.5021, 0.5063, 0.4878, 0.4837, 0.5048, 0.4725, 0.4800, 0.5036, 0.5222,\n",
      "         0.4856, 0.5086, 0.4880, 0.4939, 0.4873, 0.4902, 0.5073, 0.4790, 0.5152,\n",
      "         0.4762, 0.4699, 0.4954, 0.5101, 0.5057, 0.4731],\n",
      "        [0.4922, 0.4859, 0.4959, 0.5120, 0.5203, 0.5217, 0.4752, 0.5000, 0.5209,\n",
      "         0.5127, 0.5227, 0.5016, 0.4962, 0.5147, 0.4818, 0.4929, 0.5147, 0.5346,\n",
      "         0.4956, 0.5229, 0.4987, 0.5056, 0.4992, 0.4977, 0.5203, 0.4889, 0.5266,\n",
      "         0.4890, 0.4830, 0.5063, 0.5265, 0.5198, 0.4840],\n",
      "        [0.4760, 0.4706, 0.4771, 0.4955, 0.5024, 0.5038, 0.4628, 0.4844, 0.5068,\n",
      "         0.4979, 0.5011, 0.4821, 0.4798, 0.5005, 0.4693, 0.4749, 0.4979, 0.5197,\n",
      "         0.4811, 0.5033, 0.4830, 0.4880, 0.4823, 0.4866, 0.5009, 0.4758, 0.5116,\n",
      "         0.4716, 0.4666, 0.4918, 0.5039, 0.5014, 0.4711],\n",
      "        [0.4739, 0.4690, 0.4753, 0.4938, 0.5002, 0.5012, 0.4607, 0.4825, 0.5045,\n",
      "         0.4963, 0.4984, 0.4809, 0.4768, 0.4994, 0.4681, 0.4727, 0.4952, 0.5186,\n",
      "         0.4792, 0.5008, 0.4817, 0.4860, 0.4802, 0.4854, 0.4993, 0.4735, 0.5092,\n",
      "         0.4702, 0.4652, 0.4900, 0.5012, 0.4992, 0.4699],\n",
      "        [0.4864, 0.4803, 0.4890, 0.5057, 0.5140, 0.5154, 0.4705, 0.4947, 0.5163,\n",
      "         0.5073, 0.5155, 0.4945, 0.4910, 0.5097, 0.4776, 0.4869, 0.5093, 0.5293,\n",
      "         0.4907, 0.5164, 0.4936, 0.4997, 0.4932, 0.4938, 0.5136, 0.4841, 0.5217,\n",
      "         0.4825, 0.4767, 0.5009, 0.5193, 0.5136, 0.4791]], device='cuda:0')\n",
      "X Shape torch.Size([10, 68, 2])\n",
      "pred = tensor([[0.4810, 0.4746, 0.4828, 0.4997, 0.5087, 0.5091, 0.4668, 0.4896, 0.5112,\n",
      "         0.5026, 0.5082, 0.4897, 0.4850, 0.5058, 0.4731, 0.4814, 0.5047, 0.5231,\n",
      "         0.4866, 0.5103, 0.4887, 0.4943, 0.4887, 0.4907, 0.5079, 0.4809, 0.5162,\n",
      "         0.4764, 0.4711, 0.4962, 0.5125, 0.5069, 0.4744],\n",
      "        [0.4881, 0.4817, 0.4911, 0.5068, 0.5158, 0.5164, 0.4723, 0.4951, 0.5170,\n",
      "         0.5082, 0.5165, 0.4980, 0.4905, 0.5116, 0.4791, 0.4886, 0.5104, 0.5303,\n",
      "         0.4924, 0.5184, 0.4956, 0.5005, 0.4952, 0.4948, 0.5156, 0.4852, 0.5219,\n",
      "         0.4837, 0.4776, 0.5026, 0.5210, 0.5144, 0.4810],\n",
      "        [0.4836, 0.4777, 0.4860, 0.5029, 0.5108, 0.5121, 0.4688, 0.4911, 0.5135,\n",
      "         0.5044, 0.5108, 0.4919, 0.4863, 0.5073, 0.4751, 0.4836, 0.5049, 0.5267,\n",
      "         0.4879, 0.5122, 0.4905, 0.4955, 0.4901, 0.4916, 0.5097, 0.4819, 0.5182,\n",
      "         0.4791, 0.4740, 0.4983, 0.5144, 0.5097, 0.4778],\n",
      "        [0.4748, 0.4696, 0.4760, 0.4945, 0.5010, 0.5016, 0.4612, 0.4831, 0.5049,\n",
      "         0.4964, 0.4997, 0.4807, 0.4778, 0.4995, 0.4688, 0.4726, 0.4959, 0.5186,\n",
      "         0.4796, 0.5014, 0.4825, 0.4865, 0.4801, 0.4857, 0.4998, 0.4743, 0.5099,\n",
      "         0.4715, 0.4659, 0.4917, 0.5019, 0.4997, 0.4709],\n",
      "        [0.4859, 0.4797, 0.4886, 0.5057, 0.5138, 0.5151, 0.4706, 0.4945, 0.5154,\n",
      "         0.5074, 0.5147, 0.4954, 0.4893, 0.5099, 0.4767, 0.4868, 0.5086, 0.5286,\n",
      "         0.4899, 0.5160, 0.4925, 0.4991, 0.4937, 0.4944, 0.5139, 0.4850, 0.5208,\n",
      "         0.4825, 0.4769, 0.5008, 0.5186, 0.5132, 0.4793],\n",
      "        [0.5017, 0.4939, 0.5064, 0.5213, 0.5317, 0.5293, 0.4822, 0.5085, 0.5296,\n",
      "         0.5217, 0.5348, 0.5141, 0.5044, 0.5244, 0.4901, 0.5039, 0.5260, 0.5418,\n",
      "         0.5054, 0.5347, 0.5091, 0.5172, 0.5105, 0.5057, 0.5337, 0.4959, 0.5341,\n",
      "         0.4997, 0.4917, 0.5145, 0.5394, 0.5293, 0.4901],\n",
      "        [0.4920, 0.4843, 0.4944, 0.5114, 0.5206, 0.5203, 0.4744, 0.4995, 0.5203,\n",
      "         0.5127, 0.5213, 0.5005, 0.4944, 0.5146, 0.4816, 0.4929, 0.5143, 0.5333,\n",
      "         0.4952, 0.5219, 0.4970, 0.5053, 0.4990, 0.4985, 0.5206, 0.4888, 0.5254,\n",
      "         0.4887, 0.4818, 0.5066, 0.5238, 0.5180, 0.4831],\n",
      "        [0.4895, 0.4824, 0.4923, 0.5083, 0.5177, 0.5169, 0.4729, 0.4964, 0.5190,\n",
      "         0.5102, 0.5182, 0.4994, 0.4917, 0.5130, 0.4802, 0.4907, 0.5133, 0.5309,\n",
      "         0.4943, 0.5196, 0.4971, 0.5036, 0.4972, 0.4967, 0.5181, 0.4858, 0.5229,\n",
      "         0.4857, 0.4785, 0.5033, 0.5227, 0.5161, 0.4808],\n",
      "        [0.4833, 0.4769, 0.4851, 0.5024, 0.5107, 0.5114, 0.4683, 0.4916, 0.5124,\n",
      "         0.5043, 0.5106, 0.4912, 0.4869, 0.5071, 0.4750, 0.4829, 0.5055, 0.5260,\n",
      "         0.4879, 0.5121, 0.4902, 0.4960, 0.4901, 0.4918, 0.5102, 0.4819, 0.5182,\n",
      "         0.4794, 0.4736, 0.4989, 0.5139, 0.5086, 0.4770],\n",
      "        [0.4928, 0.4868, 0.4971, 0.5122, 0.5211, 0.5219, 0.4755, 0.5001, 0.5224,\n",
      "         0.5128, 0.5232, 0.5028, 0.4960, 0.5153, 0.4826, 0.4946, 0.5156, 0.5352,\n",
      "         0.4965, 0.5240, 0.4999, 0.5066, 0.5000, 0.4982, 0.5213, 0.4887, 0.5269,\n",
      "         0.4895, 0.4828, 0.5066, 0.5276, 0.5207, 0.4844]], device='cuda:0')\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test(test_loader, model, criterion)\n",
    "\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 68, 2) float32\n",
      "tensor([[0.4836, 0.4771, 0.4849, 0.5029, 0.5108, 0.5113, 0.4676, 0.4917, 0.5123,\n",
      "         0.5046, 0.5106, 0.4903, 0.4864, 0.5068, 0.4755, 0.4828, 0.5050, 0.5263,\n",
      "         0.4873, 0.5115, 0.4891, 0.4954, 0.4896, 0.4922, 0.5101, 0.4816, 0.5178,\n",
      "         0.4800, 0.4738, 0.4994, 0.5123, 0.5088, 0.4772]], device='cuda:0') torch.Size([1, 33])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_ = np.array([[\n",
    "    [-0.36369685665697504, -0.21546217565417758], [-0.3574112677601623, -\n",
    "                                                   0.12973942618903955], [-0.34360908442591476, -0.041386801250754224],\n",
    "    [-0.32728893521312186, 0.047003126885731494], [-0.3008596192879478,\n",
    "                                                   0.13302430193647485], [-0.24910143178451916, 0.20934664545504422],\n",
    "    [-0.1820489330188162, 0.27330297877009146], [-0.1047753579461298,\n",
    "                                                 0.32733666136376105], [-0.016907674584453414, 0.3462680344506007],\n",
    "    [0.0714076471556312, 0.3349838169948983], [0.15768994459377905,\n",
    "                                               0.290928739919908], [0.23164353502660584, 0.22906138570409784],\n",
    "    [0.28812057710241923, 0.15686104558670244], [0.32215974546052994,\n",
    "                                                 0.06921718141423006], [0.3461270503044599, -0.018575895551044996],\n",
    "    [0.3676136924680454, -0.1089242415930659], [0.3790284711174502, -\n",
    "                                                0.19942180042788948], [-0.29962861374732563, -0.29006857205551595],\n",
    "    [-0.25378298315870307, -0.3246486367875362], [-0.19323989247901685, -\n",
    "                                                  0.3313072576663558], [-0.13799385594382574, -0.3204147237917603],\n",
    "    [-0.08297163859783863, -0.2944143946458938], [0.0857693784628889, -\n",
    "                                                  0.2944330462449941], [0.146536288331779, -0.3161994623950847],\n",
    "    [0.20970925448461253, -0.330374677711339], [0.27513906412858635, -\n",
    "                                                0.3268868286795764], [0.32741949640682433, -0.2858160074606396],\n",
    "    [0.00400076800702176, -0.21505184047396997], [0.0031054912502057608, -\n",
    "                                                  0.1546206593888858], [-0.0002704481869548747, -0.09674474738054728],\n",
    "    [-0.0037209940205167324, -0.033832903615118703], [-0.07230292391244719,\n",
    "                                                      0.005447364090185869], [-0.03971858028416264, 0.01600416918097547],\n",
    "    [-0.004616270777332843, 0.02659827746996546], [0.0307844643151024,\n",
    "                                                   0.017048658730594135], [0.06366723352899217, 0.007461736793021978],\n",
    "    [-0.2304684842832848, -0.19837731087827104], [-0.19228866092489982, -\n",
    "                                                  0.22551538756925782], [-0.14192934335399632, -0.22476932360524426],\n",
    "    [-0.10212683087388219, -0.19143891601294627], [-0.14767403587689942, -\n",
    "                                                   0.17700257830928734], [-0.19803335344780293, -0.17774864227330067],\n",
    "    [0.10182840528827686, -0.1884173569586921], [0.14263810411980915, -\n",
    "                                                 0.22307202808711402], [0.19558999396565901, -0.22732459268199012],\n",
    "    [0.23802238191892033, -0.20151077952712704], [0.1999171649569368, -\n",
    "                                                  0.17940863459323053], [0.1470025783092873, -0.17767403587689945],\n",
    "    [-0.137005321191508, 0.12285918042679256], [-0.08882824071534368,\n",
    "                                                0.10090624828569872], [-0.04324373251412583, 0.08395194470349432],\n",
    "    [-0.008104119809095423, 0.09202808711393928], [0.027259312085139098,\n",
    "                                                   0.0849964342531132], [0.07483954139009275, 0.10333095616874222],\n",
    "    [0.1299363651324813, 0.12429535355751808], [0.07649953371002238,\n",
    "                                                0.16128147457348174], [0.030989631905205983, 0.17319984639859565],\n",
    "    [-0.006854462669372952, 0.17767623018267586], [-0.04706731032969447,\n",
    "                                                   0.17204344725437493], [-0.08961160787755773, 0.15378353173514736],\n",
    "    [-0.11182566240605629, 0.12323221240879911], [-0.04624663996927969,\n",
    "                                                  0.11664819792638104], [-0.008514454989302811, 0.11972571177793634],\n",
    "    [0.026774370508530265, 0.11772999067420054], [0.10723736902737391,\n",
    "                                                  0.1264775906522574], [0.0292550331888749, 0.1202852597509464],\n",
    "    [-0.006071095507158897, 0.12479894673322722], [-0.043765977288935165, 0.1192034670031269]]], dtype=np.float32)\n",
    "\n",
    "print(x_.shape, x_.dtype)\n",
    "model.eval()\n",
    "x = torch.Tensor(x_).to(device)\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    print(f'{pred} {pred.shape}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
