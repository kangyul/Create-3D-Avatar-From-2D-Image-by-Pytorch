{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from FaceFeatureDataset import FaceFeatureDataset\n",
    "import dlib_index as DI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10154363 -0.1885906 ]\n",
      " [ 0.14489932 -0.22053692]\n",
      " [ 0.19510067 -0.22281879]\n",
      " [ 0.2361745  -0.20456375]\n",
      " [ 0.19966443 -0.18174496]\n",
      " [ 0.14946309 -0.17946309]]\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('./outimg/Train/facefeature.csv')\n",
    "features = features.iloc[0, 1:]\n",
    "features = np.array(features, dtype=np.float32).reshape(-1,2);\n",
    "features = np.take(features, DI.LEFT_EYE, axis=0)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10154363 -0.1885906 ]\n",
      " [ 0.14489932 -0.22053692]\n",
      " [ 0.19510067 -0.22281879]\n",
      " [ 0.2361745  -0.20456375]\n",
      " [ 0.19966443 -0.18174496]\n",
      " [ 0.14946309 -0.17946309]]\n",
      "[[ 0.10061225 -0.19306122]\n",
      " [ 0.14455782 -0.2185034 ]\n",
      " [ 0.19312926 -0.2185034 ]\n",
      " [ 0.23707484 -0.20462584]\n",
      " [ 0.20006803 -0.1814966 ]\n",
      " [ 0.14918368 -0.1814966 ]]\n",
      "[[ 0.09998894 -0.1884398 ]\n",
      " [ 0.1412192  -0.22190624]\n",
      " [ 0.19430037 -0.22635248]\n",
      " [ 0.23756564 -0.20675485]\n",
      " [ 0.20120913 -0.18048787]\n",
      " [ 0.14812796 -0.17604165]]\n",
      "[[ 0.10433789 -0.19365232]\n",
      " [ 0.14599673 -0.21977147]\n",
      " [ 0.19216487 -0.2215405 ]\n",
      " [ 0.23073658 -0.2015609 ]\n",
      " [ 0.19643134 -0.18019384]\n",
      " [ 0.1502979  -0.18085289]]\n",
      "[[ 0.09296875 -0.189375  ]\n",
      " [ 0.14078125 -0.2265625 ]\n",
      " [ 0.19921875 -0.22921875]\n",
      " [ 0.244375   -0.2       ]\n",
      " [ 0.20453125 -0.1734375 ]\n",
      " [ 0.14609376 -0.1734375 ]]\n",
      "[[ 0.10243012 -0.19049683]\n",
      " [ 0.14516678 -0.2226826 ]\n",
      " [ 0.19516408 -0.22231497]\n",
      " [ 0.23497811 -0.19702221]\n",
      " [ 0.19983295 -0.17728063]\n",
      " [ 0.14733578 -0.17766665]]\n",
      "[[ 0.09309501 -0.18581812]\n",
      " [ 0.14227039 -0.2259809 ]\n",
      " [ 0.19601934 -0.23028542]\n",
      " [ 0.24163963 -0.20403549]\n",
      " [ 0.20284124 -0.17394224]\n",
      " [ 0.14657491 -0.17223196]]\n",
      "[[ 0.09998894 -0.1884398 ]\n",
      " [ 0.14361332 -0.21947792]\n",
      " [ 0.19428328 -0.22394125]\n",
      " [ 0.2351202  -0.2019495 ]\n",
      " [ 0.1987808  -0.17809375]\n",
      " [ 0.14814505 -0.17845288]]\n",
      "[[ 0.09680556 -0.19055556]\n",
      " [ 0.14166667 -0.22361112]\n",
      " [ 0.19597222 -0.22361112]\n",
      " [ 0.24083333 -0.2023611 ]\n",
      " [ 0.20069444 -0.17638889]\n",
      " [ 0.14638889 -0.17402777]]\n",
      "[[ 0.0994471  -0.18769966]\n",
      " [ 0.14261433 -0.22877815]\n",
      " [ 0.19884779 -0.23019385]\n",
      " [ 0.24020478 -0.19825938]\n",
      " [ 0.20307167 -0.17389078]\n",
      " [ 0.14690785 -0.17528328]]\n",
      "Shape of X [N, F, C]: torch.Size([10, 6, 2])\n",
      "Shape of Tensor y: torch.Size([10, 33]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 머리      headSize =0, headWidth = 1,\n",
    "# 이마      foreheadPosition= 2,foreheadSize = 3,\n",
    "# 턱        jawsPosition =4,jawsSize = 5,\n",
    "# 아래턱    chinPosition = 6,chinSize = 7,chinPronounced = 8,\n",
    "# 뺨        lowCheek = 9, cheekPosition = 10, cheekSize = 11,\n",
    "# 귀        earsPosition = 12, earsRotation =13, earsSize = 14\n",
    "# 눈        eyeSize = 15, eyePosition= 16, eyeDepth = 17, eyeRotation = 18, eyeDistance = 19, eyeSquint= 20,\n",
    "# 코        noseSize,nosePosition,noseFlatten,nosePronounced,noseWidth,noseBridge,noseCurve,noseInclination,\n",
    "# 입        mouthSize,mouthPosition,mouthPronounced,lipsSize\n",
    "\n",
    "eyeIndexes = ['eyeSize', 'eyePosition', 'eyeDepth', 'eyeRotation', 'eyeDistance', 'eyeSquint']\n",
    "#eyeIndexes = list(range(15, 21))\n",
    "\n",
    "training_data = FaceFeatureDataset(feature_file=\"./outimg/Train/facefeature.csv\", feature_indexes=DI.LEFT_EYE,\n",
    "                                   label_file=\"./Dataset/Train/csv/train.csv\", label_indexes=eyeIndexes)\n",
    "test_data = FaceFeatureDataset(\n",
    "    feature_file=\"./outimg/Test/facefeature.csv\", label_file=\"./Dataset/Test/csv/test.csv\")\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 데이터 로드 확인\n",
    "for X, y in train_loader:\n",
    "    # N , Channel, H= width W = height\n",
    "    print(f\"Shape of X [N, F, C]: {X.shape}\")\n",
    "    print(f\"Shape of Tensor y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "n_total_steps = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=136, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=33, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_classes = 33\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(68 * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "            #nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)        \n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('batch',  batch)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "[[ 0.35026845 -0.12241611]\n",
      " [ 0.36167786 -0.21597315]\n",
      " [-0.29322147 -0.2867114 ]\n",
      " [-0.2475839  -0.31637585]\n",
      " [-0.18597315 -0.3209396 ]\n",
      " [-0.13120805 -0.30724832]]\n",
      "[[ 0.34809524 -0.10517007]\n",
      " [ 0.35965985 -0.19537415]\n",
      " [-0.2995238  -0.27632654]\n",
      " [-0.2532653  -0.3110204 ]\n",
      " [-0.19312926 -0.3179592 ]\n",
      " [-0.13530612 -0.30408162]]\n",
      "[[ 0.35997385 -0.12631224]\n",
      " [ 0.3726969  -0.22026455]\n",
      " [-0.30439594 -0.29017302]\n",
      " [-0.25353786 -0.32115984]\n",
      " [-0.19322301 -0.32555476]\n",
      " [-0.13545619 -0.310677  ]]\n",
      "[[ 0.36275455 -0.09281779]\n",
      " [ 0.3738543  -0.18980208]\n",
      " [-0.30229545 -0.2893185 ]\n",
      " [-0.255607   -0.32750866]\n",
      " [-0.19240767 -0.33146298]\n",
      " [-0.13673536 -0.31852478]]\n",
      "[[ 0.40109375 -0.09375   ]\n",
      " [ 0.414375   -0.2053125 ]\n",
      " [-0.31875    -0.295625  ]\n",
      " [-0.26828125 -0.33546874]\n",
      " [-0.201875   -0.34078124]\n",
      " [-0.14078125 -0.3275    ]]\n",
      "[[ 0.3641623  -0.08607233]\n",
      " [ 0.37484187 -0.17849381]\n",
      " [-0.30692273 -0.2785068 ]\n",
      " [-0.25913122 -0.31815538]\n",
      " [-0.19656107 -0.3276953 ]\n",
      " [-0.13415635 -0.31473646]]\n",
      "[[ 0.3604561  -0.11533149]\n",
      " [ 0.37469563 -0.21226022]\n",
      " [-0.30144182 -0.2991194 ]\n",
      " [-0.24982592 -0.33157632]\n",
      " [-0.18840954 -0.33576557]\n",
      " [-0.129818   -0.32210252]]\n",
      "[[ 0.35014084 -0.09985716]\n",
      " [ 0.3628468  -0.19139825]\n",
      " [-0.29478523 -0.28528216]\n",
      " [-0.24873252 -0.3187144 ]\n",
      " [-0.19081178 -0.32553768]\n",
      " [-0.13545619 -0.310677  ]]\n",
      "[[ 0.37305555 -0.09611111]\n",
      " [ 0.3848611  -0.19527778]\n",
      " [-0.31166667 -0.28736112]\n",
      " [-0.25972223 -0.32277778]\n",
      " [-0.19833334 -0.3298611 ]\n",
      " [-0.13694444 -0.31569445]]\n",
      "[[ 0.39987713 -0.06504437]\n",
      " [ 0.41087782 -0.16873857]\n",
      " [-0.31639728 -0.3019768 ]\n",
      " [-0.26480547 -0.34284642]\n",
      " [-0.20000818 -0.34966964]\n",
      " [-0.1385761  -0.33409694]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x12 and 136x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     train(train_loader, model, criterion, optimizer)    \n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Compute prediction error\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m pred \u001b[39m=\u001b[39m model(X)        \n\u001b[0;32m      8\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[1;32m---> 20\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_relu_stack(x)\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x12 and 136x128)"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, criterion, optimizer)    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './face_eye.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    print('test size', size )\n",
    "    # num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)            \n",
    "            print('pred =', pred)\n",
    "            #print('loss', loss)\n",
    "            #print('real', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size 100\n",
      "pred = tensor([[0.4949, 0.4872, 0.4981, 0.5107, 0.5251, 0.5245, 0.4764, 0.5042, 0.5239,\n",
      "         0.5138, 0.5233, 0.5014, 0.4990, 0.5154, 0.4866, 0.4953, 0.5167, 0.5367,\n",
      "         0.4967, 0.5231, 0.5004, 0.5102, 0.5002, 0.5012, 0.5219, 0.4913, 0.5280,\n",
      "         0.4912, 0.4862, 0.5077, 0.5270, 0.5217, 0.4868],\n",
      "        [0.4914, 0.4847, 0.4956, 0.5111, 0.5210, 0.5206, 0.4752, 0.5010, 0.5215,\n",
      "         0.5101, 0.5213, 0.4998, 0.4949, 0.5150, 0.4840, 0.4918, 0.5138, 0.5364,\n",
      "         0.4955, 0.5235, 0.4979, 0.5055, 0.4957, 0.4990, 0.5188, 0.4893, 0.5253,\n",
      "         0.4886, 0.4824, 0.5050, 0.5258, 0.5178, 0.4822],\n",
      "        [0.4968, 0.4900, 0.5007, 0.5134, 0.5280, 0.5262, 0.4786, 0.5064, 0.5254,\n",
      "         0.5142, 0.5253, 0.5038, 0.5012, 0.5185, 0.4882, 0.4979, 0.5183, 0.5407,\n",
      "         0.4995, 0.5255, 0.5018, 0.5123, 0.5016, 0.5020, 0.5233, 0.4949, 0.5300,\n",
      "         0.4924, 0.4885, 0.5092, 0.5303, 0.5236, 0.4890],\n",
      "        [0.4972, 0.4910, 0.5013, 0.5126, 0.5294, 0.5266, 0.4777, 0.5069, 0.5283,\n",
      "         0.5172, 0.5252, 0.5026, 0.5017, 0.5188, 0.4886, 0.4990, 0.5201, 0.5390,\n",
      "         0.4991, 0.5250, 0.5035, 0.5152, 0.5033, 0.5045, 0.5244, 0.4924, 0.5312,\n",
      "         0.4940, 0.4886, 0.5098, 0.5285, 0.5237, 0.4897],\n",
      "        [0.4927, 0.4858, 0.4966, 0.5103, 0.5208, 0.5223, 0.4759, 0.5024, 0.5211,\n",
      "         0.5130, 0.5228, 0.5004, 0.4963, 0.5143, 0.4832, 0.4941, 0.5153, 0.5355,\n",
      "         0.4957, 0.5223, 0.4977, 0.5068, 0.4967, 0.5000, 0.5204, 0.4899, 0.5268,\n",
      "         0.4898, 0.4839, 0.5058, 0.5253, 0.5194, 0.4845],\n",
      "        [0.4794, 0.4711, 0.4807, 0.5015, 0.5010, 0.5089, 0.4660, 0.4876, 0.5081,\n",
      "         0.5012, 0.5095, 0.4908, 0.4798, 0.5022, 0.4715, 0.4765, 0.4998, 0.5216,\n",
      "         0.4846, 0.5116, 0.4874, 0.4900, 0.4844, 0.4883, 0.5074, 0.4778, 0.5129,\n",
      "         0.4795, 0.4692, 0.4927, 0.5116, 0.5049, 0.4719],\n",
      "        [0.4904, 0.4834, 0.4938, 0.5102, 0.5175, 0.5198, 0.4749, 0.4997, 0.5191,\n",
      "         0.5096, 0.5198, 0.4993, 0.4928, 0.5131, 0.4816, 0.4900, 0.5119, 0.5339,\n",
      "         0.4946, 0.5215, 0.4960, 0.5036, 0.4944, 0.4977, 0.5180, 0.4883, 0.5241,\n",
      "         0.4876, 0.4810, 0.5035, 0.5238, 0.5164, 0.4819],\n",
      "        [0.4774, 0.4688, 0.4779, 0.4985, 0.4994, 0.5065, 0.4635, 0.4856, 0.5064,\n",
      "         0.4993, 0.5074, 0.4884, 0.4795, 0.5000, 0.4699, 0.4751, 0.4979, 0.5182,\n",
      "         0.4814, 0.5084, 0.4872, 0.4880, 0.4833, 0.4858, 0.5045, 0.4768, 0.5108,\n",
      "         0.4779, 0.4672, 0.4904, 0.5090, 0.5023, 0.4711],\n",
      "        [0.4893, 0.4818, 0.4923, 0.5089, 0.5152, 0.5178, 0.4736, 0.4972, 0.5178,\n",
      "         0.5092, 0.5185, 0.4974, 0.4907, 0.5119, 0.4800, 0.4875, 0.5102, 0.5317,\n",
      "         0.4931, 0.5207, 0.4947, 0.5015, 0.4925, 0.4970, 0.5163, 0.4855, 0.5228,\n",
      "         0.4866, 0.4789, 0.5017, 0.5222, 0.5148, 0.4801],\n",
      "        [0.4823, 0.4739, 0.4835, 0.5023, 0.5051, 0.5117, 0.4674, 0.4908, 0.5102,\n",
      "         0.5038, 0.5122, 0.4925, 0.4842, 0.5042, 0.4732, 0.4811, 0.5027, 0.5233,\n",
      "         0.4859, 0.5121, 0.4897, 0.4936, 0.4873, 0.4897, 0.5095, 0.4810, 0.5160,\n",
      "         0.4815, 0.4726, 0.4950, 0.5138, 0.5076, 0.4758]], device='cuda:0')\n",
      "pred = tensor([[0.4893, 0.4828, 0.4922, 0.5086, 0.5174, 0.5182, 0.4735, 0.4983, 0.5192,\n",
      "         0.5094, 0.5181, 0.4975, 0.4927, 0.5129, 0.4801, 0.4888, 0.5105, 0.5324,\n",
      "         0.4932, 0.5199, 0.4966, 0.5028, 0.4940, 0.4968, 0.5163, 0.4860, 0.5232,\n",
      "         0.4868, 0.4795, 0.5016, 0.5217, 0.5142, 0.4820],\n",
      "        [0.4877, 0.4808, 0.4907, 0.5073, 0.5143, 0.5174, 0.4718, 0.4966, 0.5171,\n",
      "         0.5085, 0.5172, 0.4966, 0.4902, 0.5101, 0.4793, 0.4873, 0.5092, 0.5303,\n",
      "         0.4921, 0.5184, 0.4946, 0.5014, 0.4927, 0.4954, 0.5154, 0.4853, 0.5214,\n",
      "         0.4860, 0.4783, 0.5009, 0.5200, 0.5138, 0.4805],\n",
      "        [0.4868, 0.4792, 0.4893, 0.5066, 0.5124, 0.5159, 0.4720, 0.4953, 0.5152,\n",
      "         0.5068, 0.5168, 0.4956, 0.4891, 0.5100, 0.4780, 0.4860, 0.5080, 0.5292,\n",
      "         0.4908, 0.5180, 0.4935, 0.4993, 0.4908, 0.4938, 0.5136, 0.4853, 0.5207,\n",
      "         0.4849, 0.4766, 0.4994, 0.5199, 0.5123, 0.4788],\n",
      "        [0.4929, 0.4853, 0.4963, 0.5104, 0.5215, 0.5213, 0.4761, 0.5007, 0.5211,\n",
      "         0.5113, 0.5216, 0.4989, 0.4957, 0.5154, 0.4834, 0.4922, 0.5136, 0.5348,\n",
      "         0.4958, 0.5227, 0.4983, 0.5063, 0.4963, 0.4983, 0.5185, 0.4899, 0.5258,\n",
      "         0.4886, 0.4825, 0.5048, 0.5257, 0.5182, 0.4845],\n",
      "        [0.4881, 0.4801, 0.4901, 0.5075, 0.5128, 0.5169, 0.4728, 0.4963, 0.5168,\n",
      "         0.5087, 0.5165, 0.4967, 0.4888, 0.5096, 0.4783, 0.4856, 0.5085, 0.5292,\n",
      "         0.4919, 0.5184, 0.4937, 0.5003, 0.4920, 0.4962, 0.5159, 0.4840, 0.5211,\n",
      "         0.4858, 0.4775, 0.5005, 0.5196, 0.5133, 0.4799],\n",
      "        [0.4735, 0.4649, 0.4735, 0.4960, 0.4936, 0.5021, 0.4608, 0.4810, 0.5031,\n",
      "         0.4968, 0.5036, 0.4850, 0.4743, 0.4971, 0.4654, 0.4701, 0.4935, 0.5137,\n",
      "         0.4781, 0.5054, 0.4842, 0.4834, 0.4791, 0.4826, 0.5007, 0.4723, 0.5072,\n",
      "         0.4750, 0.4620, 0.4862, 0.5046, 0.4969, 0.4674],\n",
      "        [0.4904, 0.4825, 0.4924, 0.5071, 0.5180, 0.5198, 0.4730, 0.4993, 0.5188,\n",
      "         0.5105, 0.5188, 0.4977, 0.4937, 0.5116, 0.4807, 0.4907, 0.5112, 0.5312,\n",
      "         0.4926, 0.5183, 0.4968, 0.5044, 0.4959, 0.4968, 0.5170, 0.4878, 0.5237,\n",
      "         0.4877, 0.4811, 0.5028, 0.5216, 0.5156, 0.4837],\n",
      "        [0.4825, 0.4756, 0.4848, 0.5036, 0.5080, 0.5125, 0.4679, 0.4915, 0.5120,\n",
      "         0.5034, 0.5125, 0.4930, 0.4855, 0.5060, 0.4752, 0.4818, 0.5034, 0.5252,\n",
      "         0.4877, 0.5135, 0.4919, 0.4950, 0.4886, 0.4897, 0.5101, 0.4822, 0.5161,\n",
      "         0.4822, 0.4733, 0.4961, 0.5155, 0.5080, 0.4766],\n",
      "        [0.4932, 0.4865, 0.4969, 0.5107, 0.5231, 0.5235, 0.4752, 0.5032, 0.5229,\n",
      "         0.5124, 0.5219, 0.5012, 0.4972, 0.5145, 0.4856, 0.4943, 0.5152, 0.5360,\n",
      "         0.4966, 0.5220, 0.4996, 0.5089, 0.4993, 0.5001, 0.5209, 0.4909, 0.5267,\n",
      "         0.4903, 0.4855, 0.5068, 0.5254, 0.5201, 0.4861],\n",
      "        [0.4994, 0.4921, 0.5032, 0.5137, 0.5317, 0.5286, 0.4795, 0.5092, 0.5289,\n",
      "         0.5177, 0.5264, 0.5047, 0.5039, 0.5194, 0.4896, 0.5005, 0.5210, 0.5413,\n",
      "         0.5007, 0.5259, 0.5044, 0.5159, 0.5046, 0.5054, 0.5262, 0.4947, 0.5324,\n",
      "         0.4940, 0.4908, 0.5114, 0.5304, 0.5255, 0.4914]], device='cuda:0')\n",
      "pred = tensor([[0.5011, 0.4950, 0.5057, 0.5157, 0.5352, 0.5311, 0.4812, 0.5117, 0.5307,\n",
      "         0.5188, 0.5288, 0.5066, 0.5068, 0.5224, 0.4918, 0.5039, 0.5233, 0.5442,\n",
      "         0.5030, 0.5280, 0.5063, 0.5187, 0.5068, 0.5062, 0.5279, 0.4978, 0.5347,\n",
      "         0.4961, 0.4935, 0.5138, 0.5339, 0.5276, 0.4938],\n",
      "        [0.4797, 0.4715, 0.4801, 0.4999, 0.5034, 0.5091, 0.4644, 0.4882, 0.5088,\n",
      "         0.5020, 0.5088, 0.4903, 0.4823, 0.5022, 0.4709, 0.4785, 0.4994, 0.5210,\n",
      "         0.4831, 0.5092, 0.4894, 0.4910, 0.4861, 0.4875, 0.5071, 0.4783, 0.5132,\n",
      "         0.4796, 0.4698, 0.4927, 0.5109, 0.5040, 0.4739],\n",
      "        [0.4839, 0.4755, 0.4852, 0.5024, 0.5087, 0.5129, 0.4682, 0.4922, 0.5138,\n",
      "         0.5058, 0.5131, 0.4925, 0.4865, 0.5055, 0.4754, 0.4830, 0.5053, 0.5244,\n",
      "         0.4866, 0.5141, 0.4919, 0.4970, 0.4899, 0.4923, 0.5109, 0.4813, 0.5176,\n",
      "         0.4833, 0.4739, 0.4964, 0.5148, 0.5089, 0.4769],\n",
      "        [0.4820, 0.4746, 0.4831, 0.5025, 0.5071, 0.5114, 0.4675, 0.4911, 0.5113,\n",
      "         0.5033, 0.5114, 0.4923, 0.4849, 0.5058, 0.4733, 0.4816, 0.5025, 0.5243,\n",
      "         0.4861, 0.5126, 0.4910, 0.4942, 0.4879, 0.4894, 0.5091, 0.4816, 0.5160,\n",
      "         0.4820, 0.4722, 0.4951, 0.5144, 0.5060, 0.4761],\n",
      "        [0.4933, 0.4861, 0.4971, 0.5121, 0.5210, 0.5226, 0.4771, 0.5024, 0.5214,\n",
      "         0.5123, 0.5224, 0.5007, 0.4955, 0.5156, 0.4836, 0.4931, 0.5151, 0.5364,\n",
      "         0.4973, 0.5241, 0.4976, 0.5068, 0.4966, 0.5006, 0.5204, 0.4904, 0.5267,\n",
      "         0.4894, 0.4838, 0.5064, 0.5263, 0.5191, 0.4842],\n",
      "        [0.4833, 0.4753, 0.4849, 0.5023, 0.5092, 0.5126, 0.4674, 0.4918, 0.5127,\n",
      "         0.5045, 0.5129, 0.4926, 0.4865, 0.5057, 0.4752, 0.4830, 0.5040, 0.5246,\n",
      "         0.4866, 0.5127, 0.4924, 0.4963, 0.4895, 0.4901, 0.5102, 0.4822, 0.5169,\n",
      "         0.4824, 0.4735, 0.4962, 0.5151, 0.5085, 0.4772],\n",
      "        [0.4869, 0.4806, 0.4904, 0.5074, 0.5123, 0.5171, 0.4715, 0.4967, 0.5160,\n",
      "         0.5085, 0.5172, 0.4970, 0.4894, 0.5092, 0.4786, 0.4869, 0.5094, 0.5300,\n",
      "         0.4919, 0.5179, 0.4935, 0.5002, 0.4917, 0.4957, 0.5155, 0.4845, 0.5212,\n",
      "         0.4863, 0.4782, 0.5006, 0.5194, 0.5134, 0.4797],\n",
      "        [0.4751, 0.4669, 0.4757, 0.4973, 0.4964, 0.5039, 0.4615, 0.4826, 0.5053,\n",
      "         0.4988, 0.5049, 0.4861, 0.4759, 0.4986, 0.4671, 0.4725, 0.4950, 0.5159,\n",
      "         0.4799, 0.5068, 0.4858, 0.4857, 0.4809, 0.4846, 0.5028, 0.4730, 0.5090,\n",
      "         0.4766, 0.4637, 0.4881, 0.5063, 0.4986, 0.4688],\n",
      "        [0.4904, 0.4836, 0.4933, 0.5078, 0.5204, 0.5196, 0.4728, 0.4995, 0.5210,\n",
      "         0.5102, 0.5186, 0.4977, 0.4946, 0.5127, 0.4822, 0.4907, 0.5117, 0.5329,\n",
      "         0.4933, 0.5195, 0.4985, 0.5055, 0.4967, 0.4971, 0.5172, 0.4876, 0.5238,\n",
      "         0.4880, 0.4811, 0.5029, 0.5224, 0.5157, 0.4836],\n",
      "        [0.4903, 0.4829, 0.4929, 0.5084, 0.5198, 0.5190, 0.4734, 0.4997, 0.5212,\n",
      "         0.5097, 0.5184, 0.4983, 0.4936, 0.5129, 0.4822, 0.4900, 0.5117, 0.5334,\n",
      "         0.4933, 0.5202, 0.4979, 0.5050, 0.4963, 0.4979, 0.5175, 0.4872, 0.5240,\n",
      "         0.4878, 0.4808, 0.5027, 0.5222, 0.5155, 0.4823]], device='cuda:0')\n",
      "pred = tensor([[0.4867, 0.4794, 0.4887, 0.5055, 0.5120, 0.5161, 0.4710, 0.4953, 0.5156,\n",
      "         0.5085, 0.5156, 0.4953, 0.4889, 0.5085, 0.4774, 0.4856, 0.5077, 0.5272,\n",
      "         0.4902, 0.5165, 0.4937, 0.4996, 0.4914, 0.4946, 0.5141, 0.4833, 0.5203,\n",
      "         0.4852, 0.4764, 0.4993, 0.5180, 0.5116, 0.4797],\n",
      "        [0.4913, 0.4845, 0.4946, 0.5093, 0.5207, 0.5211, 0.4741, 0.5008, 0.5222,\n",
      "         0.5115, 0.5195, 0.4990, 0.4946, 0.5131, 0.4833, 0.4911, 0.5129, 0.5336,\n",
      "         0.4948, 0.5206, 0.4986, 0.5067, 0.4969, 0.4990, 0.5193, 0.4874, 0.5246,\n",
      "         0.4889, 0.4819, 0.5046, 0.5228, 0.5170, 0.4838],\n",
      "        [0.4907, 0.4836, 0.4927, 0.5072, 0.5198, 0.5201, 0.4729, 0.5003, 0.5206,\n",
      "         0.5113, 0.5182, 0.4975, 0.4948, 0.5125, 0.4814, 0.4914, 0.5119, 0.5312,\n",
      "         0.4923, 0.5184, 0.4986, 0.5060, 0.4968, 0.4979, 0.5175, 0.4874, 0.5242,\n",
      "         0.4885, 0.4809, 0.5032, 0.5215, 0.5148, 0.4842],\n",
      "        [0.5040, 0.4978, 0.5095, 0.5192, 0.5373, 0.5342, 0.4847, 0.5150, 0.5328,\n",
      "         0.5213, 0.5324, 0.5094, 0.5081, 0.5245, 0.4949, 0.5060, 0.5275, 0.5476,\n",
      "         0.5065, 0.5324, 0.5066, 0.5213, 0.5080, 0.5108, 0.5320, 0.4997, 0.5372,\n",
      "         0.4986, 0.4964, 0.5173, 0.5370, 0.5321, 0.4947],\n",
      "        [0.4880, 0.4803, 0.4904, 0.5082, 0.5138, 0.5166, 0.4724, 0.4965, 0.5161,\n",
      "         0.5072, 0.5170, 0.4968, 0.4895, 0.5115, 0.4784, 0.4863, 0.5080, 0.5311,\n",
      "         0.4918, 0.5186, 0.4944, 0.4995, 0.4912, 0.4949, 0.5148, 0.4855, 0.5213,\n",
      "         0.4848, 0.4774, 0.5007, 0.5208, 0.5125, 0.4793],\n",
      "        [0.4901, 0.4832, 0.4932, 0.5087, 0.5175, 0.5201, 0.4734, 0.4997, 0.5188,\n",
      "         0.5104, 0.5194, 0.4990, 0.4935, 0.5118, 0.4814, 0.4905, 0.5116, 0.5326,\n",
      "         0.4938, 0.5190, 0.4967, 0.5039, 0.4955, 0.4971, 0.5179, 0.4877, 0.5239,\n",
      "         0.4880, 0.4818, 0.5034, 0.5222, 0.5167, 0.4833],\n",
      "        [0.4921, 0.4849, 0.4956, 0.5107, 0.5189, 0.5211, 0.4762, 0.5007, 0.5201,\n",
      "         0.5115, 0.5216, 0.4999, 0.4940, 0.5141, 0.4822, 0.4914, 0.5135, 0.5346,\n",
      "         0.4961, 0.5228, 0.4965, 0.5052, 0.4953, 0.4991, 0.5193, 0.4891, 0.5256,\n",
      "         0.4886, 0.4822, 0.5045, 0.5249, 0.5180, 0.4836],\n",
      "        [0.4889, 0.4817, 0.4916, 0.5082, 0.5146, 0.5184, 0.4734, 0.4978, 0.5179,\n",
      "         0.5095, 0.5177, 0.4974, 0.4907, 0.5107, 0.4796, 0.4874, 0.5101, 0.5304,\n",
      "         0.4927, 0.5194, 0.4948, 0.5020, 0.4929, 0.4969, 0.5167, 0.4853, 0.5221,\n",
      "         0.4867, 0.4787, 0.5019, 0.5207, 0.5145, 0.4810],\n",
      "        [0.4940, 0.4875, 0.4978, 0.5117, 0.5243, 0.5228, 0.4772, 0.5029, 0.5242,\n",
      "         0.5131, 0.5224, 0.5006, 0.4973, 0.5168, 0.4852, 0.4940, 0.5158, 0.5367,\n",
      "         0.4975, 0.5241, 0.5001, 0.5093, 0.4985, 0.5007, 0.5209, 0.4903, 0.5275,\n",
      "         0.4906, 0.4841, 0.5062, 0.5265, 0.5195, 0.4859],\n",
      "        [0.5009, 0.4960, 0.5066, 0.5160, 0.5366, 0.5313, 0.4811, 0.5116, 0.5328,\n",
      "         0.5197, 0.5292, 0.5058, 0.5074, 0.5229, 0.4934, 0.5046, 0.5248, 0.5444,\n",
      "         0.5035, 0.5291, 0.5073, 0.5203, 0.5072, 0.5074, 0.5284, 0.4969, 0.5349,\n",
      "         0.4969, 0.4933, 0.5146, 0.5337, 0.5282, 0.4934]], device='cuda:0')\n",
      "pred = tensor([[0.4978, 0.4906, 0.5018, 0.5140, 0.5281, 0.5273, 0.4795, 0.5074, 0.5259,\n",
      "         0.5160, 0.5266, 0.5037, 0.5018, 0.5187, 0.4881, 0.4991, 0.5199, 0.5405,\n",
      "         0.5002, 0.5265, 0.5015, 0.5131, 0.5018, 0.5039, 0.5246, 0.4946, 0.5311,\n",
      "         0.4928, 0.4891, 0.5105, 0.5304, 0.5250, 0.4894],\n",
      "        [0.4934, 0.4877, 0.4981, 0.5129, 0.5225, 0.5224, 0.4777, 0.5027, 0.5233,\n",
      "         0.5129, 0.5230, 0.5009, 0.4962, 0.5171, 0.4844, 0.4937, 0.5161, 0.5376,\n",
      "         0.4980, 0.5259, 0.4987, 0.5078, 0.4968, 0.5018, 0.5210, 0.4899, 0.5277,\n",
      "         0.4905, 0.4838, 0.5064, 0.5272, 0.5194, 0.4842],\n",
      "        [0.4921, 0.4846, 0.4952, 0.5089, 0.5219, 0.5218, 0.4735, 0.5017, 0.5227,\n",
      "         0.5119, 0.5201, 0.4993, 0.4955, 0.5131, 0.4845, 0.4923, 0.5139, 0.5338,\n",
      "         0.4947, 0.5202, 0.4995, 0.5079, 0.4988, 0.4998, 0.5195, 0.4885, 0.5252,\n",
      "         0.4896, 0.4834, 0.5052, 0.5233, 0.5183, 0.4848],\n",
      "        [0.5051, 0.4989, 0.5106, 0.5196, 0.5395, 0.5352, 0.4849, 0.5153, 0.5329,\n",
      "         0.5216, 0.5330, 0.5095, 0.5105, 0.5256, 0.4954, 0.5078, 0.5273, 0.5490,\n",
      "         0.5071, 0.5324, 0.5080, 0.5221, 0.5092, 0.5096, 0.5318, 0.5015, 0.5381,\n",
      "         0.4985, 0.4977, 0.5180, 0.5388, 0.5333, 0.4964],\n",
      "        [0.4968, 0.4902, 0.5011, 0.5132, 0.5275, 0.5263, 0.4784, 0.5067, 0.5258,\n",
      "         0.5161, 0.5256, 0.5028, 0.5008, 0.5187, 0.4874, 0.4986, 0.5195, 0.5394,\n",
      "         0.4992, 0.5256, 0.5018, 0.5126, 0.5014, 0.5039, 0.5237, 0.4933, 0.5307,\n",
      "         0.4928, 0.4881, 0.5098, 0.5292, 0.5232, 0.4887],\n",
      "        [0.4860, 0.4792, 0.4887, 0.5067, 0.5110, 0.5154, 0.4715, 0.4946, 0.5141,\n",
      "         0.5066, 0.5166, 0.4957, 0.4884, 0.5095, 0.4769, 0.4856, 0.5070, 0.5291,\n",
      "         0.4908, 0.5174, 0.4925, 0.4979, 0.4901, 0.4930, 0.5133, 0.4846, 0.5202,\n",
      "         0.4847, 0.4766, 0.4989, 0.5195, 0.5119, 0.4789],\n",
      "        [0.4931, 0.4866, 0.4971, 0.5115, 0.5231, 0.5231, 0.4761, 0.5033, 0.5223,\n",
      "         0.5120, 0.5229, 0.5016, 0.4974, 0.5156, 0.4853, 0.4944, 0.5157, 0.5374,\n",
      "         0.4969, 0.5234, 0.4990, 0.5081, 0.4980, 0.4999, 0.5207, 0.4915, 0.5271,\n",
      "         0.4900, 0.4851, 0.5066, 0.5265, 0.5201, 0.4853],\n",
      "        [0.4811, 0.4733, 0.4820, 0.5014, 0.5049, 0.5100, 0.4668, 0.4893, 0.5110,\n",
      "         0.5030, 0.5099, 0.4906, 0.4830, 0.5041, 0.4722, 0.4789, 0.5011, 0.5216,\n",
      "         0.4848, 0.5118, 0.4903, 0.4930, 0.4862, 0.4892, 0.5082, 0.4788, 0.5147,\n",
      "         0.4807, 0.4698, 0.4934, 0.5122, 0.5047, 0.4745],\n",
      "        [0.4861, 0.4786, 0.4885, 0.5051, 0.5136, 0.5153, 0.4698, 0.4954, 0.5157,\n",
      "         0.5062, 0.5155, 0.4952, 0.4896, 0.5091, 0.4781, 0.4860, 0.5071, 0.5290,\n",
      "         0.4893, 0.5161, 0.4946, 0.4993, 0.4915, 0.4932, 0.5129, 0.4847, 0.5197,\n",
      "         0.4842, 0.4764, 0.4990, 0.5187, 0.5110, 0.4790],\n",
      "        [0.4835, 0.4755, 0.4850, 0.5033, 0.5080, 0.5127, 0.4682, 0.4916, 0.5135,\n",
      "         0.5051, 0.5122, 0.4927, 0.4849, 0.5055, 0.4752, 0.4812, 0.5039, 0.5242,\n",
      "         0.4875, 0.5139, 0.4918, 0.4962, 0.4891, 0.4917, 0.5111, 0.4803, 0.5167,\n",
      "         0.4827, 0.4729, 0.4962, 0.5146, 0.5083, 0.4766]], device='cuda:0')\n",
      "pred = tensor([[0.4898, 0.4831, 0.4924, 0.5086, 0.5173, 0.5197, 0.4743, 0.4990, 0.5190,\n",
      "         0.5098, 0.5184, 0.4983, 0.4930, 0.5122, 0.4809, 0.4893, 0.5107, 0.5318,\n",
      "         0.4938, 0.5196, 0.4964, 0.5036, 0.4946, 0.4963, 0.5172, 0.4874, 0.5232,\n",
      "         0.4876, 0.4804, 0.5027, 0.5220, 0.5152, 0.4831],\n",
      "        [0.4802, 0.4721, 0.4814, 0.5016, 0.5031, 0.5093, 0.4664, 0.4893, 0.5099,\n",
      "         0.5019, 0.5096, 0.4913, 0.4812, 0.5032, 0.4718, 0.4775, 0.5007, 0.5220,\n",
      "         0.4849, 0.5115, 0.4891, 0.4914, 0.4854, 0.4893, 0.5082, 0.4782, 0.5137,\n",
      "         0.4803, 0.4696, 0.4930, 0.5115, 0.5045, 0.4732],\n",
      "        [0.4790, 0.4711, 0.4804, 0.5013, 0.5017, 0.5082, 0.4650, 0.4870, 0.5080,\n",
      "         0.5003, 0.5088, 0.4904, 0.4801, 0.5024, 0.4710, 0.4762, 0.4987, 0.5213,\n",
      "         0.4842, 0.5108, 0.4882, 0.4898, 0.4845, 0.4870, 0.5064, 0.4778, 0.5125,\n",
      "         0.4787, 0.4687, 0.4921, 0.5114, 0.5038, 0.4724],\n",
      "        [0.4810, 0.4736, 0.4825, 0.5022, 0.5035, 0.5105, 0.4673, 0.4896, 0.5103,\n",
      "         0.5038, 0.5111, 0.4918, 0.4825, 0.5038, 0.4724, 0.4794, 0.5023, 0.5222,\n",
      "         0.4858, 0.5124, 0.4890, 0.4927, 0.4861, 0.4900, 0.5091, 0.4789, 0.5152,\n",
      "         0.4813, 0.4709, 0.4941, 0.5123, 0.5060, 0.4745],\n",
      "        [0.5018, 0.4955, 0.5069, 0.5172, 0.5354, 0.5310, 0.4822, 0.5116, 0.5300,\n",
      "         0.5189, 0.5301, 0.5071, 0.5064, 0.5236, 0.4919, 0.5036, 0.5236, 0.5462,\n",
      "         0.5041, 0.5297, 0.5058, 0.5178, 0.5055, 0.5067, 0.5284, 0.4981, 0.5349,\n",
      "         0.4957, 0.4932, 0.5142, 0.5353, 0.5284, 0.4933],\n",
      "        [0.4859, 0.4788, 0.4882, 0.5057, 0.5116, 0.5150, 0.4704, 0.4947, 0.5164,\n",
      "         0.5075, 0.5145, 0.4947, 0.4878, 0.5086, 0.4772, 0.4847, 0.5072, 0.5274,\n",
      "         0.4901, 0.5167, 0.4937, 0.4993, 0.4911, 0.4949, 0.5136, 0.4824, 0.5197,\n",
      "         0.4850, 0.4755, 0.4988, 0.5172, 0.5102, 0.4784],\n",
      "        [0.4859, 0.4784, 0.4878, 0.5050, 0.5117, 0.5149, 0.4703, 0.4944, 0.5149,\n",
      "         0.5070, 0.5151, 0.4949, 0.4884, 0.5086, 0.4767, 0.4849, 0.5067, 0.5275,\n",
      "         0.4894, 0.5158, 0.4934, 0.4989, 0.4911, 0.4933, 0.5129, 0.4834, 0.5197,\n",
      "         0.4842, 0.4758, 0.4981, 0.5175, 0.5109, 0.4792],\n",
      "        [0.4752, 0.4672, 0.4758, 0.4973, 0.4968, 0.5040, 0.4616, 0.4829, 0.5049,\n",
      "         0.4981, 0.5050, 0.4866, 0.4765, 0.4989, 0.4673, 0.4724, 0.4949, 0.5159,\n",
      "         0.4800, 0.5065, 0.4862, 0.4858, 0.4815, 0.4837, 0.5024, 0.4743, 0.5088,\n",
      "         0.4766, 0.4641, 0.4880, 0.5069, 0.4989, 0.4695],\n",
      "        [0.4843, 0.4753, 0.4857, 0.5037, 0.5093, 0.5126, 0.4684, 0.4915, 0.5119,\n",
      "         0.5036, 0.5134, 0.4928, 0.4866, 0.5069, 0.4754, 0.4820, 0.5036, 0.5261,\n",
      "         0.4874, 0.5146, 0.4920, 0.4952, 0.4889, 0.4900, 0.5098, 0.4831, 0.5170,\n",
      "         0.4815, 0.4739, 0.4963, 0.5169, 0.5092, 0.4769],\n",
      "        [0.4796, 0.4711, 0.4807, 0.5009, 0.5035, 0.5080, 0.4654, 0.4867, 0.5100,\n",
      "         0.5008, 0.5085, 0.4895, 0.4809, 0.5027, 0.4720, 0.4771, 0.4991, 0.5215,\n",
      "         0.4839, 0.5111, 0.4895, 0.4910, 0.4853, 0.4873, 0.5065, 0.4779, 0.5127,\n",
      "         0.4796, 0.4687, 0.4923, 0.5117, 0.5038, 0.4723]], device='cuda:0')\n",
      "pred = tensor([[0.4870, 0.4798, 0.4901, 0.5080, 0.5130, 0.5160, 0.4719, 0.4954, 0.5161,\n",
      "         0.5065, 0.5161, 0.4965, 0.4886, 0.5101, 0.4785, 0.4851, 0.5073, 0.5307,\n",
      "         0.4921, 0.5189, 0.4937, 0.4989, 0.4913, 0.4944, 0.5143, 0.4845, 0.5204,\n",
      "         0.4848, 0.4772, 0.5000, 0.5205, 0.5125, 0.4786],\n",
      "        [0.5050, 0.5002, 0.5112, 0.5193, 0.5415, 0.5353, 0.4844, 0.5165, 0.5352,\n",
      "         0.5224, 0.5335, 0.5098, 0.5119, 0.5269, 0.4965, 0.5096, 0.5289, 0.5491,\n",
      "         0.5073, 0.5325, 0.5097, 0.5243, 0.5107, 0.5105, 0.5319, 0.5017, 0.5395,\n",
      "         0.4996, 0.4984, 0.5181, 0.5386, 0.5327, 0.4973],\n",
      "        [0.4857, 0.4789, 0.4883, 0.5046, 0.5134, 0.5156, 0.4696, 0.4951, 0.5164,\n",
      "         0.5072, 0.5151, 0.4945, 0.4896, 0.5087, 0.4781, 0.4864, 0.5074, 0.5276,\n",
      "         0.4894, 0.5156, 0.4948, 0.5003, 0.4924, 0.4936, 0.5130, 0.4838, 0.5199,\n",
      "         0.4850, 0.4765, 0.4991, 0.5177, 0.5110, 0.4795],\n",
      "        [0.4861, 0.4795, 0.4892, 0.5072, 0.5110, 0.5159, 0.4716, 0.4959, 0.5157,\n",
      "         0.5076, 0.5166, 0.4966, 0.4876, 0.5094, 0.4776, 0.4856, 0.5086, 0.5295,\n",
      "         0.4916, 0.5184, 0.4924, 0.4990, 0.4906, 0.4956, 0.5148, 0.4839, 0.5207,\n",
      "         0.4856, 0.4768, 0.5000, 0.5188, 0.5117, 0.4780],\n",
      "        [0.4866, 0.4791, 0.4889, 0.5064, 0.5121, 0.5158, 0.4711, 0.4953, 0.5161,\n",
      "         0.5074, 0.5154, 0.4955, 0.4879, 0.5091, 0.4777, 0.4848, 0.5073, 0.5285,\n",
      "         0.4905, 0.5173, 0.4936, 0.4995, 0.4911, 0.4948, 0.5143, 0.4833, 0.5200,\n",
      "         0.4850, 0.4761, 0.4995, 0.5185, 0.5116, 0.4787],\n",
      "        [0.4807, 0.4724, 0.4818, 0.5014, 0.5052, 0.5097, 0.4659, 0.4898, 0.5111,\n",
      "         0.5020, 0.5100, 0.4914, 0.4827, 0.5034, 0.4736, 0.4790, 0.5016, 0.5228,\n",
      "         0.4845, 0.5119, 0.4901, 0.4927, 0.4870, 0.4897, 0.5083, 0.4792, 0.5144,\n",
      "         0.4809, 0.4709, 0.4940, 0.5122, 0.5055, 0.4735],\n",
      "        [0.4972, 0.4907, 0.5017, 0.5138, 0.5295, 0.5266, 0.4789, 0.5072, 0.5274,\n",
      "         0.5154, 0.5256, 0.5034, 0.5017, 0.5191, 0.4888, 0.4984, 0.5197, 0.5408,\n",
      "         0.5001, 0.5265, 0.5029, 0.5134, 0.5020, 0.5039, 0.5244, 0.4937, 0.5305,\n",
      "         0.4928, 0.4883, 0.5101, 0.5300, 0.5236, 0.4886],\n",
      "        [0.4768, 0.4687, 0.4773, 0.4993, 0.4969, 0.5062, 0.4640, 0.4851, 0.5055,\n",
      "         0.4998, 0.5070, 0.4886, 0.4775, 0.5000, 0.4683, 0.4742, 0.4974, 0.5182,\n",
      "         0.4816, 0.5089, 0.4854, 0.4868, 0.4818, 0.4865, 0.5047, 0.4755, 0.5109,\n",
      "         0.4780, 0.4661, 0.4901, 0.5085, 0.5013, 0.4697],\n",
      "        [0.4759, 0.4672, 0.4763, 0.4981, 0.4964, 0.5045, 0.4633, 0.4830, 0.5037,\n",
      "         0.4977, 0.5063, 0.4873, 0.4772, 0.4996, 0.4671, 0.4732, 0.4952, 0.5171,\n",
      "         0.4810, 0.5077, 0.4851, 0.4849, 0.4808, 0.4832, 0.5022, 0.4762, 0.5097,\n",
      "         0.4761, 0.4653, 0.4885, 0.5083, 0.4999, 0.4695],\n",
      "        [0.4897, 0.4830, 0.4929, 0.5082, 0.5189, 0.5193, 0.4730, 0.4991, 0.5191,\n",
      "         0.5094, 0.5192, 0.4977, 0.4938, 0.5131, 0.4814, 0.4907, 0.5113, 0.5331,\n",
      "         0.4930, 0.5194, 0.4971, 0.5041, 0.4947, 0.4958, 0.5166, 0.4881, 0.5235,\n",
      "         0.4876, 0.4804, 0.5027, 0.5228, 0.5150, 0.4825]], device='cuda:0')\n",
      "pred = tensor([[0.4867, 0.4789, 0.4891, 0.5061, 0.5120, 0.5164, 0.4712, 0.4953, 0.5147,\n",
      "         0.5068, 0.5166, 0.4956, 0.4894, 0.5089, 0.4783, 0.4857, 0.5077, 0.5287,\n",
      "         0.4904, 0.5170, 0.4933, 0.4992, 0.4912, 0.4935, 0.5138, 0.4851, 0.5201,\n",
      "         0.4848, 0.4772, 0.4994, 0.5194, 0.5131, 0.4794],\n",
      "        [0.4870, 0.4803, 0.4897, 0.5063, 0.5141, 0.5168, 0.4712, 0.4957, 0.5172,\n",
      "         0.5079, 0.5158, 0.4954, 0.4899, 0.5096, 0.4789, 0.4862, 0.5079, 0.5286,\n",
      "         0.4912, 0.5175, 0.4950, 0.5011, 0.4926, 0.4944, 0.5144, 0.4843, 0.5206,\n",
      "         0.4856, 0.4770, 0.5001, 0.5192, 0.5124, 0.4804],\n",
      "        [0.4917, 0.4850, 0.4952, 0.5107, 0.5200, 0.5210, 0.4756, 0.5012, 0.5203,\n",
      "         0.5103, 0.5209, 0.5001, 0.4947, 0.5148, 0.4831, 0.4915, 0.5132, 0.5351,\n",
      "         0.4958, 0.5224, 0.4975, 0.5055, 0.4961, 0.4984, 0.5185, 0.4899, 0.5254,\n",
      "         0.4885, 0.4826, 0.5044, 0.5251, 0.5176, 0.4837],\n",
      "        [0.4907, 0.4833, 0.4936, 0.5086, 0.5187, 0.5202, 0.4736, 0.5000, 0.5199,\n",
      "         0.5103, 0.5196, 0.4986, 0.4938, 0.5125, 0.4820, 0.4909, 0.5121, 0.5331,\n",
      "         0.4937, 0.5198, 0.4971, 0.5051, 0.4961, 0.4975, 0.5178, 0.4883, 0.5241,\n",
      "         0.4880, 0.4819, 0.5034, 0.5226, 0.5167, 0.4835],\n",
      "        [0.4793, 0.4714, 0.4805, 0.5005, 0.5036, 0.5083, 0.4650, 0.4880, 0.5093,\n",
      "         0.5010, 0.5093, 0.4902, 0.4818, 0.5032, 0.4717, 0.4780, 0.4999, 0.5217,\n",
      "         0.4834, 0.5109, 0.4892, 0.4910, 0.4852, 0.4877, 0.5066, 0.4784, 0.5133,\n",
      "         0.4796, 0.4691, 0.4925, 0.5117, 0.5036, 0.4727],\n",
      "        [0.4970, 0.4885, 0.5001, 0.5117, 0.5286, 0.5258, 0.4772, 0.5057, 0.5250,\n",
      "         0.5143, 0.5247, 0.5022, 0.5014, 0.5176, 0.4880, 0.4974, 0.5179, 0.5393,\n",
      "         0.4979, 0.5243, 0.5023, 0.5122, 0.5020, 0.5016, 0.5225, 0.4939, 0.5292,\n",
      "         0.4915, 0.4880, 0.5088, 0.5293, 0.5232, 0.4889],\n",
      "        [0.4867, 0.4806, 0.4897, 0.5059, 0.5149, 0.5164, 0.4711, 0.4959, 0.5178,\n",
      "         0.5079, 0.5159, 0.4954, 0.4906, 0.5101, 0.4792, 0.4869, 0.5084, 0.5290,\n",
      "         0.4908, 0.5175, 0.4955, 0.5015, 0.4928, 0.4944, 0.5141, 0.4844, 0.5210,\n",
      "         0.4858, 0.4771, 0.4998, 0.5191, 0.5120, 0.4802],\n",
      "        [0.4870, 0.4800, 0.4893, 0.5059, 0.5144, 0.5165, 0.4710, 0.4960, 0.5173,\n",
      "         0.5078, 0.5154, 0.4956, 0.4903, 0.5094, 0.4788, 0.4865, 0.5080, 0.5287,\n",
      "         0.4909, 0.5167, 0.4954, 0.5010, 0.4930, 0.4944, 0.5142, 0.4845, 0.5206,\n",
      "         0.4857, 0.4774, 0.4998, 0.5188, 0.5121, 0.4806],\n",
      "        [0.4725, 0.4630, 0.4722, 0.4968, 0.4917, 0.5005, 0.4610, 0.4793, 0.5006,\n",
      "         0.4937, 0.5029, 0.4853, 0.4724, 0.4972, 0.4641, 0.4687, 0.4916, 0.5154,\n",
      "         0.4781, 0.5059, 0.4816, 0.4800, 0.4777, 0.4803, 0.4991, 0.4732, 0.5067,\n",
      "         0.4735, 0.4618, 0.4859, 0.5052, 0.4963, 0.4653],\n",
      "        [0.4843, 0.4767, 0.4863, 0.5038, 0.5097, 0.5142, 0.4686, 0.4938, 0.5136,\n",
      "         0.5054, 0.5141, 0.4944, 0.4876, 0.5064, 0.4765, 0.4843, 0.5057, 0.5262,\n",
      "         0.4880, 0.5141, 0.4926, 0.4973, 0.4905, 0.4921, 0.5118, 0.4831, 0.5184,\n",
      "         0.4837, 0.4758, 0.4977, 0.5161, 0.5101, 0.4778]], device='cuda:0')\n",
      "pred = tensor([[0.4882, 0.4796, 0.4900, 0.5061, 0.5149, 0.5168, 0.4715, 0.4974, 0.5165,\n",
      "         0.5075, 0.5169, 0.4966, 0.4908, 0.5104, 0.4785, 0.4874, 0.5085, 0.5302,\n",
      "         0.4907, 0.5170, 0.4950, 0.5008, 0.4928, 0.4949, 0.5146, 0.4861, 0.5212,\n",
      "         0.4850, 0.4781, 0.5002, 0.5199, 0.5127, 0.4806],\n",
      "        [0.4801, 0.4711, 0.4804, 0.5003, 0.5031, 0.5077, 0.4656, 0.4874, 0.5099,\n",
      "         0.5018, 0.5084, 0.4894, 0.4806, 0.5031, 0.4707, 0.4766, 0.4995, 0.5202,\n",
      "         0.4832, 0.5105, 0.4894, 0.4909, 0.4849, 0.4885, 0.5067, 0.4768, 0.5132,\n",
      "         0.4791, 0.4679, 0.4917, 0.5104, 0.5026, 0.4728],\n",
      "        [0.4950, 0.4887, 0.4992, 0.5128, 0.5254, 0.5244, 0.4777, 0.5043, 0.5255,\n",
      "         0.5146, 0.5231, 0.5016, 0.4982, 0.5171, 0.4861, 0.4953, 0.5170, 0.5380,\n",
      "         0.4987, 0.5249, 0.5008, 0.5106, 0.4997, 0.5026, 0.5227, 0.4905, 0.5285,\n",
      "         0.4916, 0.4854, 0.5078, 0.5274, 0.5209, 0.4865],\n",
      "        [0.4843, 0.4760, 0.4857, 0.5031, 0.5096, 0.5138, 0.4681, 0.4931, 0.5126,\n",
      "         0.5049, 0.5137, 0.4938, 0.4876, 0.5060, 0.4757, 0.4837, 0.5048, 0.5259,\n",
      "         0.4874, 0.5134, 0.4924, 0.4965, 0.4903, 0.4911, 0.5111, 0.4832, 0.5177,\n",
      "         0.4828, 0.4752, 0.4970, 0.5160, 0.5098, 0.4779],\n",
      "        [0.4990, 0.4928, 0.5036, 0.5152, 0.5305, 0.5284, 0.4809, 0.5089, 0.5280,\n",
      "         0.5174, 0.5271, 0.5046, 0.5030, 0.5204, 0.4892, 0.5003, 0.5214, 0.5417,\n",
      "         0.5018, 0.5279, 0.5032, 0.5149, 0.5028, 0.5057, 0.5261, 0.4950, 0.5323,\n",
      "         0.4942, 0.4900, 0.5116, 0.5315, 0.5254, 0.4905],\n",
      "        [0.4792, 0.4716, 0.4805, 0.5005, 0.5040, 0.5084, 0.4649, 0.4878, 0.5097,\n",
      "         0.5012, 0.5083, 0.4897, 0.4815, 0.5031, 0.4716, 0.4779, 0.4993, 0.5209,\n",
      "         0.4838, 0.5102, 0.4901, 0.4912, 0.4856, 0.4874, 0.5066, 0.4781, 0.5128,\n",
      "         0.4797, 0.4687, 0.4928, 0.5114, 0.5029, 0.4733],\n",
      "        [0.4945, 0.4876, 0.4985, 0.5115, 0.5243, 0.5238, 0.4771, 0.5042, 0.5231,\n",
      "         0.5132, 0.5243, 0.5015, 0.4985, 0.5167, 0.4854, 0.4959, 0.5170, 0.5375,\n",
      "         0.4976, 0.5239, 0.4997, 0.5097, 0.4989, 0.5008, 0.5211, 0.4922, 0.5284,\n",
      "         0.4907, 0.4857, 0.5069, 0.5273, 0.5212, 0.4865],\n",
      "        [0.4762, 0.4675, 0.4770, 0.4990, 0.4980, 0.5046, 0.4628, 0.4838, 0.5047,\n",
      "         0.4974, 0.5062, 0.4877, 0.4772, 0.5006, 0.4683, 0.4734, 0.4953, 0.5185,\n",
      "         0.4809, 0.5080, 0.4863, 0.4855, 0.4812, 0.4838, 0.5027, 0.4761, 0.5096,\n",
      "         0.4762, 0.4652, 0.4893, 0.5089, 0.4997, 0.4692],\n",
      "        [0.4724, 0.4632, 0.4721, 0.4959, 0.4914, 0.5006, 0.4605, 0.4793, 0.5016,\n",
      "         0.4948, 0.5017, 0.4845, 0.4724, 0.4962, 0.4642, 0.4684, 0.4914, 0.5138,\n",
      "         0.4776, 0.5053, 0.4828, 0.4803, 0.4779, 0.4816, 0.4991, 0.4715, 0.5059,\n",
      "         0.4740, 0.4611, 0.4854, 0.5042, 0.4952, 0.4653],\n",
      "        [0.4882, 0.4798, 0.4901, 0.5063, 0.5155, 0.5174, 0.4712, 0.4967, 0.5162,\n",
      "         0.5070, 0.5171, 0.4961, 0.4912, 0.5103, 0.4798, 0.4873, 0.5083, 0.5304,\n",
      "         0.4909, 0.5178, 0.4952, 0.5009, 0.4931, 0.4939, 0.5143, 0.4869, 0.5207,\n",
      "         0.4851, 0.4785, 0.5010, 0.5209, 0.5136, 0.4806]], device='cuda:0')\n",
      "pred = tensor([[0.4823, 0.4742, 0.4831, 0.5015, 0.5076, 0.5112, 0.4664, 0.4911, 0.5116,\n",
      "         0.5035, 0.5110, 0.4920, 0.4853, 0.5050, 0.4734, 0.4812, 0.5022, 0.5238,\n",
      "         0.4851, 0.5116, 0.4917, 0.4944, 0.4884, 0.4897, 0.5090, 0.4807, 0.5157,\n",
      "         0.4813, 0.4720, 0.4948, 0.5138, 0.5061, 0.4762],\n",
      "        [0.4884, 0.4817, 0.4906, 0.5069, 0.5144, 0.5181, 0.4725, 0.4977, 0.5171,\n",
      "         0.5099, 0.5171, 0.4970, 0.4911, 0.5102, 0.4785, 0.4880, 0.5094, 0.5293,\n",
      "         0.4920, 0.5173, 0.4950, 0.5018, 0.4932, 0.4959, 0.5160, 0.4852, 0.5223,\n",
      "         0.4867, 0.4788, 0.5009, 0.5194, 0.5136, 0.4820],\n",
      "        [0.4837, 0.4755, 0.4854, 0.5037, 0.5080, 0.5128, 0.4683, 0.4917, 0.5118,\n",
      "         0.5044, 0.5132, 0.4936, 0.4858, 0.5058, 0.4750, 0.4822, 0.5037, 0.5257,\n",
      "         0.4875, 0.5139, 0.4913, 0.4950, 0.4886, 0.4906, 0.5107, 0.4820, 0.5169,\n",
      "         0.4821, 0.4739, 0.4963, 0.5158, 0.5089, 0.4766],\n",
      "        [0.4739, 0.4653, 0.4743, 0.4971, 0.4944, 0.5022, 0.4617, 0.4812, 0.5032,\n",
      "         0.4967, 0.5049, 0.4856, 0.4747, 0.4986, 0.4661, 0.4710, 0.4939, 0.5156,\n",
      "         0.4788, 0.5065, 0.4843, 0.4833, 0.4788, 0.4827, 0.5012, 0.4732, 0.5081,\n",
      "         0.4751, 0.4624, 0.4867, 0.5063, 0.4979, 0.4668],\n",
      "        [0.4871, 0.4792, 0.4891, 0.5049, 0.5135, 0.5161, 0.4704, 0.4966, 0.5160,\n",
      "         0.5076, 0.5165, 0.4963, 0.4904, 0.5087, 0.4783, 0.4868, 0.5087, 0.5289,\n",
      "         0.4897, 0.5162, 0.4943, 0.5002, 0.4927, 0.4946, 0.5140, 0.4849, 0.5208,\n",
      "         0.4851, 0.4779, 0.4994, 0.5184, 0.5124, 0.4801],\n",
      "        [0.5016, 0.4959, 0.5066, 0.5161, 0.5367, 0.5311, 0.4816, 0.5122, 0.5332,\n",
      "         0.5203, 0.5289, 0.5063, 0.5071, 0.5231, 0.4931, 0.5040, 0.5251, 0.5445,\n",
      "         0.5033, 0.5298, 0.5071, 0.5202, 0.5074, 0.5091, 0.5288, 0.4961, 0.5357,\n",
      "         0.4973, 0.4934, 0.5143, 0.5334, 0.5278, 0.4932],\n",
      "        [0.4922, 0.4854, 0.4960, 0.5106, 0.5223, 0.5210, 0.4751, 0.5018, 0.5218,\n",
      "         0.5110, 0.5208, 0.5002, 0.4955, 0.5153, 0.4837, 0.4919, 0.5137, 0.5359,\n",
      "         0.4957, 0.5216, 0.4992, 0.5065, 0.4967, 0.4990, 0.5193, 0.4890, 0.5257,\n",
      "         0.4886, 0.4826, 0.5048, 0.5248, 0.5174, 0.4844],\n",
      "        [0.4881, 0.4815, 0.4907, 0.5072, 0.5161, 0.5179, 0.4714, 0.4974, 0.5187,\n",
      "         0.5091, 0.5161, 0.4966, 0.4912, 0.5106, 0.4796, 0.4878, 0.5090, 0.5305,\n",
      "         0.4918, 0.5175, 0.4963, 0.5025, 0.4941, 0.4959, 0.5160, 0.4849, 0.5217,\n",
      "         0.4867, 0.4785, 0.5013, 0.5197, 0.5130, 0.4815],\n",
      "        [0.4848, 0.4769, 0.4867, 0.5044, 0.5101, 0.5134, 0.4692, 0.4929, 0.5135,\n",
      "         0.5052, 0.5139, 0.4936, 0.4870, 0.5078, 0.4756, 0.4833, 0.5049, 0.5265,\n",
      "         0.4885, 0.5148, 0.4928, 0.4968, 0.4895, 0.4918, 0.5112, 0.4827, 0.5181,\n",
      "         0.4826, 0.4744, 0.4971, 0.5167, 0.5092, 0.4778],\n",
      "        [0.4934, 0.4856, 0.4966, 0.5106, 0.5221, 0.5230, 0.4757, 0.5022, 0.5217,\n",
      "         0.5122, 0.5219, 0.4999, 0.4966, 0.5146, 0.4845, 0.4933, 0.5145, 0.5354,\n",
      "         0.4959, 0.5224, 0.4987, 0.5077, 0.4979, 0.4993, 0.5201, 0.4906, 0.5261,\n",
      "         0.4896, 0.4841, 0.5062, 0.5259, 0.5199, 0.4855]], device='cuda:0')\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test(test_loader, model, criterion)\n",
    "\n",
    "print(\"Done !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
