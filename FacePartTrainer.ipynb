{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from FaceFeatureDataset import FaceFeatureDataset\n",
    "import dlib_index as DI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxdot  shape  torch.Size([100, 10, 32])\n",
      "ll  torch.Size([100, 320])\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('./outimg/Train/facefeature.csv')\n",
    "features = features.iloc[:, 1:]\n",
    "features = np.array(features, dtype=np.float32).reshape(-1,68,2);\n",
    "features = np.swapaxes(features, 1, 2)\n",
    "#features = np.take(features, DI.LEFT_EYE, axis=0)\n",
    "#print(features.shape)\n",
    "# print(features[0])\n",
    "\n",
    "features = torch.Tensor(features)\n",
    "conv1d = nn.MaxPool1d(2)(nn.Conv1d(2, 10, 5)(features))\n",
    "nn.ReLU(inplace=True)(conv1d)\n",
    "print(\"maxdot  shape \", conv1d.shape )\n",
    "ll = nn.Flatten()(conv1d)\n",
    "print('ll ', ll.size())\n",
    "#print(nn.MaxPool1d(dot, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, F, C]: torch.Size([10, 2, 68])\n",
      "Shape of Tensor y: torch.Size([10, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 머리      headSize =0, headWidth = 1,\n",
    "# 이마      foreheadPosition= 2,foreheadSize = 3,\n",
    "# 턱        jawsPosition =4,jawsSize = 5,\n",
    "# 아래턱    chinPosition = 6,chinSize = 7,chinPronounced = 8,\n",
    "# 뺨        lowCheek = 9, cheekPosition = 10, cheekSize = 11,\n",
    "# 귀        earsPosition = 12, earsRotation =13, earsSize = 14\n",
    "# 눈        eyeSize = 15, eyePosition= 16, eyeDepth = 17, eyeRotation = 18, eyeDistance = 19, eyeSquint= 20,\n",
    "# 코        noseSize,nosePosition,noseFlatten,nosePronounced,noseWidth,noseBridge,noseCurve,noseInclination,\n",
    "# 입        mouthSize,mouthPosition,mouthPronounced,lipsSize\n",
    "\n",
    "#eyeIndexes = ['eyeSize', 'eyePosition', 'eyeDepth', 'eyeRotation', 'eyeDistance', 'eyeSquint']\n",
    "#featureIndexes = DI.LEFT_EYE\n",
    "#labelIndexes = list(range(15, 21)) # eye \n",
    "featureIndexes = None\n",
    "labelIndexes = list([1])\n",
    "\n",
    "training_data = FaceFeatureDataset(feature_file=\"./outimg/Train/facefeature.csv\", feature_indexes=featureIndexes,\n",
    "                                   label_file=\"./Dataset/Train/csv/train.csv\", label_indexes=labelIndexes)\n",
    "test_data = FaceFeatureDataset(feature_file=\"./outimg/Test/facefeature.csv\", feature_indexes=featureIndexes,\n",
    "                               label_file=\"./Dataset/Test/csv/test.csv\", label_indexes=labelIndexes)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 데이터 로드 확인\n",
    "for X, y in test_loader:\n",
    "    # N , Channel, H= width W = height\n",
    "    print(f\"Shape of X [N, F, C]: {X.shape}\")\n",
    "    print(f\"Shape of Tensor y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "n_total_steps = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (cnn1): Sequential(\n",
      "    (0): Conv1d(2, 16, kernel_size=(3,), stride=(1,))\n",
      "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (cnn2): Sequential(\n",
      "    (0): Conv1d(16, 32, kernel_size=(4,), stride=(1,))\n",
      "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=240, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[[ 0.2075,  0.1200, -0.3430],\n",
      "         [-0.3474,  0.2903, -0.0732]],\n",
      "\n",
      "        [[-0.3316, -0.0650,  0.1470],\n",
      "         [ 0.3705, -0.1290, -0.4014]],\n",
      "\n",
      "        [[ 0.2061, -0.2769, -0.3635],\n",
      "         [ 0.3473,  0.2101, -0.1603]],\n",
      "\n",
      "        [[ 0.1288,  0.2902,  0.1453],\n",
      "         [-0.1946, -0.3981,  0.0173]],\n",
      "\n",
      "        [[-0.0644, -0.0175,  0.0571],\n",
      "         [-0.2111,  0.1003, -0.1687]],\n",
      "\n",
      "        [[-0.1678, -0.0095,  0.0842],\n",
      "         [ 0.3665,  0.1447,  0.3961]],\n",
      "\n",
      "        [[-0.3291, -0.3655,  0.1273],\n",
      "         [-0.1600, -0.0552,  0.3800]],\n",
      "\n",
      "        [[ 0.0057,  0.1024, -0.3620],\n",
      "         [ 0.1545, -0.1688,  0.0585]],\n",
      "\n",
      "        [[-0.0940, -0.2448, -0.1155],\n",
      "         [-0.0505, -0.2626,  0.2266]],\n",
      "\n",
      "        [[-0.2459, -0.1613,  0.2622],\n",
      "         [ 0.0957,  0.0970,  0.0928]],\n",
      "\n",
      "        [[ 0.3628, -0.0851, -0.3926],\n",
      "         [ 0.2019, -0.3047, -0.1647]],\n",
      "\n",
      "        [[-0.3557, -0.3321,  0.0054],\n",
      "         [ 0.4051, -0.1957,  0.1890]],\n",
      "\n",
      "        [[-0.0821,  0.0903,  0.3395],\n",
      "         [-0.0893, -0.0073, -0.0949]],\n",
      "\n",
      "        [[-0.0997,  0.0215, -0.1563],\n",
      "         [ 0.1483,  0.3278, -0.2727]],\n",
      "\n",
      "        [[-0.0102, -0.3121, -0.2687],\n",
      "         [-0.3760, -0.0443,  0.2689]],\n",
      "\n",
      "        [[-0.0141, -0.1453,  0.2257],\n",
      "         [-0.0290,  0.2124,  0.0259]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-1.0111e-01, -3.1664e-01, -2.9830e-01,  8.9401e-02,  3.2626e-01,\n",
      "         1.3446e-01, -1.9214e-01,  3.8009e-01, -3.3906e-04, -3.9030e-01,\n",
      "         1.7593e-01, -3.9127e-01,  1.4718e-01,  2.7456e-01, -1.1351e-02,\n",
      "        -3.8621e-01], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[-0.0548,  0.0305, -0.0701, -0.1171],\n",
      "         [ 0.0793,  0.0890, -0.0204, -0.0557],\n",
      "         [-0.1034,  0.0953,  0.0775,  0.0135],\n",
      "         ...,\n",
      "         [-0.1170, -0.0679, -0.0013, -0.1205],\n",
      "         [-0.0517, -0.1148, -0.0161, -0.1023],\n",
      "         [ 0.0982, -0.1003,  0.0487,  0.0149]],\n",
      "\n",
      "        [[-0.0118,  0.0372,  0.0766,  0.0892],\n",
      "         [ 0.1179, -0.1027,  0.0992, -0.0155],\n",
      "         [ 0.0470,  0.0982, -0.1092,  0.0656],\n",
      "         ...,\n",
      "         [-0.0102, -0.0063,  0.0055,  0.0439],\n",
      "         [ 0.1093,  0.0325,  0.0066,  0.0865],\n",
      "         [ 0.0129,  0.0483, -0.0995,  0.0801]],\n",
      "\n",
      "        [[ 0.0232, -0.0946,  0.0739, -0.1167],\n",
      "         [ 0.0437,  0.1055, -0.0701,  0.0704],\n",
      "         [ 0.0064,  0.0679,  0.0457,  0.0172],\n",
      "         ...,\n",
      "         [-0.1196, -0.0819,  0.0036, -0.0014],\n",
      "         [ 0.0496, -0.0767, -0.0545,  0.1044],\n",
      "         [-0.0732,  0.0289,  0.0612, -0.0665]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0791, -0.0592, -0.0740, -0.0027],\n",
      "         [ 0.1021,  0.0174, -0.0960,  0.0253],\n",
      "         [-0.0880, -0.0660,  0.0767,  0.1095],\n",
      "         ...,\n",
      "         [ 0.0732,  0.0961, -0.1222,  0.0224],\n",
      "         [ 0.0404,  0.0890, -0.0891,  0.1085],\n",
      "         [ 0.0216, -0.1123, -0.0244, -0.1204]],\n",
      "\n",
      "        [[ 0.0552, -0.1174,  0.0074, -0.0811],\n",
      "         [ 0.0649,  0.0567,  0.0968,  0.0797],\n",
      "         [ 0.0105,  0.0475, -0.0104, -0.1039],\n",
      "         ...,\n",
      "         [ 0.0173, -0.0022,  0.0743, -0.0676],\n",
      "         [ 0.0114,  0.0153, -0.1245, -0.0664],\n",
      "         [-0.0194, -0.0746,  0.1216, -0.0306]],\n",
      "\n",
      "        [[ 0.0892, -0.0596, -0.0816, -0.0091],\n",
      "         [-0.0114,  0.0939,  0.0636, -0.1082],\n",
      "         [ 0.0608, -0.0550,  0.0560,  0.1085],\n",
      "         ...,\n",
      "         [-0.0666, -0.0922,  0.1099, -0.0392],\n",
      "         [ 0.1092,  0.0116, -0.0609, -0.0744],\n",
      "         [ 0.0840, -0.0710,  0.0382,  0.0820]]], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0375, -0.0142,  0.0909,  0.0436,  0.0046,  0.0615, -0.0614,  0.0819,\n",
      "         0.1214,  0.1148, -0.1243,  0.0830, -0.0723,  0.0628,  0.1042, -0.1095,\n",
      "         0.0173, -0.0259,  0.0964, -0.0286,  0.0650,  0.0988, -0.1068, -0.0746,\n",
      "        -0.0657, -0.1178,  0.0380, -0.1114,  0.0654, -0.0116,  0.0046, -0.1042],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0379,  0.0097, -0.0541,  ...,  0.0488,  0.0046,  0.0262],\n",
      "        [ 0.0533,  0.0290, -0.0456,  ..., -0.0064,  0.0417,  0.0098],\n",
      "        [-0.0526, -0.0023, -0.0483,  ...,  0.0616,  0.0539, -0.0023],\n",
      "        ...,\n",
      "        [-0.0345, -0.0514,  0.0179,  ...,  0.0459,  0.0643,  0.0113],\n",
      "        [ 0.0637,  0.0537,  0.0555,  ...,  0.0328,  0.0137, -0.0566],\n",
      "        [-0.0549, -0.0351, -0.0322,  ..., -0.0432,  0.0172,  0.0394]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 1.1117e-02,  4.5572e-02, -1.4912e-02,  7.7840e-03,  3.9189e-02,\n",
      "         3.6685e-02,  4.3995e-02,  5.2122e-02, -5.3740e-02,  5.1704e-02,\n",
      "        -3.8811e-02,  6.1128e-02,  5.9261e-02, -1.8854e-02,  1.7613e-02,\n",
      "        -6.4545e-02,  4.2709e-02,  4.8110e-02, -9.3734e-03,  1.7684e-02,\n",
      "         7.1084e-03,  5.9442e-02,  5.3637e-02,  5.0933e-04,  2.0225e-02,\n",
      "        -3.4941e-02,  2.4386e-02,  3.3903e-02, -3.1424e-02,  4.7804e-03,\n",
      "        -3.8843e-02,  3.8432e-02,  4.0765e-02, -2.0453e-02, -4.4707e-02,\n",
      "         2.9271e-02,  6.4213e-03, -9.1682e-03, -5.8270e-02,  2.4716e-02,\n",
      "         1.6361e-02, -4.0764e-02,  2.3583e-02, -4.5397e-02, -3.1128e-02,\n",
      "         4.0657e-02, -7.8274e-03,  4.3819e-02, -3.8801e-02,  1.6373e-02,\n",
      "         5.3611e-02,  7.9116e-03, -3.8339e-02,  7.0046e-03, -3.6829e-02,\n",
      "        -3.5976e-02, -6.1855e-02, -6.2510e-02, -2.4359e-02,  5.7098e-02,\n",
      "        -4.7152e-02,  6.1451e-02, -3.2712e-02,  2.1567e-03, -4.8387e-02,\n",
      "         1.9551e-02,  2.5575e-03,  5.8959e-02, -1.2453e-02,  2.3201e-02,\n",
      "        -3.5212e-02, -1.5035e-03, -3.6786e-02, -1.3504e-02,  7.2289e-04,\n",
      "        -5.3507e-02,  3.6679e-02, -4.4375e-02, -5.4513e-02,  4.1297e-02,\n",
      "        -2.5761e-03,  2.7222e-02, -6.2067e-02,  4.3842e-02,  2.6305e-02,\n",
      "         3.6462e-03,  6.2013e-03,  9.5323e-05, -5.1077e-02,  1.1586e-02,\n",
      "         1.1556e-02, -3.6151e-03,  6.7421e-03,  4.5633e-02, -2.6852e-02,\n",
      "        -8.5550e-03,  1.9486e-02, -6.1496e-02, -4.5598e-02,  4.9724e-03,\n",
      "         1.1487e-02,  1.5943e-02,  2.4176e-02, -5.4388e-02, -5.8809e-02,\n",
      "        -1.8618e-02,  4.4898e-02, -2.1969e-02,  9.8794e-04,  4.6789e-02,\n",
      "        -6.4507e-02, -1.8058e-02,  3.9470e-02, -4.3771e-02, -1.4329e-02,\n",
      "         1.9200e-02,  2.6208e-02, -4.7806e-02, -6.3868e-02,  5.4079e-02,\n",
      "         3.4654e-02, -1.8882e-02,  1.8398e-02, -6.2742e-02, -5.9100e-02,\n",
      "        -2.3138e-02,  2.0997e-02,  3.1101e-02], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0223,  0.0266, -0.0344, -0.0224,  0.0776,  0.0501,  0.0865,  0.0390,\n",
      "          0.0213, -0.0441, -0.0696, -0.0603, -0.0883, -0.0627,  0.0172,  0.0626,\n",
      "          0.0256, -0.0833, -0.0729,  0.0621,  0.0583,  0.0084,  0.0259, -0.0696,\n",
      "          0.0407, -0.0824, -0.0248,  0.0464,  0.0700,  0.0218, -0.0037,  0.0433,\n",
      "         -0.0037, -0.0783,  0.0212,  0.0454, -0.0211, -0.0243, -0.0124, -0.0793,\n",
      "          0.0419,  0.0086, -0.0511,  0.0373,  0.0599, -0.0319, -0.0643,  0.0425,\n",
      "         -0.0315,  0.0512,  0.0006,  0.0450,  0.0496, -0.0588,  0.0669,  0.0534,\n",
      "          0.0866,  0.0646, -0.0517, -0.0776,  0.0118, -0.0654,  0.0513, -0.0399,\n",
      "         -0.0386, -0.0622,  0.0701, -0.0679,  0.0499,  0.0679,  0.0870, -0.0032,\n",
      "         -0.0241,  0.0790, -0.0772,  0.0107, -0.0111,  0.0483, -0.0842,  0.0183,\n",
      "         -0.0069, -0.0615,  0.0473,  0.0517,  0.0871, -0.0804, -0.0643, -0.0520,\n",
      "          0.0099, -0.0088, -0.0589, -0.0489,  0.0353,  0.0007,  0.0602,  0.0268,\n",
      "          0.0614, -0.0265, -0.0435,  0.0092,  0.0303,  0.0818,  0.0069,  0.0387,\n",
      "         -0.0543, -0.0470,  0.0388, -0.0226, -0.0177,  0.0488, -0.0790, -0.0788,\n",
      "         -0.0294,  0.0609, -0.0543,  0.0518,  0.0041,  0.0063, -0.0724, -0.0590,\n",
      "         -0.0014,  0.0191,  0.0123, -0.0299, -0.0736, -0.0285,  0.0458,  0.0197]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0673], device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "#num_classes = len(eyeIndexes)\n",
    "#print(num_classes)\n",
    "num_classes = 1\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(2, 16, 3),   # 68 -> 66\n",
    "            nn.MaxPool1d(2),        # 66 -> 33\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, 4),   # 33 -> 30\n",
    "            nn.MaxPool1d(2),        # 30-> 15\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(15 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)        \n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('batch',  batch)\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x480 and 240x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     train(train_loader, model, criterion, optimizer)    \n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39mparameters()))\n",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Compute prediction error\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m pred \u001b[39m=\u001b[39m model(X)        \n\u001b[0;32m      8\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn2(x)\n\u001b[0;32m     30\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[1;32m---> 31\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[0;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x480 and 240x128)"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, criterion, optimizer)    \n",
    "print(\"Done!\")\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './face_eye.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    print('test size', size )\n",
    "    # num_batches = len(dataloader)\n",
    "    # model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)            \n",
    "            print('pred =', pred)\n",
    "            #print('loss', loss)\n",
    "            #print('real', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size 100\n",
      "pred = tensor([[0.4839],\n",
      "        [0.4787],\n",
      "        [0.4820],\n",
      "        [0.4863],\n",
      "        [0.4812],\n",
      "        [0.4704],\n",
      "        [0.4775],\n",
      "        [0.4687],\n",
      "        [0.4762],\n",
      "        [0.4729]], device='cuda:0')\n",
      "pred = tensor([[0.4760],\n",
      "        [0.4766],\n",
      "        [0.4747],\n",
      "        [0.4778],\n",
      "        [0.4764],\n",
      "        [0.4660],\n",
      "        [0.4801],\n",
      "        [0.4711],\n",
      "        [0.4828],\n",
      "        [0.4878]], device='cuda:0')\n",
      "pred = tensor([[0.4878],\n",
      "        [0.4714],\n",
      "        [0.4755],\n",
      "        [0.4716],\n",
      "        [0.4810],\n",
      "        [0.4738],\n",
      "        [0.4767],\n",
      "        [0.4680],\n",
      "        [0.4786],\n",
      "        [0.4788]], device='cuda:0')\n",
      "pred = tensor([[0.4760],\n",
      "        [0.4808],\n",
      "        [0.4811],\n",
      "        [0.4904],\n",
      "        [0.4752],\n",
      "        [0.4792],\n",
      "        [0.4788],\n",
      "        [0.4774],\n",
      "        [0.4801],\n",
      "        [0.4880]], device='cuda:0')\n",
      "pred = tensor([[0.4846],\n",
      "        [0.4797],\n",
      "        [0.4830],\n",
      "        [0.4898],\n",
      "        [0.4853],\n",
      "        [0.4731],\n",
      "        [0.4805],\n",
      "        [0.4707],\n",
      "        [0.4752],\n",
      "        [0.4743]], device='cuda:0')\n",
      "pred = tensor([[0.4763],\n",
      "        [0.4707],\n",
      "        [0.4691],\n",
      "        [0.4715],\n",
      "        [0.4862],\n",
      "        [0.4760],\n",
      "        [0.4748],\n",
      "        [0.4668],\n",
      "        [0.4719],\n",
      "        [0.4696]], device='cuda:0')\n",
      "pred = tensor([[0.4741],\n",
      "        [0.4904],\n",
      "        [0.4764],\n",
      "        [0.4759],\n",
      "        [0.4765],\n",
      "        [0.4723],\n",
      "        [0.4837],\n",
      "        [0.4692],\n",
      "        [0.4651],\n",
      "        [0.4771]], device='cuda:0')\n",
      "pred = tensor([[0.4748],\n",
      "        [0.4758],\n",
      "        [0.4779],\n",
      "        [0.4798],\n",
      "        [0.4700],\n",
      "        [0.4845],\n",
      "        [0.4757],\n",
      "        [0.4763],\n",
      "        [0.4623],\n",
      "        [0.4753]], device='cuda:0')\n",
      "pred = tensor([[0.4771],\n",
      "        [0.4706],\n",
      "        [0.4827],\n",
      "        [0.4747],\n",
      "        [0.4852],\n",
      "        [0.4697],\n",
      "        [0.4818],\n",
      "        [0.4663],\n",
      "        [0.4646],\n",
      "        [0.4760]], device='cuda:0')\n",
      "pred = tensor([[0.4728],\n",
      "        [0.4776],\n",
      "        [0.4729],\n",
      "        [0.4645],\n",
      "        [0.4773],\n",
      "        [0.4896],\n",
      "        [0.4784],\n",
      "        [0.4781],\n",
      "        [0.4731],\n",
      "        [0.4814]], device='cuda:0')\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test(test_loader, model, criterion)\n",
    "\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4839]], device='cuda:0') [0.7514151]\n",
      "tensor([[0.4787]], device='cuda:0') [0.3392321]\n",
      "tensor([[0.4820]], device='cuda:0') [0.699237]\n",
      "tensor([[0.4863]], device='cuda:0') [0.3948978]\n",
      "tensor([[0.4812]], device='cuda:0') [0.7726646]\n",
      "tensor([[0.4704]], device='cuda:0') [0.3179817]\n",
      "tensor([[0.4775]], device='cuda:0') [0.3590497]\n",
      "tensor([[0.4687]], device='cuda:0') [0.2156946]\n",
      "tensor([[0.4762]], device='cuda:0') [0.7897764]\n",
      "tensor([[0.4729]], device='cuda:0') [0.238782]\n",
      "tensor([[0.4760]], device='cuda:0') [0.4150633]\n",
      "tensor([[0.4766]], device='cuda:0') [0.4519213]\n",
      "tensor([[0.4747]], device='cuda:0') [0.7632524]\n",
      "tensor([[0.4778]], device='cuda:0') [0.777518]\n",
      "tensor([[0.4764]], device='cuda:0') [0.5334069]\n",
      "tensor([[0.4660]], device='cuda:0') [0.3720133]\n",
      "tensor([[0.4801]], device='cuda:0') [0.3742987]\n",
      "tensor([[0.4711]], device='cuda:0') [0.5546108]\n",
      "tensor([[0.4828]], device='cuda:0') [0.5770053]\n",
      "tensor([[0.4878]], device='cuda:0') [0.5383425]\n",
      "tensor([[0.4878]], device='cuda:0') [0.3348784]\n",
      "tensor([[0.4714]], device='cuda:0') [0.4457583]\n",
      "tensor([[0.4755]], device='cuda:0') [0.5877371]\n",
      "tensor([[0.4716]], device='cuda:0') [0.6813793]\n",
      "tensor([[0.4810]], device='cuda:0') [0.5947482]\n",
      "tensor([[0.4738]], device='cuda:0') [0.7874305]\n",
      "tensor([[0.4767]], device='cuda:0') [0.44857]\n",
      "tensor([[0.4680]], device='cuda:0') [0.4890919]\n",
      "tensor([[0.4786]], device='cuda:0') [0.7687374]\n",
      "tensor([[0.4788]], device='cuda:0') [0.555645]\n",
      "tensor([[0.4760]], device='cuda:0') [0.5210066]\n",
      "tensor([[0.4808]], device='cuda:0') [0.5105873]\n",
      "tensor([[0.4811]], device='cuda:0') [0.393123]\n",
      "tensor([[0.4904]], device='cuda:0') [0.776121]\n",
      "tensor([[0.4752]], device='cuda:0') [0.3544974]\n",
      "tensor([[0.4792]], device='cuda:0') [0.2871353]\n",
      "tensor([[0.4788]], device='cuda:0') [0.7716774]\n",
      "tensor([[0.4774]], device='cuda:0') [0.6427369]\n",
      "tensor([[0.4801]], device='cuda:0') [0.7460871]\n",
      "tensor([[0.4880]], device='cuda:0') [0.5899132]\n",
      "tensor([[0.4846]], device='cuda:0') [0.5343608]\n",
      "tensor([[0.4797]], device='cuda:0') [0.4251157]\n",
      "tensor([[0.4830]], device='cuda:0') [0.5428584]\n",
      "tensor([[0.4898]], device='cuda:0') [0.3899072]\n",
      "tensor([[0.4853]], device='cuda:0') [0.2191725]\n",
      "tensor([[0.4731]], device='cuda:0') [0.300104]\n",
      "tensor([[0.4805]], device='cuda:0') [0.4320756]\n",
      "tensor([[0.4707]], device='cuda:0') [0.2306103]\n",
      "tensor([[0.4752]], device='cuda:0') [0.2954167]\n",
      "tensor([[0.4743]], device='cuda:0') [0.2496997]\n",
      "tensor([[0.4763]], device='cuda:0') [0.3417028]\n",
      "tensor([[0.4707]], device='cuda:0') [0.2408551]\n",
      "tensor([[0.4691]], device='cuda:0') [0.3337229]\n",
      "tensor([[0.4715]], device='cuda:0') [0.4248672]\n",
      "tensor([[0.4862]], device='cuda:0') [0.6069309]\n",
      "tensor([[0.4760]], device='cuda:0') [0.2658185]\n",
      "tensor([[0.4748]], device='cuda:0') [0.3821279]\n",
      "tensor([[0.4668]], device='cuda:0') [0.2814378]\n",
      "tensor([[0.4719]], device='cuda:0') [0.3760981]\n",
      "tensor([[0.4696]], device='cuda:0') [0.6699055]\n",
      "tensor([[0.4741]], device='cuda:0') [0.6601395]\n",
      "tensor([[0.4904]], device='cuda:0') [0.4436725]\n",
      "tensor([[0.4764]], device='cuda:0') [0.4709488]\n",
      "tensor([[0.4759]], device='cuda:0') [0.2026214]\n",
      "tensor([[0.4765]], device='cuda:0') [0.3348584]\n",
      "tensor([[0.4723]], device='cuda:0') [0.3671478]\n",
      "tensor([[0.4837]], device='cuda:0') [0.7648237]\n",
      "tensor([[0.4692]], device='cuda:0') [0.2536433]\n",
      "tensor([[0.4651]], device='cuda:0') [0.6598772]\n",
      "tensor([[0.4771]], device='cuda:0') [0.7141988]\n",
      "tensor([[0.4748]], device='cuda:0') [0.6109616]\n",
      "tensor([[0.4758]], device='cuda:0') [0.3685029]\n",
      "tensor([[0.4779]], device='cuda:0') [0.3824021]\n",
      "tensor([[0.4798]], device='cuda:0') [0.2971699]\n",
      "tensor([[0.4700]], device='cuda:0') [0.7510576]\n",
      "tensor([[0.4845]], device='cuda:0') [0.7144451]\n",
      "tensor([[0.4757]], device='cuda:0') [0.7086709]\n",
      "tensor([[0.4763]], device='cuda:0') [0.3045474]\n",
      "tensor([[0.4623]], device='cuda:0') [0.525554]\n",
      "tensor([[0.4753]], device='cuda:0') [0.2506679]\n",
      "tensor([[0.4771]], device='cuda:0') [0.6272711]\n",
      "tensor([[0.4706]], device='cuda:0') [0.3634543]\n",
      "tensor([[0.4827]], device='cuda:0') [0.264377]\n",
      "tensor([[0.4747]], device='cuda:0') [0.4505621]\n",
      "tensor([[0.4852]], device='cuda:0') [0.5781866]\n",
      "tensor([[0.4697]], device='cuda:0') [0.4045408]\n",
      "tensor([[0.4818]], device='cuda:0') [0.2469307]\n",
      "tensor([[0.4663]], device='cuda:0') [0.3974999]\n",
      "tensor([[0.4646]], device='cuda:0') [0.3276154]\n",
      "tensor([[0.4760]], device='cuda:0') [0.672255]\n",
      "tensor([[0.4728]], device='cuda:0') [0.6616972]\n",
      "tensor([[0.4776]], device='cuda:0') [0.4873055]\n",
      "tensor([[0.4729]], device='cuda:0') [0.6611649]\n",
      "tensor([[0.4645]], device='cuda:0') [0.7930877]\n",
      "tensor([[0.4773]], device='cuda:0') [0.5814181]\n",
      "tensor([[0.4896]], device='cuda:0') [0.7410051]\n",
      "tensor([[0.4784]], device='cuda:0') [0.5021181]\n",
      "tensor([[0.4781]], device='cuda:0') [0.278518]\n",
      "tensor([[0.4731]], device='cuda:0') [0.2869007]\n",
      "tensor([[0.4814]], device='cuda:0') [0.6269171]\n"
     ]
    }
   ],
   "source": [
    "#print(training_data[0])\n",
    "\n",
    "test_iter = iter(test_data)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_iter:\n",
    "        x = np.array([x])        \n",
    "        x = torch.Tensor(x).to(device)\n",
    "        pred = model(x)\n",
    "        print(pred, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
